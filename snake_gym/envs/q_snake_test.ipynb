{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc5e3f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install stable-baselines3 gym\n",
    "# !pip install \"shimmy>=2.0\"\n",
    "# !pip install opencv-python\n",
    "# !pip install seaborn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "747dcab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add project root to import path (temporary)\n",
    "import sys\n",
    "sys.path.insert(0, r\"c:\\Users\\Acer\\Desktop\\snake-gym\")\n",
    "import snake_gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "027a4664",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Acer\\Desktop\\snake-gym\\.venv\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:31: UserWarning: \u001b[33mWARN: A Box observation space has an unconventional shape (neither an image, nor a 1D vector). We recommend flattening the observation to have only a 1D vector or use a custom policy to properly process the data. Actual observation shape: (150, 150)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import snake_gym\n",
    "import random\n",
    "\n",
    "env = gym.make(\"snake-v0\")\n",
    "# obs = env.reset()\n",
    "# done = False\n",
    "\n",
    "# while not done:\n",
    "#     action = random.choice([0, 1, 2, 3])\n",
    "#     obs, reward, done, info = env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b8685c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Training Q-Learning (Seed: 42)\n",
      "Recording gameplay every 500 episodes to: recordings_q-learning_seed42/\n",
      "======================================================================\n",
      "\n",
      "  üèÜ New best score: 1 at episode 1\n",
      "  üèÜ New best score: 2 at episode 39\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "import psutil\n",
    "import pickle\n",
    "import cv2\n",
    "import pygame\n",
    "from collections import deque\n",
    "from snake_gym.envs.snake import SnakeGame\n",
    "from snake_gym.envs.modules import GRIDSIZE\n",
    "\n",
    "class MetricsLogger:\n",
    "    \"\"\"Comprehensive metrics tracking for RL training\"\"\"\n",
    "    \n",
    "    def __init__(self, agent_name, seed, record_gameplay=True, record_frequency=500):\n",
    "        self.agent_name = agent_name\n",
    "        self.seed = seed\n",
    "        self.start_time = time.time()\n",
    "        \n",
    "        # Gameplay recording settings\n",
    "        self.record_gameplay = record_gameplay\n",
    "        self.record_frequency = record_frequency  # Record every N episodes\n",
    "        self.recorded_episodes = []  # List of episode numbers that were recorded\n",
    "        self.best_episode_score = 0\n",
    "        self.best_episode_number = 0\n",
    "        \n",
    "        # Episode-level metrics\n",
    "        self.episode_rewards = []\n",
    "        self.episode_scores = []\n",
    "        self.episode_lengths = []\n",
    "        self.episode_times = []\n",
    "        \n",
    "        # Training stability metrics\n",
    "        self.reward_variance_per_block = []\n",
    "        self.score_variance_per_block = []\n",
    "        \n",
    "        # Exploration metrics\n",
    "        self.epsilon_history = []\n",
    "        \n",
    "        # Action distribution (for behavioral analysis)\n",
    "        self.action_counts = {0: 0, 1: 0, 2: 0, 3: 0}\n",
    "        self.action_history_per_block = []\n",
    "        \n",
    "        # Computational metrics\n",
    "        self.cpu_usage = []\n",
    "        self.memory_usage = []\n",
    "        \n",
    "        # Convergence metrics\n",
    "        self.moving_avg_reward = []\n",
    "        self.moving_avg_score = []\n",
    "        \n",
    "        # Sample efficiency\n",
    "        self.episodes_to_threshold = {}\n",
    "    \n",
    "    def should_record_episode(self, episode_num):\n",
    "        \"\"\"Determine if this episode should be recorded\"\"\"\n",
    "        if not self.record_gameplay:\n",
    "            return False\n",
    "        \n",
    "        # Record at specific intervals\n",
    "        if episode_num % self.record_frequency == 0:\n",
    "            return True\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def is_best_episode(self, score):\n",
    "        \"\"\"Check if this is the best episode so far\"\"\"\n",
    "        return score > self.best_episode_score\n",
    "    \n",
    "    def update_best_episode(self, episode_num, score):\n",
    "        \"\"\"Update best episode tracker\"\"\"\n",
    "        if score > self.best_episode_score:\n",
    "            self.best_episode_score = score\n",
    "            self.best_episode_number = episode_num\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def log_episode(self, reward, score, length, epsilon, episode_time, actions_taken):\n",
    "        \"\"\"Log metrics for a single episode\"\"\"\n",
    "        self.episode_rewards.append(reward)\n",
    "        self.episode_scores.append(score)\n",
    "        self.episode_lengths.append(length)\n",
    "        self.episode_times.append(episode_time)\n",
    "        self.epsilon_history.append(epsilon)\n",
    "        \n",
    "        # Update action counts\n",
    "        for action in actions_taken:\n",
    "            self.action_counts[action] += 1\n",
    "        \n",
    "        # Calculate moving averages (window of 100)\n",
    "        window = 100\n",
    "        if len(self.episode_rewards) >= window:\n",
    "            self.moving_avg_reward.append(np.mean(self.episode_rewards[-window:]))\n",
    "            self.moving_avg_score.append(np.mean(self.episode_scores[-window:]))\n",
    "        else:\n",
    "            self.moving_avg_reward.append(np.mean(self.episode_rewards))\n",
    "            self.moving_avg_score.append(np.mean(self.episode_scores))\n",
    "        \n",
    "        # Log system metrics\n",
    "        self.cpu_usage.append(psutil.cpu_percent(interval=0.1))\n",
    "        self.memory_usage.append(psutil.virtual_memory().percent)\n",
    "    \n",
    "    def log_block_statistics(self, block_size=1000):\n",
    "        \"\"\"Calculate variance for blocks of episodes\"\"\"\n",
    "        if len(self.episode_rewards) >= block_size:\n",
    "            recent_rewards = self.episode_rewards[-block_size:]\n",
    "            recent_scores = self.episode_scores[-block_size:]\n",
    "            \n",
    "            self.reward_variance_per_block.append(np.var(recent_rewards))\n",
    "            self.score_variance_per_block.append(np.var(recent_scores))\n",
    "            \n",
    "            # Action distribution for this block\n",
    "            total_actions = sum(self.action_counts.values())\n",
    "            if total_actions > 0:\n",
    "                action_dist = {k: v/total_actions for k, v in self.action_counts.items()}\n",
    "                self.action_history_per_block.append(action_dist)\n",
    "    \n",
    "    def check_convergence_threshold(self, threshold_percent=0.8):\n",
    "        \"\"\"Check if agent reached threshold of max performance\"\"\"\n",
    "        if len(self.moving_avg_score) < 100:\n",
    "            return\n",
    "        \n",
    "        max_score = max(self.moving_avg_score)\n",
    "        threshold = threshold_percent * max_score\n",
    "        \n",
    "        threshold_name = f\"{int(threshold_percent*100)}%_max\"\n",
    "        \n",
    "        if threshold_name not in self.episodes_to_threshold:\n",
    "            for i, score in enumerate(self.moving_avg_score):\n",
    "                if score >= threshold:\n",
    "                    self.episodes_to_threshold[threshold_name] = i + 1\n",
    "                    print(f\"  ‚úì Reached {threshold_name} performance at episode {i+1}\")\n",
    "                    break\n",
    "    \n",
    "    def get_training_duration(self):\n",
    "        \"\"\"Get total training time\"\"\"\n",
    "        if hasattr(self, 'end_time') and self.end_time is not None:\n",
    "            return self.end_time - self.start_time\n",
    "        elif hasattr(self, 'start_time') and self.start_time is not None:\n",
    "            return time.time() - self.start_time\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    \n",
    "    def get_avg_episode_time(self):\n",
    "        \"\"\"Get average time per episode\"\"\"\n",
    "        return np.mean(self.episode_times) if self.episode_times else 0\n",
    "    \n",
    "    def save(self, filename):\n",
    "        \"\"\"Save all metrics to file\"\"\"\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump(self.__dict__, f)\n",
    "        print(f\"‚úÖ Metrics saved to {filename}\")\n",
    "\n",
    "\n",
    "def record_episode(env, state_func, Q, epsilon, episode_num, agent_name, seed, \n",
    "                   video_path, max_steps=1000):\n",
    "    \"\"\"\n",
    "    Record a single episode to video\n",
    "    \n",
    "    Args:\n",
    "        env: Snake environment\n",
    "        state_func: Function to get state representation\n",
    "        Q: Q-table\n",
    "        epsilon: Current exploration rate\n",
    "        episode_num: Episode number\n",
    "        agent_name: Name of the agent\n",
    "        seed: Random seed\n",
    "        video_path: Path to save video\n",
    "        max_steps: Maximum steps per episode\n",
    "    \"\"\"\n",
    "    # Capture first frame to determine video size\n",
    "    state = env.reset()\n",
    "    state = state_func(env)\n",
    "    first_frame = pygame.surfarray.array3d(env.screen)\n",
    "    first_frame = np.transpose(first_frame, (1, 0, 2))\n",
    "    first_frame = cv2.cvtColor(first_frame, cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    # Overlay height for info text\n",
    "    overlay_height = 50\n",
    "    height, width, _ = first_frame.shape\n",
    "    frame_height = height + overlay_height\n",
    "    \n",
    "    # Initialize VideoWriter\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    fps = 15\n",
    "    video_writer = cv2.VideoWriter(video_path, fourcc, fps, (width, frame_height))\n",
    "    \n",
    "    state = env.reset()\n",
    "    state = state_func(env)\n",
    "    done = False\n",
    "    steps = 0\n",
    "    total_reward = 0\n",
    "    actions_taken = []\n",
    "    \n",
    "    while not done and steps < max_steps:\n",
    "        # Q-values\n",
    "        if state not in Q:\n",
    "            Q[state] = {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0}\n",
    "        \n",
    "        # Epsilon-greedy\n",
    "        if random.random() < min(epsilon, 0.1):  # less exploration for recording\n",
    "            action = random.randint(0, 3)\n",
    "        else:\n",
    "            action = max(Q[state], key=Q[state].get)\n",
    "        \n",
    "        actions_taken.append(action)\n",
    "        \n",
    "        # Step in environment\n",
    "        _, reward, done, _ = env.step(action)\n",
    "        next_state = state_func(env)\n",
    "        \n",
    "        # Capture frame\n",
    "        frame = pygame.surfarray.array3d(env.screen)\n",
    "        frame = np.transpose(frame, (1, 0, 2))\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        # Create overlay frame\n",
    "        overlay = np.zeros((frame_height, width, 3), dtype=np.uint8)\n",
    "        overlay[0:height, :, :] = frame  # original game frame\n",
    "        \n",
    "        # Add info text\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        font_scale = 0.35\n",
    "        color = (255, 255, 255)\n",
    "        thickness = 1\n",
    "        y_offset = height + 15\n",
    "        \n",
    "        text_lines = [\n",
    "            f\"Agent: {agent_name}\",\n",
    "            f\"Seed: {seed}\",\n",
    "            f\"Episode: {episode_num}\",\n",
    "            f\"Step: {steps}\",\n",
    "            f\"Score: {env.snake.length - 1}\"\n",
    "        ]\n",
    "        \n",
    "        for i, text in enumerate(text_lines):\n",
    "            cv2.putText(overlay, text, (5, y_offset + i*12), \n",
    "                        font, font_scale, color, thickness, cv2.LINE_AA)\n",
    "        \n",
    "        overlay = np.ascontiguousarray(overlay, dtype=np.uint8)\n",
    "        \n",
    "        # Write frame\n",
    "        video_writer.write(overlay)\n",
    "        \n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        steps += 1\n",
    "    \n",
    "    video_writer.release()\n",
    "    score = env.snake.length - 1\n",
    "    \n",
    "    return score, total_reward, steps, actions_taken\n",
    "\n",
    "\n",
    "def add_info_overlay(frame, episode, steps, score, agent_name, seed):\n",
    "    \"\"\"Add informative text overlay to frame\"\"\"\n",
    "    # Make frame larger to fit text\n",
    "    overlay = np.zeros((200, 150, 3), dtype=np.uint8)\n",
    "    overlay[0:150, :] = frame\n",
    "    \n",
    "    # Add text information\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    font_scale = 0.35\n",
    "    color = (255, 255, 255)\n",
    "    thickness = 1\n",
    "    \n",
    "    text_lines = [\n",
    "        f\"Agent: {agent_name}\",\n",
    "        f\"Seed: {seed}\",\n",
    "        f\"Episode: {episode}\",\n",
    "        f\"Step: {steps}\",\n",
    "        f\"Score: {score}\"\n",
    "    ]\n",
    "    \n",
    "    y_offset = 160\n",
    "    for i, text in enumerate(text_lines):\n",
    "        cv2.putText(overlay, text, (5, y_offset + i * 10), \n",
    "                   font, font_scale, color, thickness, cv2.LINE_AA)\n",
    "    \n",
    "    return overlay\n",
    "\n",
    "\n",
    "def get_state(env):\n",
    "    \"\"\"State representation function\"\"\"\n",
    "    head_x, head_y = env.snake.get_head_position()\n",
    "    apple_x, apple_y = env.apple.position\n",
    "    direction = env.snake.direction\n",
    "    \n",
    "    def is_danger(dx, dy):\n",
    "        new_x = head_x + dx * GRIDSIZE\n",
    "        new_y = head_y + dy * GRIDSIZE\n",
    "        \n",
    "        if new_x < 0 or new_x >= 150 or new_y < 0 or new_y >= 150:\n",
    "            return 1\n",
    "        if (new_x, new_y) in env.snake.positions[:-1]:\n",
    "            return 1\n",
    "        return 0\n",
    "    \n",
    "    danger_up = is_danger(0, -GRIDSIZE)\n",
    "    danger_down = is_danger(0, GRIDSIZE)\n",
    "    danger_left = is_danger(-GRIDSIZE, 0)\n",
    "    danger_right = is_danger(GRIDSIZE, 0)\n",
    "    \n",
    "    apple_up = int(apple_y < head_y)\n",
    "    apple_down = int(apple_y > head_y)\n",
    "    apple_left = int(apple_x < head_x)\n",
    "    apple_right = int(apple_x > head_x)\n",
    "    \n",
    "    dir_up = int(direction == (0, -1))\n",
    "    dir_down = int(direction == (0, 1))\n",
    "    dir_left = int(direction == (-1, 0))\n",
    "    dir_right = int(direction == (1, 0))\n",
    "    \n",
    "    return (danger_up, danger_down, danger_left, danger_right,\n",
    "            apple_up, apple_down, apple_left, apple_right,\n",
    "            dir_up, dir_down, dir_left, dir_right)\n",
    "\n",
    "\n",
    "def train_q_learning(seed=42, num_episodes=2000, agent_name=\"Q-Learning\", \n",
    "                    record_gameplay=True, record_frequency=500):\n",
    "    \"\"\"Train Q-Learning with comprehensive metrics and gameplay recording\"\"\"\n",
    "    \n",
    "    # Set random seed for reproducibility\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Create recordings directory\n",
    "    import os\n",
    "    recordings_dir = f\"recordings_{agent_name.lower().replace(' ', '_')}_seed{seed}\"\n",
    "    os.makedirs(recordings_dir, exist_ok=True)\n",
    "    \n",
    "    # Hyperparameters\n",
    "    alpha = 0.1\n",
    "    gamma = 0.95\n",
    "    epsilon = 1.0\n",
    "    epsilon_min = 0.01\n",
    "    epsilon_decay = 0.995\n",
    "    \n",
    "    # Initialize\n",
    "    env = SnakeGame()\n",
    "    Q = {}\n",
    "    metrics = MetricsLogger(agent_name, seed, record_gameplay, record_frequency)\n",
    "    metrics.start_time = time.time()     # ‚úÖ start timer\n",
    "\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Training {agent_name} (Seed: {seed})\")\n",
    "    if record_gameplay:\n",
    "        print(f\"Recording gameplay every {record_frequency} episodes to: {recordings_dir}/\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        episode_start = time.time()\n",
    "        \n",
    "        # Check if we should record this episode\n",
    "        should_record = metrics.should_record_episode(episode + 1)\n",
    "        \n",
    "        if should_record:\n",
    "            print(f\"  üé• Recording episode {episode + 1}...\")\n",
    "            video_path = f\"{recordings_dir}/episode_{episode+1:05d}.mp4\"\n",
    "            score, total_reward, steps, actions_taken = record_episode(\n",
    "                env, get_state, Q, epsilon, episode + 1, agent_name, seed, video_path\n",
    "            )\n",
    "            metrics.recorded_episodes.append(episode + 1)\n",
    "            episode_time = time.time() - episode_start\n",
    "            \n",
    "        else:\n",
    "            # Normal training without recording\n",
    "            state = env.reset()\n",
    "            state = get_state(env)\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "            steps = 0\n",
    "            actions_taken = []\n",
    "            \n",
    "            while not done and steps < 1000:\n",
    "                # Initialize Q-values\n",
    "                if state not in Q:\n",
    "                    Q[state] = {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0}\n",
    "                \n",
    "                # Epsilon-greedy\n",
    "                if random.random() < epsilon:\n",
    "                    action = random.randint(0, 3)\n",
    "                else:\n",
    "                    action = max(Q[state], key=Q[state].get)\n",
    "                \n",
    "                actions_taken.append(action)\n",
    "                \n",
    "                # Take step\n",
    "                _, reward, done, info = env.step(action)\n",
    "                next_state = get_state(env)\n",
    "                \n",
    "                if next_state not in Q:\n",
    "                    Q[next_state] = {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0}\n",
    "                \n",
    "                # Q-Learning update\n",
    "                best_next = max(Q[next_state].values()) if not done else 0\n",
    "                Q[state][action] += alpha * (reward + gamma * best_next - Q[state][action])\n",
    "                \n",
    "                state = next_state\n",
    "                total_reward += reward\n",
    "                steps += 1\n",
    "            \n",
    "            score = env.snake.length - 1\n",
    "            episode_time = time.time() - episode_start\n",
    "            \n",
    "        metrics.end_time = time.time()       # ‚úÖ end timer\n",
    "\n",
    "        \n",
    "        # Check if this is the best episode so far\n",
    "        if metrics.update_best_episode(episode + 1, score):\n",
    "            print(f\"  üèÜ New best score: {score} at episode {episode + 1}\")\n",
    "        \n",
    "        # Decay epsilon\n",
    "        epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "        \n",
    "        # Log metrics\n",
    "        metrics.log_episode(total_reward, score, steps, epsilon, episode_time, actions_taken)\n",
    "        \n",
    "        # Check convergence thresholds\n",
    "        if (episode + 1) % 100 == 0:\n",
    "            metrics.check_convergence_threshold(0.5)\n",
    "            metrics.check_convergence_threshold(0.8)\n",
    "            metrics.check_convergence_threshold(0.9)\n",
    "        \n",
    "        # Log block statistics\n",
    "        if (episode + 1) % 1000 == 0:\n",
    "            metrics.log_block_statistics(1000)\n",
    "        \n",
    "        # Print progress\n",
    "        if (episode + 1) % 100 == 0:\n",
    "            avg_score = np.mean(metrics.episode_scores[-100:])\n",
    "            avg_reward = np.mean(metrics.episode_rewards[-100:])\n",
    "            max_score = max(metrics.episode_scores[-100:])\n",
    "            print(f\"Episode {episode+1:5d} | \"\n",
    "                  f\"Avg Score: {avg_score:5.2f} | \"\n",
    "                  f\"Max Score: {max_score:3d} | \"\n",
    "                  f\"Avg Reward: {avg_reward:7.2f} | \"\n",
    "                  f\"Œµ: {epsilon:.3f} | \"\n",
    "                  f\"States: {len(Q):5d} | \"\n",
    "                  f\"Time: {metrics.get_avg_episode_time()*1000:.1f}ms/ep\")\n",
    "    \n",
    "    # Record the best episode one more time at the end\n",
    "    if record_gameplay and metrics.best_episode_number > 0:\n",
    "        print(f\"\\nüé¨ Recording best episode replay (Episode {metrics.best_episode_number}, Score: {metrics.best_episode_score})...\")\n",
    "        best_video_path = f\"{recordings_dir}/BEST_episode_{metrics.best_episode_number:05d}_score_{metrics.best_episode_score}.mp4\"\n",
    "        record_episode(env, get_state, Q, 0.0, metrics.best_episode_number, \n",
    "                      agent_name, seed, best_video_path)\n",
    "    \n",
    "    # Final statistics\n",
    "    total_time = metrics.get_training_duration()\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Training Complete!\")\n",
    "    print(f\"  Total time: {total_time/60:.2f} minutes\")\n",
    "    print(f\"  Avg time per episode: {metrics.get_avg_episode_time()*1000:.2f}ms\")\n",
    "    print(f\"  Best score: {max(metrics.episode_scores)}\")\n",
    "    print(f\"  Final avg score (last 100): {np.mean(metrics.episode_scores[-100:]):.2f}\")\n",
    "    print(f\"  Total states explored: {len(Q)}\")\n",
    "    print(f\"  Convergence milestones: {metrics.episodes_to_threshold}\")\n",
    "    if record_gameplay:\n",
    "        print(f\"  Recorded episodes: {len(metrics.recorded_episodes)}\")\n",
    "        print(f\"  Videos saved to: {recordings_dir}/\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Save Q-table and metrics\n",
    "    with open(f'q_table_seed{seed}.pkl', 'wb') as f:\n",
    "        pickle.dump(Q, f)\n",
    "    \n",
    "    metrics.save(f'metrics_{agent_name.lower().replace(\" \", \"_\")}_seed{seed}.pkl')\n",
    "    \n",
    "    # üé¨ Record final greedy gameplay after full training\n",
    "    print(\"\\nüé¨ Recording final gameplay with trained Q-Learning agent...\")\n",
    "    env = SnakeGame()\n",
    "    final_video_path = f\"{recordings_dir}/FINAL_gameplay_seed{seed}.mp4\"\n",
    "    record_episode(\n",
    "        env, get_state, Q,\n",
    "        epsilon=0.0,                     # no exploration\n",
    "        episode_num=num_episodes,        # tag as final episode\n",
    "        agent_name=\"Q-Learning-Final\",\n",
    "        seed=seed,\n",
    "        video_path=final_video_path\n",
    "    )\n",
    "    print(f\"‚úÖ Final gameplay recording complete! Saved to {final_video_path}\")\n",
    "    \n",
    "    return Q, metrics\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN EXECUTION\n",
    "# =============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Train with multiple seeds and automatic gameplay recording\n",
    "    seeds = [42, 123, 456]\n",
    "    \n",
    "    all_metrics = []\n",
    "    \n",
    "    for seed in seeds:\n",
    "        Q, metrics = train_q_learning(\n",
    "            seed=seed, \n",
    "            num_episodes=2000, \n",
    "            agent_name=\"Q-Learning\",\n",
    "            record_gameplay=True,  # Enable automatic recording\n",
    "            record_frequency=500   # Record every 500 episodes\n",
    "        )\n",
    "        all_metrics.append(metrics)\n",
    "        \n",
    "        # Small break between runs\n",
    "        time.sleep(2)\n",
    "    \n",
    "    print(\"\\nüéâ All training runs complete!\")\n",
    "    print(f\"Metrics saved for {len(seeds)} different seeds\")\n",
    "    print(\"Check the recordings_* directories for gameplay videos!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deecac3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "VIDEO COMPILATION CREATOR\n",
      "================================================================================\n",
      "\n",
      "Found 5 recorded episodes\n",
      "Creating progression video: progression_seed42.mp4\n",
      "  Processing episode_00200.mp4...\n",
      "  Processing episode_00400.mp4...\n",
      "  Processing episode_00600.mp4...\n",
      "  Processing episode_00800.mp4...\n",
      "  Processing episode_01000.mp4...\n",
      "‚úÖ Compilation video saved: progression_seed42.mp4\n",
      "Found 5 recorded episodes\n",
      "Creating progression video: progression_seed123.mp4\n",
      "  Processing episode_00200.mp4...\n",
      "  Processing episode_00400.mp4...\n",
      "  Processing episode_00600.mp4...\n",
      "  Processing episode_00800.mp4...\n",
      "  Processing episode_01000.mp4...\n",
      "‚úÖ Compilation video saved: progression_seed123.mp4\n",
      "Found 5 recorded episodes\n",
      "Creating progression video: progression_seed456.mp4\n",
      "  Processing episode_00200.mp4...\n",
      "  Processing episode_00400.mp4...\n",
      "  Processing episode_00600.mp4...\n",
      "  Processing episode_00800.mp4...\n",
      "  Processing episode_01000.mp4...\n",
      "‚úÖ Compilation video saved: progression_seed456.mp4\n",
      "Creating best episodes compilation for Q-Learning...\n",
      "  Adding BEST_episode_00340_score_13.mp4...\n",
      "  Adding BEST_episode_00471_score_21.mp4...\n",
      "  Adding BEST_episode_00717_score_18.mp4...\n",
      "‚úÖ Best episodes compilation saved: best_episodes_q-learning.mp4\n",
      "\n",
      "üéâ All compilation videos created!\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# =========================\n",
    "# Safe text overlay\n",
    "# =========================\n",
    "def safe_put_text(img, text, start_x, start_y, font_scale=0.4,\n",
    "                  color=(255,255,255), thickness=1, line_height=15, max_chars_per_line=30):\n",
    "    \"\"\"\n",
    "    Safely put text on a frame (ensures contiguous array and correct type)\n",
    "    \"\"\"\n",
    "    # Ensure img is contiguous and uint8\n",
    "    img = np.ascontiguousarray(img, dtype=np.uint8)\n",
    "\n",
    "    # Split long text into lines\n",
    "    lines = [text[i:i+max_chars_per_line] for i in range(0, len(text), max_chars_per_line)]\n",
    "    for i, line in enumerate(lines):\n",
    "        y = int(start_y + i*line_height)\n",
    "        cv2.putText(img, line, (int(start_x), y),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, font_scale, color, thickness, cv2.LINE_AA)\n",
    "    return img\n",
    "\n",
    "# =========================\n",
    "# Training progression video\n",
    "# =========================\n",
    "def create_training_progression_video(recordings_dir, output_path='training_progression.mp4'):\n",
    "    \"\"\"\n",
    "    Create a compilation showing training progression\n",
    "    \"\"\"\n",
    "    video_files = sorted(Path(recordings_dir).glob(\"episode_*.mp4\"))\n",
    "    \n",
    "    if not video_files:\n",
    "        print(f\"‚ùå No videos found in {recordings_dir}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(video_files)} recorded episodes\")\n",
    "    print(f\"Creating progression video: {output_path}\")\n",
    "    \n",
    "    # Read first video to get dimensions and FPS\n",
    "    first_video = cv2.VideoCapture(str(video_files[0]))\n",
    "    width = int(first_video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(first_video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = int(first_video.get(cv2.CAP_PROP_FPS))\n",
    "    first_video.release()\n",
    "    \n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    output_video = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "    \n",
    "    for video_file in video_files:\n",
    "        print(f\"  Processing {video_file.name}...\")\n",
    "        cap = cv2.VideoCapture(str(video_file))\n",
    "        \n",
    "        # Safe title frame\n",
    "        title_frame = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "        title_frame = np.ascontiguousarray(title_frame, dtype=np.uint8)\n",
    "        \n",
    "        episode_number = video_file.stem.split('_')[1]\n",
    "        text = f\"Episode {episode_number}\"\n",
    "        \n",
    "        # Center text\n",
    "        text_size = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.8, 2)[0]\n",
    "        text_x = (width - text_size[0]) // 2\n",
    "        text_y = (height + text_size[1]) // 2\n",
    "        \n",
    "        # Draw text safely\n",
    "        title_frame = safe_put_text(title_frame, text, text_x, text_y, font_scale=0.8, thickness=2, max_chars_per_line=50)\n",
    "        \n",
    "        # Write title frame for 1 second\n",
    "        for _ in range(fps):\n",
    "            output_video.write(title_frame)\n",
    "        \n",
    "        # Write all frames from this episode\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame = np.ascontiguousarray(frame, dtype=np.uint8)\n",
    "            output_video.write(frame)\n",
    "        \n",
    "        cap.release()\n",
    "        \n",
    "        # Black separator for 0.5 seconds\n",
    "        black_frame = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "        black_frame = np.ascontiguousarray(black_frame, dtype=np.uint8)\n",
    "        for _ in range(fps // 2):\n",
    "            output_video.write(black_frame)\n",
    "    \n",
    "    output_video.release()\n",
    "    print(f\"‚úÖ Compilation video saved: {output_path}\")\n",
    "\n",
    "# =========================\n",
    "# Best episodes compilation\n",
    "# =========================\n",
    "def create_best_episodes_compilation(seed_list=[42, 123, 456], agent_name=\"Q-Learning\"):\n",
    "    output_path = f'best_episodes_{agent_name.lower().replace(\" \", \"_\")}.mp4'\n",
    "    print(f\"Creating best episodes compilation for {agent_name}...\")\n",
    "\n",
    "    best_videos = []\n",
    "    for seed in seed_list:\n",
    "        recordings_dir = f\"recordings_{agent_name.lower().replace(' ', '_')}_seed{seed}\"\n",
    "        best_video = list(Path(recordings_dir).glob(\"BEST_*.mp4\"))\n",
    "        if best_video:\n",
    "            best_videos.extend(best_video)\n",
    "\n",
    "    if not best_videos:\n",
    "        print(\"‚ùå No best episode videos found\")\n",
    "        return\n",
    "\n",
    "    # Get dimensions\n",
    "    cap = cv2.VideoCapture(str(best_videos[0]))\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    cap.release()\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    output_video = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "    for video_file in best_videos:\n",
    "        print(f\"  Adding {video_file.name}...\")\n",
    "\n",
    "        # Safe title frame\n",
    "        title_frame = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "        title_frame = np.ascontiguousarray(title_frame, dtype=np.uint8)\n",
    "        title_text = video_file.stem.replace('_', ' ').title()\n",
    "        title_frame = safe_put_text(title_frame, title_text, width//8, height//2, font_scale=0.6, thickness=2)\n",
    "\n",
    "        for _ in range(fps):\n",
    "            output_video.write(title_frame)\n",
    "\n",
    "        # Video frames\n",
    "        cap = cv2.VideoCapture(str(video_file))\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame = np.ascontiguousarray(frame, dtype=np.uint8)\n",
    "            output_video.write(frame)\n",
    "        cap.release()\n",
    "\n",
    "        # Black separator for 1 second\n",
    "        black_frame = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "        black_frame = np.ascontiguousarray(black_frame, dtype=np.uint8)\n",
    "        for _ in range(fps):\n",
    "            output_video.write(black_frame)\n",
    "\n",
    "    output_video.release()\n",
    "    print(f\"‚úÖ Best episodes compilation saved: {output_path}\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# MAIN EXECUTION\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"VIDEO COMPILATION CREATOR\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "    # Create progression videos for each seed\n",
    "    for seed in [42, 123, 456]:\n",
    "        recordings_dir = f\"recordings_q-learning_seed{seed}\"\n",
    "        if Path(recordings_dir).exists():\n",
    "            create_training_progression_video(recordings_dir, f'progression_seed{seed}.mp4')\n",
    "\n",
    "    # Create best episodes compilation\n",
    "    create_best_episodes_compilation([42, 123, 456], \"Q-Learning\")\n",
    "\n",
    "    print(\"\\nüéâ All compilation videos created!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8c255a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "COMPREHENSIVE METRICS VISUALIZATION\n",
      "================================================================================\n",
      "\n",
      "Loading metrics files...\n",
      "‚úÖ Loaded 3 metric files\n",
      "\n",
      "Generating visualizations...\n",
      "\n",
      "‚úÖ Saved: 1_sample_efficiency.png\n",
      "‚úÖ Saved: 2_exploration_stability.png\n",
      "‚úÖ Saved: 3_computational_efficiency.png\n",
      "‚úÖ Saved: 4_convergence_stability.png\n",
      "‚úÖ Saved: 5_policy_behavior.png\n",
      "\n",
      "Generating summary report...\n",
      "\n",
      "‚úÖ Saved: summary_report.txt\n",
      "\n",
      "================================================================================\n",
      "COMPREHENSIVE BENCHMARKING REPORT: Q-Learning\n",
      "================================================================================\n",
      "\n",
      "1. SAMPLE EFFICIENCY ANALYSIS\n",
      "--------------------------------------------------------------------------------\n",
      "  Seed 123:\n",
      "    - Final Avg Score (last 100): 1.12\n",
      "    - Best Score: 21\n",
      "    - Episodes to 80% max: 1\n",
      "  Seed 42:\n",
      "    - Final Avg Score (last 100): 0.46\n",
      "    - Best Score: 13\n",
      "    - Episodes to 80% max: 1\n",
      "  Seed 456:\n",
      "    - Final Avg Score (last 100): 0.68\n",
      "    - Best Score: 18\n",
      "    - Episodes to 80% max: 55\n",
      "  Aggregate Statistics:\n",
      "    - Mean final score: 0.75 ¬± 0.27\n",
      "    - Mean best score: 17.33 ¬± 3.30\n",
      "\n",
      "2. EXPLORATION & STABILITY ANALYSIS\n",
      "--------------------------------------------------------------------------------\n",
      "  Seed 123:\n",
      "    - Average reward variance: 2437.00\n",
      "    - Final epsilon: 0.0100\n",
      "  Seed 42:\n",
      "    - Average reward variance: 1983.98\n",
      "    - Final epsilon: 0.0100\n",
      "  Seed 456:\n",
      "    - Average reward variance: 2201.71\n",
      "    - Final epsilon: 0.0100\n",
      "\n",
      "3. COMPUTATIONAL EFFICIENCY\n",
      "--------------------------------------------------------------------------------\n",
      "  Training Duration:\n",
      "    - Mean: 0.00 minutes\n",
      "    - Std: 0.00 minutes\n",
      "  Episode Duration:\n",
      "    - Mean: 0.00 ms/episode\n",
      "    - Std: 0.00 ms/episode\n",
      "  Resource Usage:\n",
      "    - Mean CPU usage: 26.2%\n",
      "    - Mean Memory usage: 84.7%\n",
      "\n",
      "4. CONVERGENCE & STABILITY\n",
      "--------------------------------------------------------------------------------\n",
      "  Cross-seed variability:\n",
      "    - Coefficient of Variation (CV): 36.42%\n",
      "    - Interpretation: VARIABLE (CV ‚â• 20%)\n",
      "\n",
      "5. POLICY BEHAVIOR EVOLUTION\n",
      "--------------------------------------------------------------------------------\n",
      "  Seed 123:\n",
      "    - Early score (first 500): 1.49\n",
      "    - Late score (last 500): 1.01\n",
      "    - Improvement: -32.1%\n",
      "    - Early survival: 36.1 steps\n",
      "    - Late survival: 63.1 steps\n",
      "  Seed 42:\n",
      "    - Early score (first 500): 1.36\n",
      "    - Late score (last 500): 0.91\n",
      "    - Improvement: -32.7%\n",
      "    - Early survival: 35.4 steps\n",
      "    - Late survival: 61.0 steps\n",
      "  Seed 456:\n",
      "    - Early score (first 500): 1.35\n",
      "    - Late score (last 500): 0.93\n",
      "    - Improvement: -30.9%\n",
      "    - Early survival: 36.6 steps\n",
      "    - Late survival: 61.1 steps\n",
      "\n",
      "================================================================================\n",
      "END OF REPORT\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "üéâ ALL VISUALIZATIONS COMPLETE!\n",
      "================================================================================\n",
      "\n",
      "Generated files:\n",
      "  1. 1_sample_efficiency.png\n",
      "  2. 2_exploration_stability.png\n",
      "  3. 3_computational_efficiency.png\n",
      "  4. 4_convergence_stability.png\n",
      "  5. 5_policy_behavior.png\n",
      "  6. summary_report.txt\n",
      "\n",
      "These visualizations are ready for your paper/presentation!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# =============================\n",
    "# Self-contained MetricsLogger\n",
    "# =============================\n",
    "class MetricsLogger:\n",
    "    def __init__(self, agent_name, seed):\n",
    "        self.agent_name = agent_name\n",
    "        self.seed = seed\n",
    "        \n",
    "        # Placeholders for metrics (fill with actual data from pickle)\n",
    "        self.episode_scores = []\n",
    "        self.moving_avg_reward = []\n",
    "        self.moving_avg_score = []\n",
    "        self.episodes_to_threshold = {}\n",
    "        self.epsilon_history = []\n",
    "        self.reward_variance_per_block = []\n",
    "        self.score_variance_per_block = []\n",
    "        self.action_counts = {0:0, 1:0, 2:0, 3:0}\n",
    "        self.episode_lengths = []\n",
    "        self.cpu_usage = []\n",
    "        self.memory_usage = []\n",
    "        self.episode_rewards = []\n",
    "\n",
    "    def get_training_duration(self):\n",
    "        return getattr(self, 'training_duration', 0)\n",
    "\n",
    "    def get_avg_episode_time(self):\n",
    "        return getattr(self, 'avg_episode_time', 0)\n",
    "\n",
    "# =============================\n",
    "# Load metrics\n",
    "# =============================\n",
    "def load_metrics(pattern=\"metrics_q-learning_seed*.pkl\"):\n",
    "    metrics_list = []\n",
    "    for file in Path('.').glob(pattern):\n",
    "        with open(file, 'rb') as f:\n",
    "            metrics = pickle.load(f)\n",
    "            m = MetricsLogger(metrics['agent_name'], metrics['seed'])\n",
    "            m.__dict__.update(metrics)\n",
    "            metrics_list.append(m)\n",
    "    return metrics_list\n",
    "\n",
    "\n",
    "def plot_sample_efficiency(metrics_list, save_path='1_sample_efficiency.png'):\n",
    "    \"\"\"\n",
    "    METRIC 1: Sample Efficiency Analysis\n",
    "    Shows how fast each agent learns\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('1. Sample Efficiency Analysis', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    colors = ['#2E86AB', '#A23B72', '#F18F01']\n",
    "    \n",
    "    # Plot 1: Reward vs Episodes (all seeds)\n",
    "    ax = axes[0, 0]\n",
    "    for i, m in enumerate(metrics_list):\n",
    "        episodes = range(1, len(m.moving_avg_reward) + 1)\n",
    "        ax.plot(episodes, m.moving_avg_reward, \n",
    "               label=f'Seed {m.seed}', alpha=0.7, color=colors[i % len(colors)])\n",
    "    ax.set_xlabel('Episode')\n",
    "    ax.set_ylabel('Moving Avg Reward (window=100)')\n",
    "    ax.set_title('Learning Curves Across Seeds')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Score vs Episodes (all seeds)\n",
    "    ax = axes[0, 1]\n",
    "    for i, m in enumerate(metrics_list):\n",
    "        episodes = range(1, len(m.moving_avg_score) + 1)\n",
    "        ax.plot(episodes, m.moving_avg_score,\n",
    "               label=f'Seed {m.seed}', alpha=0.7, color=colors[i % len(colors)])\n",
    "    ax.set_xlabel('Episode')\n",
    "    ax.set_ylabel('Moving Avg Score (window=100)')\n",
    "    ax.set_title('Score Progression Across Seeds')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Episodes to Convergence\n",
    "    ax = axes[1, 0]\n",
    "    thresholds = ['50%_max', '80%_max', '90%_max']\n",
    "    threshold_data = {t: [] for t in thresholds}\n",
    "    \n",
    "    for m in metrics_list:\n",
    "        for t in thresholds:\n",
    "            value = m.episodes_to_threshold.get(t, np.nan)\n",
    "            threshold_data[t].append(value)\n",
    "    \n",
    "    x_pos = np.arange(len(thresholds))\n",
    "    means = [np.nanmean(threshold_data[t]) for t in thresholds]\n",
    "    stds = [np.nanstd(threshold_data[t]) for t in thresholds]\n",
    "    \n",
    "    ax.bar(x_pos, means, yerr=stds, capsize=5, color='#2E86AB', alpha=0.7)\n",
    "    ax.set_xticks(x_pos)\n",
    "    ax.set_xticklabels(thresholds)\n",
    "    ax.set_ylabel('Episodes Required')\n",
    "    ax.set_title('Sample Efficiency: Episodes to Reach Performance Thresholds')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Plot 4: Cumulative max score\n",
    "    ax = axes[1, 1]\n",
    "    for i, m in enumerate(metrics_list):\n",
    "        cumulative_max = np.maximum.accumulate(m.episode_scores)\n",
    "        episodes = range(1, len(cumulative_max) + 1)\n",
    "        ax.plot(episodes, cumulative_max,\n",
    "               label=f'Seed {m.seed}', alpha=0.7, color=colors[i % len(colors)])\n",
    "    ax.set_xlabel('Episode')\n",
    "    ax.set_ylabel('Cumulative Max Score')\n",
    "    ax.set_title('Best Performance Over Time')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"‚úÖ Saved: {save_path}\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_exploration_stability(metrics_list, save_path='2_exploration_stability.png'):\n",
    "    \"\"\"\n",
    "    METRIC 2: Exploration Stability\n",
    "    Shows how exploration affects learning stability\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('2. Exploration & Behavioral Stability', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    colors = ['#2E86AB', '#A23B72', '#F18F01']\n",
    "    \n",
    "    # Plot 1: Epsilon decay with reward overlay\n",
    "    ax = axes[0, 0]\n",
    "    ax2 = ax.twinx()\n",
    "    \n",
    "    for i, m in enumerate(metrics_list):\n",
    "        episodes = range(1, len(m.epsilon_history) + 1)\n",
    "        ax.plot(episodes, m.epsilon_history,\n",
    "               label=f'Œµ (Seed {m.seed})', alpha=0.5, linestyle='--', color=colors[i % len(colors)])\n",
    "        ax2.plot(episodes, m.moving_avg_reward,\n",
    "                label=f'Reward (Seed {m.seed})', alpha=0.7, color=colors[i % len(colors)])\n",
    "    \n",
    "    ax.set_xlabel('Episode')\n",
    "    ax.set_ylabel('Epsilon (Exploration Rate)', color='black')\n",
    "    ax2.set_ylabel('Moving Avg Reward', color='black')\n",
    "    ax.set_title('Exploration Decay vs Learning Performance')\n",
    "    ax.legend(loc='upper left')\n",
    "    ax2.legend(loc='upper right')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Reward variance per block\n",
    "    ax = axes[0, 1]\n",
    "    for i, m in enumerate(metrics_list):\n",
    "        blocks = range(1, len(m.reward_variance_per_block) + 1)\n",
    "        block_episodes = [b * 1000 for b in blocks]\n",
    "        ax.plot(block_episodes, m.reward_variance_per_block,\n",
    "               label=f'Seed {m.seed}', marker='o', alpha=0.7, color=colors[i % len(colors)])\n",
    "    ax.set_xlabel('Episode (√ó1000)')\n",
    "    ax.set_ylabel('Reward Variance')\n",
    "    ax.set_title('Learning Stability: Reward Variance per 1000 Episodes')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Score variance per block\n",
    "    ax = axes[1, 0]\n",
    "    for i, m in enumerate(metrics_list):\n",
    "        blocks = range(1, len(m.score_variance_per_block) + 1)\n",
    "        block_episodes = [b * 1000 for b in blocks]\n",
    "        ax.plot(block_episodes, m.score_variance_per_block,\n",
    "               label=f'Seed {m.seed}', marker='o', alpha=0.7, color=colors[i % len(colors)])\n",
    "    ax.set_xlabel('Episode (√ó1000)')\n",
    "    ax.set_ylabel('Score Variance')\n",
    "    ax.set_title('Performance Stability: Score Variance per 1000 Episodes')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 4: Action distribution (last seed only for clarity)\n",
    "    ax = axes[1, 1]\n",
    "    m = metrics_list[-1]  # Use last seed\n",
    "    action_names = ['UP (0)', 'DOWN (1)', 'LEFT (2)', 'RIGHT (3)']\n",
    "    total_actions = sum(m.action_counts.values())\n",
    "    action_percentages = [m.action_counts[i] / total_actions * 100 for i in range(4)]\n",
    "    \n",
    "    ax.bar(action_names, action_percentages, color='#2E86AB', alpha=0.7)\n",
    "    ax.set_ylabel('Percentage of Total Actions (%)')\n",
    "    ax.set_title(f'Action Distribution (Seed {m.seed})')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"‚úÖ Saved: {save_path}\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_computational_efficiency(metrics_list, save_path='3_computational_efficiency.png'):\n",
    "    \"\"\"\n",
    "    METRIC 3: Runtime / Computational Efficiency\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('3. Computational Efficiency', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    colors = ['#2E86AB', '#A23B72', '#F18F01']\n",
    "    \n",
    "    # Plot 1: Training time comparison\n",
    "    ax = axes[0, 0]\n",
    "    seeds = [m.seed for m in metrics_list]\n",
    "    training_times = [m.get_training_duration() / 60 for m in metrics_list]  # in minutes\n",
    "    \n",
    "    ax.bar(range(len(seeds)), training_times, color='#2E86AB', alpha=0.7)\n",
    "    ax.set_xticks(range(len(seeds)))\n",
    "    ax.set_xticklabels([f'Seed {s}' for s in seeds])\n",
    "    ax.set_ylabel('Training Time (minutes)')\n",
    "    ax.set_title('Total Training Duration')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add mean line\n",
    "    mean_time = np.mean(training_times)\n",
    "    ax.axhline(mean_time, color='red', linestyle='--', label=f'Mean: {mean_time:.2f} min')\n",
    "    ax.legend()\n",
    "    \n",
    "    # Plot 2: Average time per episode\n",
    "    ax = axes[0, 1]\n",
    "    avg_times = [m.get_avg_episode_time() * 1000 for m in metrics_list]  # in ms\n",
    "    \n",
    "    ax.bar(range(len(seeds)), avg_times, color='#A23B72', alpha=0.7)\n",
    "    ax.set_xticks(range(len(seeds)))\n",
    "    ax.set_xticklabels([f'Seed {s}' for s in seeds])\n",
    "    ax.set_ylabel('Time per Episode (ms)')\n",
    "    ax.set_title('Average Episode Duration')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    mean_time_ep = np.mean(avg_times)\n",
    "    ax.axhline(mean_time_ep, color='red', linestyle='--', label=f'Mean: {mean_time_ep:.2f} ms')\n",
    "    ax.legend()\n",
    "    \n",
    "    # Plot 3: CPU usage over time\n",
    "    ax = axes[1, 0]\n",
    "    for i, m in enumerate(metrics_list):\n",
    "        # Sample every 100 episodes for clarity\n",
    "        sampled_episodes = range(0, len(m.cpu_usage), 100)\n",
    "        sampled_cpu = [m.cpu_usage[i] for i in sampled_episodes]\n",
    "        ax.plot(sampled_episodes, sampled_cpu,\n",
    "               label=f'Seed {m.seed}', alpha=0.7, color=colors[i % len(colors)])\n",
    "    ax.set_xlabel('Episode')\n",
    "    ax.set_ylabel('CPU Usage (%)')\n",
    "    ax.set_title('CPU Utilization During Training')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 4: Memory usage over time\n",
    "    ax = axes[1, 1]\n",
    "    for i, m in enumerate(metrics_list):\n",
    "        sampled_episodes = range(0, len(m.memory_usage), 100)\n",
    "        sampled_mem = [m.memory_usage[i] for i in sampled_episodes]\n",
    "        ax.plot(sampled_episodes, sampled_mem,\n",
    "               label=f'Seed {m.seed}', alpha=0.7, color=colors[i % len(colors)])\n",
    "    ax.set_xlabel('Episode')\n",
    "    ax.set_ylabel('Memory Usage (%)')\n",
    "    ax.set_title('Memory Utilization During Training')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"‚úÖ Saved: {save_path}\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_convergence_stability(metrics_list, save_path='4_convergence_stability.png'):\n",
    "    \"\"\"\n",
    "    METRIC 4: Convergence and Stability Visualization\n",
    "    Shows how consistently the agent converges across multiple runs\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('4. Convergence & Stability Analysis', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    colors = ['#2E86AB', '#A23B72', '#F18F01']\n",
    "    \n",
    "    # Plot 1: All reward curves together\n",
    "    ax = axes[0, 0]\n",
    "    for i, m in enumerate(metrics_list):\n",
    "        episodes = range(1, len(m.moving_avg_reward) + 1)\n",
    "        ax.plot(episodes, m.moving_avg_reward,\n",
    "               label=f'Seed {m.seed}', alpha=0.7, linewidth=2, color=colors[i % len(colors)])\n",
    "    \n",
    "    # Add mean and std dev band\n",
    "    max_len = max(len(m.moving_avg_reward) for m in metrics_list)\n",
    "    all_rewards = np.zeros((len(metrics_list), max_len))\n",
    "    for i, m in enumerate(metrics_list):\n",
    "        all_rewards[i, :len(m.moving_avg_reward)] = m.moving_avg_reward\n",
    "        if len(m.moving_avg_reward) < max_len:\n",
    "            all_rewards[i, len(m.moving_avg_reward):] = m.moving_avg_reward[-1]\n",
    "    \n",
    "    mean_reward = np.mean(all_rewards, axis=0)\n",
    "    std_reward = np.std(all_rewards, axis=0)\n",
    "    episodes = range(1, max_len + 1)\n",
    "    \n",
    "    ax.fill_between(episodes, mean_reward - std_reward, mean_reward + std_reward,\n",
    "                    alpha=0.2, color='gray', label='¬±1 Std Dev')\n",
    "    ax.plot(episodes, mean_reward, 'k--', linewidth=2, label='Mean')\n",
    "    \n",
    "    ax.set_xlabel('Episode')\n",
    "    ax.set_ylabel('Moving Avg Reward')\n",
    "    ax.set_title('Reward Convergence Across Multiple Seeds')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: All score curves together\n",
    "    ax = axes[0, 1]\n",
    "    for i, m in enumerate(metrics_list):\n",
    "        episodes = range(1, len(m.moving_avg_score) + 1)\n",
    "        ax.plot(episodes, m.moving_avg_score,\n",
    "               label=f'Seed {m.seed}', alpha=0.7, linewidth=2, color=colors[i % len(colors)])\n",
    "    \n",
    "    # Add mean and std dev band\n",
    "    all_scores = np.zeros((len(metrics_list), max_len))\n",
    "    for i, m in enumerate(metrics_list):\n",
    "        all_scores[i, :len(m.moving_avg_score)] = m.moving_avg_score\n",
    "        if len(m.moving_avg_score) < max_len:\n",
    "            all_scores[i, len(m.moving_avg_score):] = m.moving_avg_score[-1]\n",
    "    \n",
    "    mean_score = np.mean(all_scores, axis=0)\n",
    "    std_score = np.std(all_scores, axis=0)\n",
    "    \n",
    "    ax.fill_between(episodes, mean_score - std_score, mean_score + std_score,\n",
    "                    alpha=0.2, color='gray', label='¬±1 Std Dev')\n",
    "    ax.plot(episodes, mean_score, 'k--', linewidth=2, label='Mean')\n",
    "    \n",
    "    ax.set_xlabel('Episode')\n",
    "    ax.set_ylabel('Moving Avg Score')\n",
    "    ax.set_title('Score Convergence Across Multiple Seeds')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Variance over time\n",
    "    ax = axes[1, 0]\n",
    "    \n",
    "    window = 100\n",
    "    variance_over_time = []\n",
    "    for i in range(0, max_len, window):\n",
    "        window_scores = all_scores[:, i:min(i+window, max_len)]\n",
    "        variance_over_time.append(np.mean(np.var(window_scores, axis=0)))\n",
    "    \n",
    "    episodes_binned = range(window, max_len + 1, window)\n",
    "    ax.plot(episodes_binned, variance_over_time, marker='o', color='#2E86AB', linewidth=2)\n",
    "    ax.set_xlabel('Episode')\n",
    "    ax.set_ylabel(f'Mean Variance (per {window} episodes)')\n",
    "    ax.set_title('Stability: Inter-Seed Variance Over Training')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 4: Final performance comparison\n",
    "    ax = axes[1, 1]\n",
    "    \n",
    "    final_scores = [np.mean(m.episode_scores[-100:]) for m in metrics_list]\n",
    "    final_rewards = [np.mean(m.episode_rewards[-100:]) for m in metrics_list]\n",
    "    seeds = [m.seed for m in metrics_list]\n",
    "    \n",
    "    x_pos = np.arange(len(seeds))\n",
    "    width = 0.35\n",
    "    \n",
    "    ax.bar(x_pos - width/2, final_scores, width, label='Avg Score (last 100)', color='#2E86AB', alpha=0.7)\n",
    "    ax.bar(x_pos + width/2, final_rewards, width, label='Avg Reward (last 100)', color='#F18F01', alpha=0.7)\n",
    "    \n",
    "    ax.set_xticks(x_pos)\n",
    "    ax.set_xticklabels([f'Seed {s}' for s in seeds])\n",
    "    ax.set_ylabel('Performance')\n",
    "    ax.set_title('Final Performance Comparison')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"‚úÖ Saved: {save_path}\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_policy_behavior(metrics_list, save_path='5_policy_behavior.png'):\n",
    "    \"\"\"\n",
    "    METRIC 5: Policy Behavior Check\n",
    "    Shows how the policy evolves over time\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle\n",
    "    fig.suptitle('5. Policy Behavior & Evolution', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    colors = ['#2E86AB', '#A23B72', '#F18F01']\n",
    "    \n",
    "    # Plot 1: Average survival time over training\n",
    "    ax = axes[0, 0]\n",
    "    for i, m in enumerate(metrics_list):\n",
    "        # Calculate moving average of episode lengths (survival time)\n",
    "        window = 100\n",
    "        if len(m.episode_lengths) >= window:\n",
    "            moving_avg_length = [np.mean(m.episode_lengths[max(0, j-window):j+1]) \n",
    "                                for j in range(len(m.episode_lengths))]\n",
    "        else:\n",
    "            moving_avg_length = m.episode_lengths\n",
    "        \n",
    "        episodes = range(1, len(moving_avg_length) + 1)\n",
    "        ax.plot(episodes, moving_avg_length,\n",
    "               label=f'Seed {m.seed}', alpha=0.7, color=colors[i % len(colors)])\n",
    "    \n",
    "    ax.set_xlabel('Episode')\n",
    "    ax.set_ylabel('Average Steps Survived (window=100)')\n",
    "    ax.set_title('Policy Evolution: Survival Time Over Training')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Early vs Late behavior comparison\n",
    "    ax = axes[0, 1]\n",
    "    \n",
    "    # Compare first 500 episodes vs last 500 episodes\n",
    "    early_window = slice(0, 500)\n",
    "    late_window = slice(-500, None)\n",
    "    \n",
    "    comparison_data = {\n",
    "        'Early Training\\n(Episodes 1-500)': [],\n",
    "        'Late Training\\n(Last 500 episodes)': []\n",
    "    }\n",
    "    \n",
    "    for m in metrics_list:\n",
    "        comparison_data['Early Training\\n(Episodes 1-500)'].append(\n",
    "            np.mean(m.episode_scores[early_window])\n",
    "        )\n",
    "        comparison_data['Late Training\\n(Last 500 episodes)'].append(\n",
    "            np.mean(m.episode_scores[late_window])\n",
    "        )\n",
    "    \n",
    "    x_pos = np.arange(len(comparison_data))\n",
    "    means = [np.mean(comparison_data[k]) for k in comparison_data.keys()]\n",
    "    stds = [np.std(comparison_data[k]) for k in comparison_data.keys()]\n",
    "    \n",
    "    ax.bar(x_pos, means, yerr=stds, capsize=5, color=['#E63946', '#06FFA5'], alpha=0.7)\n",
    "    ax.set_xticks(x_pos)\n",
    "    ax.set_xticklabels(comparison_data.keys())\n",
    "    ax.set_ylabel('Average Score')\n",
    "    ax.set_title('Behavioral Shift: Early vs Late Training')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add significance annotation\n",
    "    improvement = ((means[1] - means[0]) / means[0] * 100) if means[0] > 0 else 0\n",
    "    ax.text(0.5, max(means) * 0.9, f'+{improvement:.1f}% improvement', \n",
    "           ha='center', fontsize=12, fontweight='bold',\n",
    "           bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "    \n",
    "    # Plot 3: Action distribution evolution\n",
    "    ax = axes[1, 0]\n",
    "    \n",
    "    # Use first seed for detailed analysis\n",
    "    m = metrics_list[0]\n",
    "    \n",
    "    # Divide training into 4 phases\n",
    "    num_episodes = len(m.episode_scores)\n",
    "    phase_size = num_episodes // 4\n",
    "    phases = ['Early\\n(0-25%)', 'Mid-Early\\n(25-50%)', 'Mid-Late\\n(50-75%)', 'Late\\n(75-100%)']\n",
    "    \n",
    "    # We need to reconstruct action distribution per phase\n",
    "    # Since we only have total counts, we'll show the final distribution\n",
    "    # and note that this is a limitation\n",
    "    \n",
    "    action_names = ['UP', 'DOWN', 'LEFT', 'RIGHT']\n",
    "    total_actions = sum(m.action_counts.values())\n",
    "    action_percentages = [m.action_counts[i] / total_actions * 100 for i in range(4)]\n",
    "    \n",
    "    colors_actions = ['#E63946', '#F1FAEE', '#A8DADC', '#457B9D']\n",
    "    ax.bar(action_names, action_percentages, color=colors_actions, alpha=0.7)\n",
    "    ax.set_ylabel('Percentage of Total Actions (%)')\n",
    "    ax.set_title(f'Action Distribution (Seed {m.seed})')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add balanced line\n",
    "    ax.axhline(25, color='red', linestyle='--', linewidth=1, label='Balanced (25%)')\n",
    "    ax.legend()\n",
    "    \n",
    "    # Plot 4: Performance consistency (success rate per block)\n",
    "    ax = axes[1, 1]\n",
    "    \n",
    "    # Calculate \"success rate\" as percentage of episodes with score > 0\n",
    "    block_size = 500\n",
    "    \n",
    "    for i, m in enumerate(metrics_list):\n",
    "        success_rates = []\n",
    "        block_episodes = []\n",
    "        \n",
    "        for start in range(0, len(m.episode_scores), block_size):\n",
    "            end = min(start + block_size, len(m.episode_scores))\n",
    "            block_scores = m.episode_scores[start:end]\n",
    "            success_rate = sum(1 for s in block_scores if s > 0) / len(block_scores) * 100\n",
    "            success_rates.append(success_rate)\n",
    "            block_episodes.append(start + block_size // 2)\n",
    "        \n",
    "        ax.plot(block_episodes, success_rates,\n",
    "               label=f'Seed {m.seed}', marker='o', alpha=0.7, color=colors[i % len(colors)])\n",
    "    \n",
    "    ax.set_xlabel('Episode')\n",
    "    ax.set_ylabel('Success Rate (% with score > 0)')\n",
    "    ax.set_title(f'Learning Progress: Success Rate per {block_size} Episodes')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_ylim([0, 105])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"‚úÖ Saved: {save_path}\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def generate_summary_report(metrics_list, agent_name=\"Q-Learning\", save_path='summary_report.txt'):\n",
    "    \"\"\"\n",
    "    Generate a comprehensive text summary of all metrics\n",
    "    \"\"\"\n",
    "    report = []\n",
    "    report.append(\"=\"*80)\n",
    "    report.append(f\"COMPREHENSIVE BENCHMARKING REPORT: {agent_name}\")\n",
    "    report.append(\"=\"*80)\n",
    "    report.append(\"\")\n",
    "    \n",
    "    # 1. Sample Efficiency\n",
    "    report.append(\"1. SAMPLE EFFICIENCY ANALYSIS\")\n",
    "    report.append(\"-\" * 80)\n",
    "    \n",
    "    for m in metrics_list:\n",
    "        report.append(f\"  Seed {m.seed}:\")\n",
    "        report.append(f\"    - Final Avg Score (last 100): {np.mean(m.episode_scores[-100:]):.2f}\")\n",
    "        report.append(f\"    - Best Score: {max(m.episode_scores)}\")\n",
    "        report.append(f\"    - Episodes to 80% max: {m.episodes_to_threshold.get('80%_max', 'N/A')}\")\n",
    "    \n",
    "    # Calculate aggregate statistics\n",
    "    final_scores = [np.mean(m.episode_scores[-100:]) for m in metrics_list]\n",
    "    best_scores = [max(m.episode_scores) for m in metrics_list]\n",
    "    \n",
    "    report.append(f\"  Aggregate Statistics:\")\n",
    "    report.append(f\"    - Mean final score: {np.mean(final_scores):.2f} ¬± {np.std(final_scores):.2f}\")\n",
    "    report.append(f\"    - Mean best score: {np.mean(best_scores):.2f} ¬± {np.std(best_scores):.2f}\")\n",
    "    report.append(\"\")\n",
    "    \n",
    "    # 2. Exploration Stability\n",
    "    report.append(\"2. EXPLORATION & STABILITY ANALYSIS\")\n",
    "    report.append(\"-\" * 80)\n",
    "    \n",
    "    for m in metrics_list:\n",
    "        if m.reward_variance_per_block:\n",
    "            avg_variance = np.mean(m.reward_variance_per_block)\n",
    "            report.append(f\"  Seed {m.seed}:\")\n",
    "            report.append(f\"    - Average reward variance: {avg_variance:.2f}\")\n",
    "            report.append(f\"    - Final epsilon: {m.epsilon_history[-1]:.4f}\")\n",
    "    \n",
    "    report.append(\"\")\n",
    "    \n",
    "    # 3. Computational Efficiency\n",
    "    report.append(\"3. COMPUTATIONAL EFFICIENCY\")\n",
    "    report.append(\"-\" * 80)\n",
    "    \n",
    "    training_times = [m.get_training_duration() / 60 for m in metrics_list]\n",
    "    episode_times = [m.get_avg_episode_time() * 1000 for m in metrics_list]\n",
    "    \n",
    "    report.append(f\"  Training Duration:\")\n",
    "    report.append(f\"    - Mean: {np.mean(training_times):.2f} minutes\")\n",
    "    report.append(f\"    - Std: {np.std(training_times):.2f} minutes\")\n",
    "    report.append(f\"  Episode Duration:\")\n",
    "    report.append(f\"    - Mean: {np.mean(episode_times):.2f} ms/episode\")\n",
    "    report.append(f\"    - Std: {np.std(episode_times):.2f} ms/episode\")\n",
    "    \n",
    "    # CPU and Memory\n",
    "    avg_cpu = [np.mean(m.cpu_usage) for m in metrics_list]\n",
    "    avg_memory = [np.mean(m.memory_usage) for m in metrics_list]\n",
    "    \n",
    "    report.append(f\"  Resource Usage:\")\n",
    "    report.append(f\"    - Mean CPU usage: {np.mean(avg_cpu):.1f}%\")\n",
    "    report.append(f\"    - Mean Memory usage: {np.mean(avg_memory):.1f}%\")\n",
    "    report.append(\"\")\n",
    "    \n",
    "    # 4. Convergence Stability\n",
    "    report.append(\"4. CONVERGENCE & STABILITY\")\n",
    "    report.append(\"-\" * 80)\n",
    "    \n",
    "    # Calculate coefficient of variation across seeds\n",
    "    if len(metrics_list) > 1:\n",
    "        final_score_cv = np.std(final_scores) / np.mean(final_scores) * 100\n",
    "        report.append(f\"  Cross-seed variability:\")\n",
    "        report.append(f\"    - Coefficient of Variation (CV): {final_score_cv:.2f}%\")\n",
    "        \n",
    "        if final_score_cv < 10:\n",
    "            report.append(f\"    - Interpretation: HIGHLY STABLE (CV < 10%)\")\n",
    "        elif final_score_cv < 20:\n",
    "            report.append(f\"    - Interpretation: MODERATELY STABLE (10% ‚â§ CV < 20%)\")\n",
    "        else:\n",
    "            report.append(f\"    - Interpretation: VARIABLE (CV ‚â• 20%)\")\n",
    "    \n",
    "    report.append(\"\")\n",
    "    \n",
    "    # 5. Policy Behavior\n",
    "    report.append(\"5. POLICY BEHAVIOR EVOLUTION\")\n",
    "    report.append(\"-\" * 80)\n",
    "    \n",
    "    for m in metrics_list:\n",
    "        early_score = np.mean(m.episode_scores[:500])\n",
    "        late_score = np.mean(m.episode_scores[-500:])\n",
    "        improvement = ((late_score - early_score) / early_score * 100) if early_score > 0 else 0\n",
    "        \n",
    "        early_survival = np.mean(m.episode_lengths[:500])\n",
    "        late_survival = np.mean(m.episode_lengths[-500:])\n",
    "        \n",
    "        report.append(f\"  Seed {m.seed}:\")\n",
    "        report.append(f\"    - Early score (first 500): {early_score:.2f}\")\n",
    "        report.append(f\"    - Late score (last 500): {late_score:.2f}\")\n",
    "        report.append(f\"    - Improvement: {improvement:.1f}%\")\n",
    "        report.append(f\"    - Early survival: {early_survival:.1f} steps\")\n",
    "        report.append(f\"    - Late survival: {late_survival:.1f} steps\")\n",
    "    \n",
    "    report.append(\"\")\n",
    "    report.append(\"=\"*80)\n",
    "    report.append(\"END OF REPORT\")\n",
    "    report.append(\"=\"*80)\n",
    "    \n",
    "    # Save report\n",
    "    report_text = \"\\n\".join(report)\n",
    "    with open(save_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(report_text)\n",
    "    \n",
    "    print(f\"‚úÖ Saved: {save_path}\")\n",
    "    print(\"\\n\" + report_text)\n",
    "    \n",
    "    return report_text\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN EXECUTION\n",
    "# =============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"COMPREHENSIVE METRICS VISUALIZATION\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    # Load metrics\n",
    "    print(\"Loading metrics files...\")\n",
    "    metrics_list = load_metrics(\"metrics_q-learning_seed*.pkl\")\n",
    "    \n",
    "    if not metrics_list:\n",
    "        print(\"‚ùå No metrics files found! Run train_with_metrics.py first.\")\n",
    "        exit(1)\n",
    "    \n",
    "    print(f\"‚úÖ Loaded {len(metrics_list)} metric files\\n\")\n",
    "    \n",
    "    # Generate all visualizations\n",
    "    print(\"Generating visualizations...\\n\")\n",
    "    \n",
    "    plot_sample_efficiency(metrics_list, '1_sample_efficiency.png')\n",
    "    plot_exploration_stability(metrics_list, '2_exploration_stability.png')\n",
    "    plot_computational_efficiency(metrics_list, '3_computational_efficiency.png')\n",
    "    plot_convergence_stability(metrics_list, '4_convergence_stability.png')\n",
    "    plot_policy_behavior(metrics_list, '5_policy_behavior.png')\n",
    "    \n",
    "    # Generate summary report\n",
    "    print(\"\\nGenerating summary report...\\n\")\n",
    "    generate_summary_report(metrics_list, agent_name=\"Q-Learning\", save_path='summary_report.txt')\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üéâ ALL VISUALIZATIONS COMPLETE!\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\nGenerated files:\")\n",
    "    print(\"  1. 1_sample_efficiency.png\")\n",
    "    print(\"  2. 2_exploration_stability.png\")\n",
    "    print(\"  3. 3_computational_efficiency.png\")\n",
    "    print(\"  4. 4_convergence_stability.png\")\n",
    "    print(\"  5. 5_policy_behavior.png\")\n",
    "    print(\"  6. summary_report.txt\")\n",
    "    print(\"\\nThese visualizations are ready for your paper/presentation!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc2d753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ...existing code...\n",
    "# def record_episode(env, state_func, Q, epsilon, episode_num, agent_name, seed,\n",
    "#                    video_path, max_steps=1000):\n",
    "#     \"\"\"\n",
    "#     Evaluation recorder: keeps the last valid score even if the env auto-resets on death.\n",
    "#     \"\"\"\n",
    "#     # Reset and size video from first frame\n",
    "#     env.reset()\n",
    "#     _ = state_func(env)\n",
    "\n",
    "#     surf = getattr(env, \"screen\", None) or getattr(env, \"surface\", None)\n",
    "#     assert surf is not None, \"SnakeGame has no screen/surface attribute\"\n",
    "\n",
    "#     first_frame = pygame.surfarray.array3d(surf)\n",
    "#     first_frame = np.transpose(first_frame, (1, 0, 2))\n",
    "#     first_frame = cv2.cvtColor(first_frame, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "#     overlay_h = 50\n",
    "#     h, w, _ = first_frame.shape\n",
    "#     frame_h = h + overlay_h\n",
    "\n",
    "#     fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "#     fps = 15\n",
    "#     vw = cv2.VideoWriter(video_path, fourcc, fps, (w, frame_h))\n",
    "\n",
    "#     state = state_func(env)\n",
    "#     done = False\n",
    "#     steps = 0\n",
    "#     total_reward = 0.0\n",
    "#     actions_taken = []\n",
    "#     last_score = max(0, getattr(env.snake, \"length\", 1) - 1)\n",
    "\n",
    "#     while not done and steps < max_steps:\n",
    "#         if state not in Q:\n",
    "#             Q[state] = {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0}\n",
    "\n",
    "#         # Greedy (tiny exploration cap just in case)\n",
    "#         if random.random() < min(epsilon, 0.1):\n",
    "#             action = random.randint(0, 3)\n",
    "#         else:\n",
    "#             action = int(max(Q[state], key=Q[state].get))\n",
    "\n",
    "#         actions_taken.append(action)\n",
    "\n",
    "#         # Step\n",
    "#         prior_score = max(0, getattr(env.snake, \"length\", 1) - 1)\n",
    "#         _, reward, done, info = env.step(action)\n",
    "#         next_state = state_func(env)\n",
    "#         total_reward += reward\n",
    "\n",
    "#         # Stable score even if env auto-resets\n",
    "#         info_score = info.get(\"score\") if isinstance(info, dict) else None\n",
    "#         cur_score = max(0, getattr(env.snake, \"length\", 1) - 1)\n",
    "#         last_score = max(last_score, prior_score, cur_score if info_score is None else int(info_score))\n",
    "\n",
    "#         # Capture frame (screen/surface fallback)\n",
    "#         surf = getattr(env, \"screen\", None) or getattr(env, \"surface\", None)\n",
    "#         frame = pygame.surfarray.array3d(surf)\n",
    "#         frame = np.transpose(frame, (1, 0, 2))\n",
    "#         frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "#         # Overlay\n",
    "#         overlay = np.zeros((frame_h, w, 3), dtype=np.uint8)\n",
    "#         overlay[0:h, :, :] = frame\n",
    "#         font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "#         fs, color, th = 0.35, (255, 255, 255), 1\n",
    "#         y0 = h + 15\n",
    "#         for i, text in enumerate([\n",
    "#             f\"Agent: {agent_name}\",\n",
    "#             f\"Seed: {seed}\",\n",
    "#             f\"Episode: {episode_num}\",\n",
    "#             f\"Step: {steps+1}\",\n",
    "#             f\"Score: {last_score}\",\n",
    "#         ]):\n",
    "#             cv2.putText(overlay, text, (5, y0 + i*12), font, fs, color, th, cv2.LINE_AA)\n",
    "\n",
    "#         vw.write(np.ascontiguousarray(overlay, dtype=np.uint8))\n",
    "\n",
    "#         state = next_state\n",
    "#         steps += 1\n",
    "\n",
    "#     vw.release()\n",
    "#     return last_score, total_reward, steps, actions_taken\n",
    "# # ...existing code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c404c28a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replay done. Score: 14, Steps: 104, Total reward: 137.7200000000002\n",
      "Saved video: replay_q_learning_seed123.mp4\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from snake_gym.envs.snake import SnakeGame\n",
    "# from your_q_module import get_state, record_episode  # if not defined in this notebook\n",
    "\n",
    "seed = 123\n",
    "agent_name = \"Q-Learning\"\n",
    "\n",
    "# Load Q-table saved during training\n",
    "with open(f\"q_table_seed{seed}.pkl\", \"rb\") as f:\n",
    "    Q = pickle.load(f)\n",
    "\n",
    "env = SnakeGame()\n",
    "video_path = f\"replay_q_learning_seed{seed}.mp4\"\n",
    "\n",
    "# Greedy replay (epsilon=0.0)\n",
    "score, total_reward, steps, actions = record_episode(\n",
    "    env, get_state, Q, epsilon=0.0,\n",
    "    episode_num=1, agent_name=agent_name, seed=seed, video_path=video_path\n",
    ")\n",
    "\n",
    "print(f\"Replay done. Score: {score}, Steps: {steps}, Total reward: {total_reward}\")\n",
    "print(f\"Saved video: {video_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c331a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open(\"q_table.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(Q, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7458fa0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# import numpy as np\n",
    "# import pygame\n",
    "# from snake_gym.envs.snake import SnakeGame\n",
    "# from snake_gym.envs.modules import GRIDSIZE\n",
    "# import cv2\n",
    "# import time\n",
    "\n",
    "# # Load Q-table\n",
    "# with open('q_table.pkl', 'rb') as f:\n",
    "#     Q = pickle.load(f)\n",
    "\n",
    "# print(f\"Loaded Q-table with {len(Q)} states\")\n",
    "\n",
    "# def get_state(env):\n",
    "#     \"\"\"Same as training\"\"\"\n",
    "#     head_x, head_y = env.snake.get_head_position()\n",
    "#     apple_x, apple_y = env.apple.position\n",
    "#     direction = env.snake.direction\n",
    "    \n",
    "#     def is_danger(dx, dy):\n",
    "#         new_x = head_x + dx * GRIDSIZE\n",
    "#         new_y = head_y + dy * GRIDSIZE\n",
    "#         if new_x < 0 or new_x >= 150 or new_y < 0 or new_y >= 150:\n",
    "#             return 1\n",
    "#         if (new_x, new_y) in env.snake.positions[:-1]:\n",
    "#             return 1\n",
    "#         return 0\n",
    "    \n",
    "#     return (is_danger(0, -GRIDSIZE), is_danger(0, GRIDSIZE), \n",
    "#             is_danger(-GRIDSIZE, 0), is_danger(GRIDSIZE, 0),\n",
    "#             int(apple_y < head_y), int(apple_y > head_y),\n",
    "#             int(apple_x < head_x), int(apple_x > head_x),\n",
    "#             int(direction == (0, -1)), int(direction == (0, 1)),\n",
    "#             int(direction == (-1, 0)), int(direction == (1, 0)))\n",
    "\n",
    "# # Initialize environment\n",
    "# env = SnakeGame()\n",
    "# state = env.reset()\n",
    "# state = get_state(env)\n",
    "# done = False\n",
    "# steps = 0\n",
    "\n",
    "# # --- Video recording setup ---\n",
    "# frame_width, frame_height = 150, 150\n",
    "# video_filename = 'snake_play.mp4'\n",
    "# fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "# out = cv2.VideoWriter(video_filename, fourcc, 10.0, (frame_width, frame_height))\n",
    "\n",
    "# print(\"\\nPlaying game... (close window to stop)\")\n",
    "\n",
    "# while not done and steps < 1000:\n",
    "#     # Get best action from Q-table\n",
    "#     if state in Q:\n",
    "#         action = max(Q[state], key=Q[state].get)\n",
    "#     else:\n",
    "#         action = np.random.randint(0, 4)\n",
    "    \n",
    "#     _, reward, done, _ = env.step(action)\n",
    "    \n",
    "#     # Capture frame from Pygame surface\n",
    "#     frame = pygame.surfarray.array3d(env.surface)\n",
    "#     frame = np.transpose(frame, (1, 0, 2))  # Swap axes for OpenCV\n",
    "#     frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "#     # Write frame to video\n",
    "#     out.write(frame)\n",
    "    \n",
    "#     pygame.display.flip()\n",
    "#     time.sleep(0.05)  # Slow down for viewing\n",
    "    \n",
    "#     state = get_state(env)\n",
    "#     steps += 1\n",
    "\n",
    "# # Release video writer\n",
    "# out.release()\n",
    "# pygame.quit()\n",
    "\n",
    "# print(f\"\\nGame Over!\")\n",
    "# print(f\"Final Score: {env.snake.length - 1}\")\n",
    "# print(f\"Steps survived: {steps}\")\n",
    "# print(f\"Video saved as {video_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e077e9ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "============================================================\n",
      "Episode   100 | Avg Score:  0.72 | Max Score:   4 | Avg Reward:   -2.80 | Epsilon: 0.606 | States: 34\n",
      "Episode   200 | Avg Score:  1.23 | Max Score:   6 | Avg Reward:    9.20 | Epsilon: 0.367 | States: 35\n",
      "Episode   300 | Avg Score:  1.67 | Max Score:  11 | Avg Reward:   25.00 | Epsilon: 0.222 | States: 36\n",
      "Episode   400 | Avg Score:  1.71 | Max Score:  13 | Avg Reward:   36.80 | Epsilon: 0.135 | States: 36\n",
      "Episode   500 | Avg Score:  1.10 | Max Score:  10 | Avg Reward:   44.80 | Epsilon: 0.082 | States: 36\n",
      "Episode   600 | Avg Score:  1.65 | Max Score:  15 | Avg Reward:   56.50 | Epsilon: 0.049 | States: 36\n",
      "Episode   700 | Avg Score:  1.03 | Max Score:  10 | Avg Reward:   60.90 | Epsilon: 0.030 | States: 36\n",
      "Episode   800 | Avg Score:  1.08 | Max Score:  12 | Avg Reward:   81.20 | Epsilon: 0.018 | States: 36\n",
      "Episode   900 | Avg Score:  0.96 | Max Score:  16 | Avg Reward:   73.70 | Epsilon: 0.011 | States: 36\n",
      "Episode  1000 | Avg Score:  0.62 | Max Score:   8 | Avg Reward:   82.10 | Epsilon: 0.010 | States: 36\n",
      "\n",
      "============================================================\n",
      "Training Complete!\n",
      "Best Score: 16\n",
      "Final Avg (last 100): 0.62\n"
     ]
    }
   ],
   "source": [
    "# import random\n",
    "# import numpy as np\n",
    "# from snake_gym.envs.snake import SnakeGame\n",
    "\n",
    "# # Hyperparameters\n",
    "# alpha = 0.1\n",
    "# gamma = 0.95\n",
    "# epsilon = 1.0\n",
    "# epsilon_min = 0.01\n",
    "# epsilon_decay = 0.995\n",
    "# num_episodes = 1000\n",
    "\n",
    "# # Initialize\n",
    "# env = SnakeGame()\n",
    "# Q = {}\n",
    "\n",
    "# def get_state(env):\n",
    "#     \"\"\"Get a simplified state representation\"\"\"\n",
    "#     head_x, head_y = env.snake.get_head_position()\n",
    "#     apple_x, apple_y = env.apple.position\n",
    "#     direction = env.snake.direction\n",
    "    \n",
    "#     # Danger detection\n",
    "#     def is_danger(dx, dy):\n",
    "#         new_x = head_x + dx * GRIDSIZE\n",
    "#         new_y = head_y + dy * GRIDSIZE\n",
    "        \n",
    "#         # Wall collision\n",
    "#         if new_x < 0 or new_x >= 150 or new_y < 0 or new_y >= 150:\n",
    "#             return 1\n",
    "        \n",
    "#         # Self collision\n",
    "#         if (new_x, new_y) in env.snake.positions[:-1]:\n",
    "#             return 1\n",
    "        \n",
    "#         return 0\n",
    "    \n",
    "#     # Check danger in all 4 directions\n",
    "#     danger_up = is_danger(0, -GRIDSIZE)\n",
    "#     danger_down = is_danger(0, GRIDSIZE)\n",
    "#     danger_left = is_danger(-GRIDSIZE, 0)\n",
    "#     danger_right = is_danger(GRIDSIZE, 0)\n",
    "    \n",
    "#     # Apple direction\n",
    "#     apple_up = int(apple_y < head_y)\n",
    "#     apple_down = int(apple_y > head_y)\n",
    "#     apple_left = int(apple_x < head_x)\n",
    "#     apple_right = int(apple_x > head_x)\n",
    "    \n",
    "#     # Current direction\n",
    "#     dir_up = int(direction == (0, -1))\n",
    "#     dir_down = int(direction == (0, 1))\n",
    "#     dir_left = int(direction == (-1, 0))\n",
    "#     dir_right = int(direction == (1, 0))\n",
    "    \n",
    "#     return (danger_up, danger_down, danger_left, danger_right,\n",
    "#             apple_up, apple_down, apple_left, apple_right,\n",
    "#             dir_up, dir_down, dir_left, dir_right)\n",
    "\n",
    "# # Import GRIDSIZE from modules\n",
    "# from snake_gym.envs.modules import GRIDSIZE\n",
    "\n",
    "# # Training\n",
    "# scores = []\n",
    "# rewards_list = []\n",
    "\n",
    "# print(\"Starting training...\")\n",
    "# print(\"=\" * 60)\n",
    "\n",
    "# for episode in range(num_episodes):\n",
    "#     state = env.reset()\n",
    "#     state = get_state(env)\n",
    "#     done = False\n",
    "#     total_reward = 0\n",
    "#     steps = 0\n",
    "    \n",
    "#     while not done and steps < 1000:\n",
    "#         # Initialize Q-values\n",
    "#         if state not in Q:\n",
    "#             Q[state] = {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0}\n",
    "        \n",
    "#         # Epsilon-greedy\n",
    "#         if random.random() < epsilon:\n",
    "#             action = random.randint(0, 3)\n",
    "#         else:\n",
    "#             action = max(Q[state], key=Q[state].get)\n",
    "        \n",
    "#         # Take step\n",
    "#         _, reward, done, info = env.step(action)\n",
    "#         next_state = get_state(env)\n",
    "        \n",
    "#         if next_state not in Q:\n",
    "#             Q[next_state] = {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0}\n",
    "        \n",
    "#         # Q-Learning update\n",
    "#         best_next = max(Q[next_state].values()) if not done else 0\n",
    "#         Q[state][action] += alpha * (reward + gamma * best_next - Q[state][action])\n",
    "        \n",
    "#         state = next_state\n",
    "#         total_reward += reward\n",
    "#         steps += 1\n",
    "    \n",
    "#     # Decay epsilon\n",
    "#     epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "    \n",
    "#     # Track metrics\n",
    "#     score = env.snake.length - 1\n",
    "#     scores.append(score)\n",
    "#     rewards_list.append(total_reward)\n",
    "    \n",
    "#     # Print progress\n",
    "#     if (episode + 1) % 100 == 0:\n",
    "#         avg_score = np.mean(scores[-100:])\n",
    "#         avg_reward = np.mean(rewards_list[-100:])\n",
    "#         max_score = max(scores[-100:])\n",
    "#         print(f\"Episode {episode+1:5d} | \"\n",
    "#               f\"Avg Score: {avg_score:5.2f} | \"\n",
    "#               f\"Max Score: {max_score:3d} | \"\n",
    "#               f\"Avg Reward: {avg_reward:7.2f} | \"\n",
    "#               f\"Epsilon: {epsilon:.3f} | \"\n",
    "#               f\"States: {len(Q)}\")\n",
    "\n",
    "# print(\"\\n\" + \"=\" * 60)\n",
    "# print(\"Training Complete!\")\n",
    "# print(f\"Best Score: {max(scores)}\")\n",
    "# print(f\"Final Avg (last 100): {np.mean(scores[-100:]):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54168a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Q-table with 36 states\n",
      "\n",
      "Playing game... (close window to stop)\n",
      "\n",
      "Game Over!\n",
      "Final Score: 0\n",
      "Steps survived: 78\n"
     ]
    }
   ],
   "source": [
    "# import pickle\n",
    "# import numpy as np\n",
    "# import pygame\n",
    "# from snake_gym.envs.snake import SnakeGame\n",
    "# from snake_gym.envs.modules import GRIDSIZE\n",
    "# import time\n",
    "\n",
    "# # Load Q-table\n",
    "# with open('q_table.pkl', 'rb') as f:\n",
    "#     Q = pickle.load(f)\n",
    "\n",
    "# print(f\"Loaded Q-table with {len(Q)} states\")\n",
    "\n",
    "# def get_state(env):\n",
    "#     \"\"\"Same as training\"\"\"\n",
    "#     head_x, head_y = env.snake.get_head_position()\n",
    "#     apple_x, apple_y = env.apple.position\n",
    "#     direction = env.snake.direction\n",
    "    \n",
    "#     def is_danger(dx, dy):\n",
    "#         new_x = head_x + dx * GRIDSIZE\n",
    "#         new_y = head_y + dy * GRIDSIZE\n",
    "#         if new_x < 0 or new_x >= 150 or new_y < 0 or new_y >= 150:\n",
    "#             return 1\n",
    "#         if (new_x, new_y) in env.snake.positions[:-1]:\n",
    "#             return 1\n",
    "#         return 0\n",
    "    \n",
    "#     return (is_danger(0, -GRIDSIZE), is_danger(0, GRIDSIZE), \n",
    "#             is_danger(-GRIDSIZE, 0), is_danger(GRIDSIZE, 0),\n",
    "#             int(apple_y < head_y), int(apple_y > head_y),\n",
    "#             int(apple_x < head_x), int(apple_x > head_x),\n",
    "#             int(direction == (0, -1)), int(direction == (0, 1)),\n",
    "#             int(direction == (-1, 0)), int(direction == (1, 0)))\n",
    "\n",
    "# # Play game\n",
    "# env = SnakeGame()\n",
    "# state = env.reset()\n",
    "# state = get_state(env)\n",
    "# done = False\n",
    "# steps = 0\n",
    "\n",
    "# print(\"\\nPlaying game... (close window to stop)\")\n",
    "\n",
    "# while not done and steps < 1000:\n",
    "#     # Get best action from Q-table\n",
    "#     if state in Q:\n",
    "#         action = max(Q[state], key=Q[state].get)\n",
    "#     else:\n",
    "#         action = np.random.randint(0, 4)\n",
    "    \n",
    "#     _, reward, done, _ = env.step(action)\n",
    "#     pygame.display.flip()\n",
    "#     time.sleep(0.1)  # Slow down for viewing\n",
    "    \n",
    "#     state = get_state(env)\n",
    "#     steps += 1\n",
    "\n",
    "# print(f\"\\nGame Over!\")\n",
    "# print(f\"Final Score: {env.snake.length - 1}\")\n",
    "# print(f\"Steps survived: {steps}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (snake-gym)",
   "language": "python",
   "name": "snake-gym"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
