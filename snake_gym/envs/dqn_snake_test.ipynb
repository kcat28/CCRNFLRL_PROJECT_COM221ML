{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "727aed34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: stable-baselines3 in c:\\users\\diana nicole\\snake-gym\\venv\\lib\\site-packages (2.7.0)\n",
      "Requirement already satisfied: gym in c:\\users\\diana nicole\\snake-gym\\venv\\lib\\site-packages (0.21.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\diana nicole\\snake-gym\\venv\\lib\\site-packages (from stable-baselines3) (3.10.7)\n",
      "Requirement already satisfied: torch<3.0,>=2.3 in c:\\users\\diana nicole\\snake-gym\\venv\\lib\\site-packages (from stable-baselines3) (2.9.0)\n",
      "Requirement already satisfied: cloudpickle in c:\\users\\diana nicole\\snake-gym\\venv\\lib\\site-packages (from stable-baselines3) (3.1.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\diana nicole\\snake-gym\\venv\\lib\\site-packages (from stable-baselines3) (2.3.3)\n",
      "Requirement already satisfied: numpy<3.0,>=1.20 in c:\\users\\diana nicole\\snake-gym\\venv\\lib\\site-packages (from stable-baselines3) (2.2.6)\n",
      "Requirement already satisfied: gymnasium<1.3.0,>=0.29.1 in c:\\users\\diana nicole\\snake-gym\\venv\\lib\\site-packages (from stable-baselines3) (1.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\diana nicole\\snake-gym\\venv\\lib\\site-packages (from gymnasium<1.3.0,>=0.29.1->stable-baselines3) (4.15.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\diana nicole\\snake-gym\\venv\\lib\\site-packages (from gymnasium<1.3.0,>=0.29.1->stable-baselines3) (0.0.4)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\diana nicole\\snake-gym\\venv\\lib\\site-packages (from torch<3.0,>=2.3->stable-baselines3) (3.4.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\diana nicole\\snake-gym\\venv\\lib\\site-packages (from torch<3.0,>=2.3->stable-baselines3) (3.20.0)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in c:\\users\\diana nicole\\snake-gym\\venv\\lib\\site-packages (from torch<3.0,>=2.3->stable-baselines3) (2025.9.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\diana nicole\\snake-gym\\venv\\lib\\site-packages (from torch<3.0,>=2.3->stable-baselines3) (3.1.6)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\diana nicole\\snake-gym\\venv\\lib\\site-packages (from torch<3.0,>=2.3->stable-baselines3) (1.14.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\diana nicole\\snake-gym\\venv\\lib\\site-packages (from matplotlib->stable-baselines3) (1.3.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\diana nicole\\snake-gym\\venv\\lib\\site-packages (from matplotlib->stable-baselines3) (4.60.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\diana nicole\\snake-gym\\venv\\lib\\site-packages (from matplotlib->stable-baselines3) (25.0)\n",
      "Requirement already satisfied: pyparsing>=3 in c:\\users\\diana nicole\\snake-gym\\venv\\lib\\site-packages (from matplotlib->stable-baselines3) (3.2.5)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\diana nicole\\snake-gym\\venv\\lib\\site-packages (from matplotlib->stable-baselines3) (0.12.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\diana nicole\\snake-gym\\venv\\lib\\site-packages (from matplotlib->stable-baselines3) (12.0.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\diana nicole\\snake-gym\\venv\\lib\\site-packages (from matplotlib->stable-baselines3) (1.4.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\diana nicole\\snake-gym\\venv\\lib\\site-packages (from matplotlib->stable-baselines3) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\diana nicole\\snake-gym\\venv\\lib\\site-packages (from pandas->stable-baselines3) (2025.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\diana nicole\\snake-gym\\venv\\lib\\site-packages (from pandas->stable-baselines3) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\diana nicole\\snake-gym\\venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->stable-baselines3) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\diana nicole\\snake-gym\\venv\\lib\\site-packages (from sympy>=1.13.3->torch<3.0,>=2.3->stable-baselines3) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\diana nicole\\snake-gym\\venv\\lib\\site-packages (from jinja2->torch<3.0,>=2.3->stable-baselines3) (3.0.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: shimmy>=2.0 in c:\\users\\diana nicole\\snake-gym\\venv\\lib\\site-packages (2.0.0)\n",
      "Requirement already satisfied: numpy>=1.18.0 in c:\\users\\diana nicole\\snake-gym\\venv\\lib\\site-packages (from shimmy>=2.0) (2.2.6)\n",
      "Requirement already satisfied: gymnasium>=1.0.0a1 in c:\\users\\diana nicole\\snake-gym\\venv\\lib\\site-packages (from shimmy>=2.0) (1.2.1)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\diana nicole\\snake-gym\\venv\\lib\\site-packages (from gymnasium>=1.0.0a1->shimmy>=2.0) (3.1.1)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\diana nicole\\snake-gym\\venv\\lib\\site-packages (from gymnasium>=1.0.0a1->shimmy>=2.0) (0.0.4)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\diana nicole\\snake-gym\\venv\\lib\\site-packages (from gymnasium>=1.0.0a1->shimmy>=2.0) (4.15.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in c:\\users\\diana nicole\\snake-gym\\venv\\lib\\site-packages (4.12.0.88)\n",
      "Requirement already satisfied: numpy<2.3.0,>=2 in c:\\users\\diana nicole\\snake-gym\\venv\\lib\\site-packages (from opencv-python) (2.2.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install stable-baselines3 gym\n",
    "!pip install \"shimmy>=2.0\"\n",
    "!pip install opencv-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b69adc82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.10.11)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import snake_gym\n",
    "import random\n",
    "\n",
    "env = gym.make(\"snake-v0\")\n",
    "# obs = env.reset()\n",
    "# done = False\n",
    "\n",
    "# while not done:\n",
    "#     action = random.choice([0, 1, 2, 3])\n",
    "#     obs, reward, done, info = env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "517c0b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  🏆 New best score: 1 at episode 21\n",
      "  🏆 New best score: 3 at episode 67\n",
      "  ✓ Reached 50%_max performance at episode 65\n",
      "  ✓ Reached 80%_max performance at episode 73\n",
      "  ✓ Reached 90%_max performance at episode 83\n",
      "Episode   100 | Avg Score:  0.19 | Max Score:   3 | Avg Reward:   -2.12 | Avg Reward/Step: -0.078 | ε: 0.606\n",
      "Episode   200 | Avg Score:  0.65 | Max Score:   3 | Avg Reward:    2.59 | Avg Reward/Step: 0.043 | ε: 0.367\n",
      "  🏆 New best score: 5 at episode 232\n",
      "  🏆 New best score: 6 at episode 270\n",
      "  🏆 New best score: 7 at episode 286\n",
      "Episode   300 | Avg Score:  0.78 | Max Score:   7 | Avg Reward:   10.13 | Avg Reward/Step: 0.276 | ε: 0.222\n",
      "  🏆 New best score: 11 at episode 324\n",
      "Episode   400 | Avg Score:  0.83 | Max Score:  11 | Avg Reward:   20.22 | Avg Reward/Step: 0.443 | ε: 0.135\n",
      "  🏆 New best score: 13 at episode 456\n",
      "Episode   500 | Avg Score:  0.94 | Max Score:  13 | Avg Reward:   22.58 | Avg Reward/Step: 0.536 | ε: 0.082\n",
      "Episode   600 | Avg Score:  0.85 | Max Score:  12 | Avg Reward:   23.37 | Avg Reward/Step: 0.528 | ε: 0.049\n",
      "  🏆 New best score: 19 at episode 612\n",
      "Episode   700 | Avg Score:  0.58 | Max Score:  19 | Avg Reward:   33.64 | Avg Reward/Step: 0.574 | ε: 0.030\n",
      "Episode   800 | Avg Score:  0.54 | Max Score:   9 | Avg Reward:   38.12 | Avg Reward/Step: 0.604 | ε: 0.018\n",
      "Episode   900 | Avg Score:  0.60 | Max Score:  12 | Avg Reward:   42.80 | Avg Reward/Step: 0.618 | ε: 0.011\n",
      "Episode  1000 | Avg Score:  0.76 | Max Score:  17 | Avg Reward:   39.92 | Avg Reward/Step: 0.669 | ε: 0.010\n",
      "✅ Metrics saved to metrics_dqn_seed42.pkl\n",
      "  🏆 New best score: 1 at episode 1\n",
      "  🏆 New best score: 2 at episode 29\n",
      "  ✓ Reached 50%_max performance at episode 1\n",
      "  ✓ Reached 80%_max performance at episode 1\n",
      "  ✓ Reached 90%_max performance at episode 1\n",
      "Episode   100 | Avg Score:  0.34 | Max Score:   2 | Avg Reward:   -0.76 | Avg Reward/Step: -0.080 | ε: 0.606\n",
      "  🏆 New best score: 4 at episode 101\n",
      "  🏆 New best score: 5 at episode 139\n",
      "  🏆 New best score: 6 at episode 187\n",
      "Episode   200 | Avg Score:  0.75 | Max Score:   6 | Avg Reward:    5.22 | Avg Reward/Step: 0.064 | ε: 0.367\n",
      "  🏆 New best score: 10 at episode 284\n",
      "Episode   300 | Avg Score:  0.98 | Max Score:  10 | Avg Reward:   11.26 | Avg Reward/Step: 0.311 | ε: 0.222\n",
      "Episode   400 | Avg Score:  0.87 | Max Score:   9 | Avg Reward:   17.99 | Avg Reward/Step: 0.435 | ε: 0.135\n",
      "Episode   500 | Avg Score:  0.70 | Max Score:   9 | Avg Reward:   26.29 | Avg Reward/Step: 0.495 | ε: 0.082\n",
      "Episode   600 | Avg Score:  0.79 | Max Score:  10 | Avg Reward:   24.93 | Avg Reward/Step: 0.517 | ε: 0.049\n",
      "  🏆 New best score: 16 at episode 658\n",
      "  🏆 New best score: 19 at episode 692\n",
      "Episode   700 | Avg Score:  0.89 | Max Score:  19 | Avg Reward:   35.16 | Avg Reward/Step: 0.607 | ε: 0.030\n",
      "Episode   800 | Avg Score:  0.72 | Max Score:  16 | Avg Reward:   41.80 | Avg Reward/Step: 0.633 | ε: 0.018\n",
      "Episode   900 | Avg Score:  0.58 | Max Score:  12 | Avg Reward:   39.62 | Avg Reward/Step: 0.651 | ε: 0.011\n",
      "Episode  1000 | Avg Score:  0.37 | Max Score:  10 | Avg Reward:   37.67 | Avg Reward/Step: 0.624 | ε: 0.010\n",
      "✅ Metrics saved to metrics_dqn_seed123.pkl\n",
      "  🏆 New best score: 1 at episode 11\n",
      "  🏆 New best score: 2 at episode 47\n",
      "  🏆 New best score: 4 at episode 77\n",
      "  ✓ Reached 50%_max performance at episode 35\n",
      "  ✓ Reached 80%_max performance at episode 79\n",
      "  ✓ Reached 90%_max performance at episode 85\n",
      "Episode   100 | Avg Score:  0.32 | Max Score:   4 | Avg Reward:   -1.53 | Avg Reward/Step: -0.087 | ε: 0.606\n",
      "  🏆 New best score: 8 at episode 193\n",
      "Episode   200 | Avg Score:  0.87 | Max Score:   8 | Avg Reward:    5.55 | Avg Reward/Step: 0.145 | ε: 0.367\n",
      "Episode   300 | Avg Score:  0.82 | Max Score:   6 | Avg Reward:   12.62 | Avg Reward/Step: 0.340 | ε: 0.222\n",
      "  🏆 New best score: 10 at episode 314\n",
      "  🏆 New best score: 12 at episode 362\n",
      "Episode   400 | Avg Score:  1.12 | Max Score:  12 | Avg Reward:   19.83 | Avg Reward/Step: 0.449 | ε: 0.135\n",
      "Episode   500 | Avg Score:  0.58 | Max Score:   8 | Avg Reward:   28.29 | Avg Reward/Step: 0.563 | ε: 0.082\n",
      "Episode   600 | Avg Score:  0.72 | Max Score:  11 | Avg Reward:   27.32 | Avg Reward/Step: 0.552 | ε: 0.049\n",
      "  🏆 New best score: 15 at episode 618\n",
      "Episode   700 | Avg Score:  0.64 | Max Score:  15 | Avg Reward:   36.12 | Avg Reward/Step: 0.559 | ε: 0.030\n",
      "Episode   800 | Avg Score:  0.26 | Max Score:  13 | Avg Reward:   43.37 | Avg Reward/Step: 0.656 | ε: 0.018\n",
      "Episode   900 | Avg Score:  0.24 | Max Score:  12 | Avg Reward:   45.08 | Avg Reward/Step: 0.633 | ε: 0.011\n",
      "  🏆 New best score: 18 at episode 954\n",
      "Episode  1000 | Avg Score:  0.66 | Max Score:  18 | Avg Reward:   40.91 | Avg Reward/Step: 0.644 | ε: 0.010\n",
      "✅ Metrics saved to metrics_dqn_seed456.pkl\n",
      "\n",
      "🎉 All DQN training runs complete!\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "import psutil\n",
    "import pickle\n",
    "import cv2\n",
    "import pygame\n",
    "from collections import deque\n",
    "from snake_gym.envs.snake import SnakeGame\n",
    "from snake_gym.envs.modules import GRIDSIZE\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "\n",
    "# =============================================================================\n",
    "# METRICS LOGGER\n",
    "# =============================================================================\n",
    "class MetricsLogger:\n",
    "    \"\"\"Comprehensive metrics tracking for RL training\"\"\"\n",
    "    \n",
    "    def __init__(self, agent_name, seed, record_gameplay=True, record_frequency=1000):\n",
    "        self.agent_name = agent_name\n",
    "        self.seed = seed\n",
    "        self.start_time = time.time()\n",
    "        self.record_gameplay = record_gameplay\n",
    "        self.record_frequency = record_frequency\n",
    "        self.recorded_episodes = []\n",
    "        self.best_episode_score = 0\n",
    "        self.best_episode_number = 0\n",
    "        self.episode_rewards = []\n",
    "        self.episode_scores = []\n",
    "        self.episode_lengths = []\n",
    "        self.episode_times = []\n",
    "        self.reward_variance_per_block = []\n",
    "        self.score_variance_per_block = []\n",
    "        self.epsilon_history = []\n",
    "        self.action_counts = {0:0,1:0,2:0,3:0}\n",
    "        self.action_history_per_block = []\n",
    "        self.cpu_usage = []\n",
    "        self.memory_usage = []\n",
    "        self.moving_avg_reward = []\n",
    "        self.moving_avg_score = []\n",
    "        self.episodes_to_threshold = {}\n",
    "    \n",
    "    def should_record_episode(self, episode_num):\n",
    "        if not self.record_gameplay:\n",
    "            return False\n",
    "        if episode_num % self.record_frequency == 0:\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def is_best_episode(self, score):\n",
    "        return score > self.best_episode_score\n",
    "    \n",
    "    def update_best_episode(self, episode_num, score):\n",
    "        if score > self.best_episode_score:\n",
    "            self.best_episode_score = score\n",
    "            self.best_episode_number = episode_num\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def log_episode(self, reward, score, length, epsilon, episode_time, actions_taken):\n",
    "        self.episode_rewards.append(reward)\n",
    "        self.episode_scores.append(score)\n",
    "        self.episode_lengths.append(length)\n",
    "        self.episode_times.append(episode_time)\n",
    "        self.epsilon_history.append(epsilon)\n",
    "        for action in actions_taken:\n",
    "            self.action_counts[action] += 1\n",
    "        window = 100\n",
    "        if len(self.episode_rewards) >= window:\n",
    "            self.moving_avg_reward.append(np.mean(self.episode_rewards[-window:]))\n",
    "            self.moving_avg_score.append(np.mean(self.episode_scores[-window:]))\n",
    "        else:\n",
    "            self.moving_avg_reward.append(np.mean(self.episode_rewards))\n",
    "            self.moving_avg_score.append(np.mean(self.episode_scores))\n",
    "        self.cpu_usage.append(psutil.cpu_percent(interval=0.1))\n",
    "        self.memory_usage.append(psutil.virtual_memory().percent)\n",
    "    \n",
    "    def log_block_statistics(self, block_size=1000):\n",
    "        if len(self.episode_rewards) >= block_size:\n",
    "            recent_rewards = self.episode_rewards[-block_size:]\n",
    "            recent_scores = self.episode_scores[-block_size:]\n",
    "            self.reward_variance_per_block.append(np.var(recent_rewards))\n",
    "            self.score_variance_per_block.append(np.var(recent_scores))\n",
    "            total_actions = sum(self.action_counts.values())\n",
    "            if total_actions > 0:\n",
    "                action_dist = {k: v/total_actions for k, v in self.action_counts.items()}\n",
    "                self.action_history_per_block.append(action_dist)\n",
    "    \n",
    "    def check_convergence_threshold(self, threshold_percent=0.8):\n",
    "        if len(self.moving_avg_score) < 100:\n",
    "            return\n",
    "        max_score = max(self.moving_avg_score)\n",
    "        threshold = threshold_percent * max_score\n",
    "        threshold_name = f\"{int(threshold_percent*100)}%_max\"\n",
    "        if threshold_name not in self.episodes_to_threshold:\n",
    "            for i, score in enumerate(self.moving_avg_score):\n",
    "                if score >= threshold:\n",
    "                    self.episodes_to_threshold[threshold_name] = i + 1\n",
    "                    print(f\"  ✓ Reached {threshold_name} performance at episode {i+1}\")\n",
    "                    break\n",
    "    \n",
    "    def get_training_duration(self):\n",
    "        return time.time() - self.start_time\n",
    "    \n",
    "    def get_avg_episode_time(self):\n",
    "        return np.mean(self.episode_times) if self.episode_times else 0\n",
    "    \n",
    "    def save(self, filename):\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump(self.__dict__, f)\n",
    "        print(f\"✅ Metrics saved to {filename}\")\n",
    "\n",
    "# =============================================================================\n",
    "# DQN NETWORK AND REPLAY BUFFER\n",
    "# =============================================================================\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim=12, output_dim=4):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, output_dim)\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=10000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    def sample(self, batch_size=64):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        return np.array(states), actions, rewards, np.array(next_states), dones\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# =============================================================================\n",
    "# STATE REPRESENTATION\n",
    "# =============================================================================\n",
    "def get_state(env):\n",
    "    head_x, head_y = env.snake.get_head_position()\n",
    "    apple_x, apple_y = env.apple.position\n",
    "    direction = env.snake.direction\n",
    "    def is_danger(dx, dy):\n",
    "        new_x = head_x + dx * GRIDSIZE\n",
    "        new_y = head_y + dy * GRIDSIZE\n",
    "        if new_x < 0 or new_x >= 150 or new_y < 0 or new_y >= 150:\n",
    "            return 1\n",
    "        if (new_x, new_y) in env.snake.positions[:-1]:\n",
    "            return 1\n",
    "        return 0\n",
    "    danger_up = is_danger(0, -GRIDSIZE)\n",
    "    danger_down = is_danger(0, GRIDSIZE)\n",
    "    danger_left = is_danger(-GRIDSIZE, 0)\n",
    "    danger_right = is_danger(GRIDSIZE, 0)\n",
    "    apple_up = int(apple_y < head_y)\n",
    "    apple_down = int(apple_y > head_y)\n",
    "    apple_left = int(apple_x < head_x)\n",
    "    apple_right = int(apple_x > head_x)\n",
    "    dir_up = int(direction == (0, -1))\n",
    "    dir_down = int(direction == (0, 1))\n",
    "    dir_left = int(direction == (-1, 0))\n",
    "    dir_right = int(direction == (1, 0))\n",
    "    return (danger_up, danger_down, danger_left, danger_right,\n",
    "            apple_up, apple_down, apple_left, apple_right,\n",
    "            dir_up, dir_down, dir_left, dir_right)\n",
    "\n",
    "# =============================================================================\n",
    "# VIDEO RECORDING\n",
    "# =============================================================================\n",
    "def add_info_overlay(frame, episode, steps, score, agent_name, seed):\n",
    "    overlay = np.zeros((200, 150, 3), dtype=np.uint8)\n",
    "    overlay[0:150, :] = frame\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    font_scale = 0.35\n",
    "    color = (255,255,255)\n",
    "    thickness = 1\n",
    "    text_lines = [\n",
    "        f\"Agent: {agent_name}\",\n",
    "        f\"Seed: {seed}\",\n",
    "        f\"Episode: {episode}\",\n",
    "        f\"Step: {steps}\",\n",
    "        f\"Score: {score}\"\n",
    "    ]\n",
    "    y_offset = 160\n",
    "    for i, text in enumerate(text_lines):\n",
    "        cv2.putText(overlay, text, (5, y_offset + i*10), font, font_scale, color, thickness, cv2.LINE_AA)\n",
    "    return overlay\n",
    "\n",
    "def record_episode(env, state_func, policy_net, epsilon, episode_num, agent_name, seed, video_path, max_steps=1000):\n",
    "    # Capture first frame to determine video size\n",
    "    state = env.reset()\n",
    "    state = state_func(env)\n",
    "    first_frame = pygame.surfarray.array3d(env.screen)\n",
    "    first_frame = np.transpose(first_frame, (1,0,2))\n",
    "    first_frame = cv2.cvtColor(first_frame, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    overlay_height = 50\n",
    "    height, width, _ = first_frame.shape\n",
    "    frame_height = height + overlay_height\n",
    "\n",
    "    # Initialize VideoWriter\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    fps = 15\n",
    "    video_writer = cv2.VideoWriter(video_path, fourcc, fps, (width, frame_height))\n",
    "\n",
    "    state = env.reset()\n",
    "    state = state_func(env)\n",
    "    done = False\n",
    "    steps = 0\n",
    "    total_reward = 0\n",
    "    actions_taken = []\n",
    "\n",
    "    while not done and steps < max_steps:\n",
    "        if random.random() < min(epsilon, 0.1):\n",
    "            action = random.randint(0,3)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                state_tensor = torch.tensor(state).float().unsqueeze(0)\n",
    "                action = policy_net(state_tensor).argmax().item()\n",
    "\n",
    "        actions_taken.append(action)\n",
    "        _, reward, done, _ = env.step(action)\n",
    "        state = state_func(env)\n",
    "        total_reward += reward\n",
    "\n",
    "        # Capture frame\n",
    "        frame = pygame.surfarray.array3d(env.screen)\n",
    "        frame = np.transpose(frame, (1,0,2))\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        # Add overlay\n",
    "        overlay = np.zeros((frame_height, width, 3), dtype=np.uint8)\n",
    "        overlay[0:height,:,:] = frame\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        font_scale = 0.35\n",
    "        color = (255,255,255)\n",
    "        thickness = 1\n",
    "        y_offset = height + 15\n",
    "        text_lines = [\n",
    "            f\"Agent: {agent_name}\",\n",
    "            f\"Seed: {seed}\",\n",
    "            f\"Episode: {episode_num}\",\n",
    "            f\"Step: {steps}\",\n",
    "            f\"Score: {env.snake.length-1}\"\n",
    "        ]\n",
    "        for i, text in enumerate(text_lines):\n",
    "            cv2.putText(overlay, text, (5, y_offset + i*12), font, font_scale, color, thickness, cv2.LINE_AA)\n",
    "\n",
    "        overlay = np.ascontiguousarray(overlay, dtype=np.uint8)\n",
    "        video_writer.write(overlay)\n",
    "        steps += 1\n",
    "\n",
    "    video_writer.release()\n",
    "    score = env.snake.length-1\n",
    "    return score, total_reward, steps, actions_taken\n",
    "\n",
    "# =============================================================================\n",
    "# DQN TRAINING\n",
    "# =============================================================================\n",
    "def train_dqn(seed=42, num_episodes=1000, agent_name=\"DQN\", record_gameplay=True, record_frequency=200):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    env = SnakeGame()\n",
    "    recordings_dir = f\"recordings_{agent_name.lower().replace(' ','_')}_seed{seed}\"\n",
    "    os.makedirs(recordings_dir, exist_ok=True)\n",
    "    \n",
    "    input_dim = 12\n",
    "    output_dim = 4\n",
    "    policy_net = DQN(input_dim, output_dim)\n",
    "    target_net = DQN(input_dim, output_dim)\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    target_net.eval()\n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr=0.001)\n",
    "    criterion = nn.MSELoss()\n",
    "    replay_buffer = ReplayBuffer(10000)\n",
    "    \n",
    "    batch_size = 64\n",
    "    gamma = 0.95\n",
    "    epsilon = 1.0\n",
    "    epsilon_min = 0.01\n",
    "    epsilon_decay = 0.995\n",
    "    target_update = 10\n",
    "    \n",
    "    metrics = MetricsLogger(agent_name, seed, record_gameplay, record_frequency)\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        episode_start = time.time()\n",
    "        should_record = metrics.should_record_episode(episode+1)\n",
    "        \n",
    "        if should_record:\n",
    "            video_path = f\"{recordings_dir}/episode_{episode+1:05d}.mp4\"\n",
    "            score, total_reward, steps, actions_taken = record_episode(env, get_state, policy_net, epsilon, episode+1, agent_name, seed, video_path)\n",
    "            metrics.recorded_episodes.append(episode+1)\n",
    "        else:\n",
    "            state = get_state(env)\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "            steps = 0\n",
    "            actions_taken = []\n",
    "            while not done and steps < 1000:\n",
    "                if random.random() < epsilon:\n",
    "                    action = random.randint(0,3)\n",
    "                else:\n",
    "                    with torch.no_grad():\n",
    "                        state_tensor = torch.tensor(state).float().unsqueeze(0)\n",
    "                        action = policy_net(state_tensor).argmax().item()\n",
    "                actions_taken.append(action)\n",
    "                _, reward, done, _ = env.step(action)\n",
    "                next_state = get_state(env)\n",
    "                replay_buffer.push(state, action, reward, next_state, done)\n",
    "                if len(replay_buffer) >= batch_size:\n",
    "                    s, a, r, s_next, d = replay_buffer.sample(batch_size)\n",
    "                    s_tensor = torch.tensor(s).float()\n",
    "                    a_tensor = torch.tensor(a).long().unsqueeze(1)\n",
    "                    r_tensor = torch.tensor(r).float().unsqueeze(1)\n",
    "                    s_next_tensor = torch.tensor(s_next).float()\n",
    "                    d_tensor = torch.tensor(d).float().unsqueeze(1)\n",
    "                    q_values = policy_net(s_tensor).gather(1, a_tensor)\n",
    "                    next_q_values = target_net(s_next_tensor).max(1)[0].unsqueeze(1)\n",
    "                    target = r_tensor + gamma * next_q_values * (1 - d_tensor)\n",
    "                    loss = criterion(q_values, target)\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                state = next_state\n",
    "                total_reward += reward\n",
    "                steps +=1\n",
    "            score = env.snake.length-1\n",
    "        \n",
    "        if metrics.update_best_episode(episode+1, score):\n",
    "            print(f\"  🏆 New best score: {score} at episode {episode+1}\")\n",
    "        \n",
    "        epsilon = max(epsilon_min, epsilon*epsilon_decay)\n",
    "        if episode % target_update == 0:\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "        \n",
    "        metrics.log_episode(total_reward, score, steps, epsilon, time.time()-episode_start, actions_taken)\n",
    "        \n",
    "        if (episode+1) % 100 ==0:\n",
    "            metrics.check_convergence_threshold(0.5)\n",
    "            metrics.check_convergence_threshold(0.8)\n",
    "            metrics.check_convergence_threshold(0.9)\n",
    "            \n",
    "            # Compute additional metrics for printing\n",
    "            avg_score = np.mean(metrics.episode_scores[-100:])\n",
    "            max_score = max(metrics.episode_scores[-100:])\n",
    "            avg_reward = np.mean(metrics.episode_rewards[-100:])\n",
    "            avg_reward_per_step = np.mean([r/l if l>0 else 0 for r,l in zip(metrics.episode_rewards[-100:], metrics.episode_lengths[-100:])])\n",
    "            current_epsilon = metrics.epsilon_history[-1] if metrics.epsilon_history else 0\n",
    "            \n",
    "            print(f\"Episode {episode+1:5d} | \"\n",
    "                f\"Avg Score: {avg_score:5.2f} | \"\n",
    "                f\"Max Score: {max_score:3d} | \"\n",
    "                f\"Avg Reward: {avg_reward:7.2f} | \"\n",
    "                f\"Avg Reward/Step: {avg_reward_per_step:.3f} | \"\n",
    "                f\"ε: {current_epsilon:.3f}\")\n",
    "\n",
    "    \n",
    "    if record_gameplay and metrics.best_episode_number>0:\n",
    "        best_video_path = f\"{recordings_dir}/BEST_episode_{metrics.best_episode_number:05d}_score_{metrics.best_episode_score}.mp4\"\n",
    "        record_episode(env, get_state, policy_net, 0.0, metrics.best_episode_number, agent_name, seed, best_video_path)\n",
    "    \n",
    "    torch.save(policy_net.state_dict(), f\"dqn_policy_net_seed{seed}.pth\")\n",
    "    metrics.save(f\"metrics_{agent_name.lower().replace(' ','_')}_seed{seed}.pkl\")\n",
    "    \n",
    "    return policy_net, metrics\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN EXECUTION\n",
    "# =============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    seeds = [42, 123, 456]\n",
    "    all_metrics = []\n",
    "    for seed in seeds:\n",
    "        policy_net, metrics = train_dqn(seed=seed, num_episodes=1000, agent_name=\"DQN\", record_gameplay=True, record_frequency=200)\n",
    "        all_metrics.append(metrics)\n",
    "        time.sleep(2)\n",
    "    \n",
    "    print(\"\\n🎉 All DQN training runs complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48865e02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "DQN VIDEO COMPILATION CREATOR\n",
      "================================================================================\n",
      "\n",
      "Found 5 recorded episodes\n",
      "Creating progression video: progression_dqn_seed42.mp4\n",
      "  Processing episode_00200.mp4...\n",
      "  Processing episode_00400.mp4...\n",
      "  Processing episode_00600.mp4...\n",
      "  Processing episode_00800.mp4...\n",
      "  Processing episode_01000.mp4...\n",
      "✅ Compilation video saved: progression_dqn_seed42.mp4\n",
      "Found 5 recorded episodes\n",
      "Creating progression video: progression_dqn_seed123.mp4\n",
      "  Processing episode_00200.mp4...\n",
      "  Processing episode_00400.mp4...\n",
      "  Processing episode_00600.mp4...\n",
      "  Processing episode_00800.mp4...\n",
      "  Processing episode_01000.mp4...\n",
      "✅ Compilation video saved: progression_dqn_seed123.mp4\n",
      "Found 5 recorded episodes\n",
      "Creating progression video: progression_dqn_seed456.mp4\n",
      "  Processing episode_00200.mp4...\n",
      "  Processing episode_00400.mp4...\n",
      "  Processing episode_00600.mp4...\n",
      "  Processing episode_00800.mp4...\n",
      "  Processing episode_01000.mp4...\n",
      "✅ Compilation video saved: progression_dqn_seed456.mp4\n",
      "Creating best episodes compilation for DQN...\n",
      "  Adding BEST_episode_00612_score_19.mp4...\n",
      "  Adding BEST_episode_00692_score_19.mp4...\n",
      "  Adding BEST_episode_00954_score_18.mp4...\n",
      "✅ Best episodes compilation saved: best_episodes_dqn.mp4\n",
      "\n",
      "🎉 All DQN compilation videos created!\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# =========================\n",
    "# Safe text overlay helper\n",
    "# =========================\n",
    "def safe_put_text(img, text, pos, font_scale=0.4, color=(255,255,255), thickness=1):\n",
    "    \"\"\"\n",
    "    Safely put text on a frame (ensures contiguous array and correct type)\n",
    "    \"\"\"\n",
    "    img = np.ascontiguousarray(img, dtype=np.uint8)\n",
    "    cv2.putText(img, text, pos, cv2.FONT_HERSHEY_SIMPLEX, font_scale, color, thickness, cv2.LINE_AA)\n",
    "    return img\n",
    "\n",
    "# =========================\n",
    "# Training progression video\n",
    "# =========================\n",
    "def create_training_progression_video(recordings_dir, output_path='training_progression.mp4'):\n",
    "    \"\"\"\n",
    "    Create a compilation showing DQN training progression\n",
    "    \"\"\"\n",
    "    video_files = sorted(Path(recordings_dir).glob(\"episode_*.mp4\"))\n",
    "    \n",
    "    if not video_files:\n",
    "        print(f\"❌ No videos found in {recordings_dir}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(video_files)} recorded episodes\")\n",
    "    print(f\"Creating progression video: {output_path}\")\n",
    "    \n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    fps = 15\n",
    "    \n",
    "    first_video = cv2.VideoCapture(str(video_files[0]))\n",
    "    width = int(first_video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(first_video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    first_video.release()\n",
    "    \n",
    "    output_video = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "    \n",
    "    for video_file in video_files:\n",
    "        print(f\"  Processing {video_file.name}...\")\n",
    "        cap = cv2.VideoCapture(str(video_file))\n",
    "        \n",
    "        # Title frame\n",
    "        title_frame = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "        title_frame = np.ascontiguousarray(title_frame, dtype=np.uint8)\n",
    "        text = f\"Episode {video_file.stem.split('_')[1]}\"\n",
    "        text_size = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.8, 2)[0]\n",
    "        text_x = (width - text_size[0]) // 2\n",
    "        text_y = (height + text_size[1]) // 2\n",
    "        title_frame = safe_put_text(title_frame, text, (text_x, text_y), font_scale=0.8, thickness=2)\n",
    "        \n",
    "        for _ in range(fps):\n",
    "            output_video.write(title_frame)\n",
    "        \n",
    "        # Write video frames\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame = np.ascontiguousarray(frame, dtype=np.uint8)\n",
    "            output_video.write(frame)\n",
    "        cap.release()\n",
    "        \n",
    "        # Black separator\n",
    "        black_frame = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "        black_frame = np.ascontiguousarray(black_frame, dtype=np.uint8)\n",
    "        for _ in range(fps // 2):\n",
    "            output_video.write(black_frame)\n",
    "    \n",
    "    output_video.release()\n",
    "    print(f\"✅ Compilation video saved: {output_path}\")\n",
    "\n",
    "# =========================\n",
    "# Best episodes compilation\n",
    "# =========================\n",
    "def create_best_episodes_compilation(seed_list=[42, 123, 456], agent_name=\"DQN\"):\n",
    "    \"\"\"\n",
    "    Create compilation of best episodes from all DQN seeds\n",
    "    \"\"\"\n",
    "    output_path = f'best_episodes_{agent_name.lower()}.mp4'\n",
    "    print(f\"Creating best episodes compilation for {agent_name}...\")\n",
    "    \n",
    "    best_videos = []\n",
    "    for seed in seed_list:\n",
    "        recordings_dir = f\"recordings_{agent_name.lower()}_seed{seed}\"\n",
    "        best_video = list(Path(recordings_dir).glob(\"BEST_*.mp4\"))\n",
    "        if best_video:\n",
    "            best_videos.extend(best_video)\n",
    "    \n",
    "    if not best_videos:\n",
    "        print(\"❌ No best episode videos found\")\n",
    "        return\n",
    "    \n",
    "    cap = cv2.VideoCapture(str(best_videos[0]))\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    cap.release()\n",
    "    \n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    output_video = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "    \n",
    "    for video_file in best_videos:\n",
    "        print(f\"  Adding {video_file.name}...\")\n",
    "        \n",
    "        # Title frame\n",
    "        title_frame = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "        title_frame = np.ascontiguousarray(title_frame, dtype=np.uint8)\n",
    "        title_text = video_file.stem.replace('_', ' ').title()\n",
    "        title_frame = safe_put_text(title_frame, title_text, (10, height//2), font_scale=0.4, thickness=1)\n",
    "        \n",
    "        for _ in range(fps):\n",
    "            output_video.write(title_frame)\n",
    "        \n",
    "        # Video frames\n",
    "        cap = cv2.VideoCapture(str(video_file))\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame = np.ascontiguousarray(frame, dtype=np.uint8)\n",
    "            output_video.write(frame)\n",
    "        cap.release()\n",
    "        \n",
    "        # Black separator\n",
    "        black_frame = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "        black_frame = np.ascontiguousarray(black_frame, dtype=np.uint8)\n",
    "        for _ in range(fps):\n",
    "            output_video.write(black_frame)\n",
    "    \n",
    "    output_video.release()\n",
    "    print(f\"✅ Best episodes compilation saved: {output_path}\")\n",
    "\n",
    "# =========================\n",
    "# MAIN EXECUTION\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"DQN VIDEO COMPILATION CREATOR\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    seeds = [42, 123, 456]\n",
    "    \n",
    "    # Progression videos\n",
    "    for seed in seeds:\n",
    "        recordings_dir = f\"recordings_dqn_seed{seed}\"\n",
    "        if Path(recordings_dir).exists():\n",
    "            create_training_progression_video(\n",
    "                recordings_dir,\n",
    "                f'progression_dqn_seed{seed}.mp4'\n",
    "            )\n",
    "    \n",
    "    # Best episodes compilation\n",
    "    create_best_episodes_compilation(seeds, \"DQN\")\n",
    "    \n",
    "    print(\"\\n🎉 All DQN compilation videos created!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6fe34037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "COMPREHENSIVE METRICS VISUALIZATION (DQN)\n",
      "================================================================================\n",
      "\n",
      "Loading DQN metrics files...\n",
      "✅ Loaded 3 DQN metric files\n",
      "\n",
      "Generating visualizations...\n",
      "\n",
      "✅ Saved: 1_sample_efficiency_dqn.png\n",
      "✅ Saved: 2_exploration_stability_dqn.png\n",
      "✅ Saved: 4_convergence_stability_dqn.png\n",
      "✅ Saved: 5_policy_behavior_dqn.png\n",
      "\n",
      "Generating summary report...\n",
      "\n",
      "✅ Saved summary report: summary_report_dqn.txt\n",
      "\n",
      "================================================================================\n",
      "🎉 ALL DQN VISUALIZATIONS COMPLETE!\n",
      "================================================================================\n",
      "\n",
      "Generated files:\n",
      "  1. 1_sample_efficiency_dqn.png\n",
      "  2. 2_exploration_stability_dqn.png\n",
      "  4. 4_convergence_stability_dqn.png\n",
      "  5. 5_policy_behavior_dqn.png\n",
      "  6. summary_report_dqn.txt\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "def load_metrics(pattern=\"metrics_dqn_seed*.pkl\"):\n",
    "    \"\"\"Load all metrics files matching pattern\"\"\"\n",
    "    from pathlib import Path\n",
    "\n",
    "    class MetricsLogger:\n",
    "        \"\"\"Dummy reconstruction class for DQN metrics\"\"\"\n",
    "        def __init__(self, agent_name, seed):\n",
    "            self.agent_name = agent_name\n",
    "            self.seed = seed\n",
    "\n",
    "    metrics_list = []\n",
    "    for file in Path('.').glob(pattern):\n",
    "        with open(file, 'rb') as f:\n",
    "            metrics = pickle.load(f)\n",
    "            # Reconstruct MetricsLogger object\n",
    "            m = MetricsLogger(metrics.get('agent_name', 'DQN'), metrics.get('seed', 0))\n",
    "            m.__dict__.update(metrics)\n",
    "            metrics_list.append(m)\n",
    "    return metrics_list\n",
    "\n",
    "\n",
    "def plot_sample_efficiency(metrics_list, save_path='1_sample_efficiency_dqn.png'):\n",
    "    \"\"\"Sample Efficiency Analysis\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('1. Sample Efficiency Analysis', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    colors = ['#2E86AB', '#A23B72', '#F18F01']\n",
    "    \n",
    "    # Plot 1: Reward vs Episodes\n",
    "    ax = axes[0, 0]\n",
    "    for i, m in enumerate(metrics_list):\n",
    "        episodes = range(1, len(m.moving_avg_reward) + 1)\n",
    "        ax.plot(episodes, m.moving_avg_reward, label=f'Seed {m.seed}', alpha=0.7, color=colors[i % len(colors)])\n",
    "    ax.set_xlabel('Episode')\n",
    "    ax.set_ylabel('Moving Avg Reward (window=100)')\n",
    "    ax.set_title('Learning Curves Across Seeds')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Score vs Episodes\n",
    "    ax = axes[0, 1]\n",
    "    for i, m in enumerate(metrics_list):\n",
    "        episodes = range(1, len(m.moving_avg_score) + 1)\n",
    "        ax.plot(episodes, m.moving_avg_score, label=f'Seed {m.seed}', alpha=0.7, color=colors[i % len(colors)])\n",
    "    ax.set_xlabel('Episode')\n",
    "    ax.set_ylabel('Moving Avg Score (window=100)')\n",
    "    ax.set_title('Score Progression Across Seeds')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Episodes to Convergence\n",
    "    ax = axes[1, 0]\n",
    "    thresholds = ['50%_max', '80%_max', '90%_max']\n",
    "    threshold_data = {t: [] for t in thresholds}\n",
    "    for m in metrics_list:\n",
    "        for t in thresholds:\n",
    "            value = m.episodes_to_threshold.get(t, np.nan)\n",
    "            threshold_data[t].append(value)\n",
    "    x_pos = np.arange(len(thresholds))\n",
    "    means = [np.nanmean(threshold_data[t]) for t in thresholds]\n",
    "    stds = [np.nanstd(threshold_data[t]) for t in thresholds]\n",
    "    ax.bar(x_pos, means, yerr=stds, capsize=5, color='#2E86AB', alpha=0.7)\n",
    "    ax.set_xticks(x_pos)\n",
    "    ax.set_xticklabels(thresholds)\n",
    "    ax.set_ylabel('Episodes Required')\n",
    "    ax.set_title('Sample Efficiency: Episodes to Reach Performance Thresholds')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Plot 4: Cumulative max score\n",
    "    ax = axes[1, 1]\n",
    "    for i, m in enumerate(metrics_list):\n",
    "        cumulative_max = np.maximum.accumulate(m.episode_scores)\n",
    "        episodes = range(1, len(cumulative_max) + 1)\n",
    "        ax.plot(episodes, cumulative_max, label=f'Seed {m.seed}', alpha=0.7, color=colors[i % len(colors)])\n",
    "    ax.set_xlabel('Episode')\n",
    "    ax.set_ylabel('Cumulative Max Score')\n",
    "    ax.set_title('Best Performance Over Time')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"✅ Saved: {save_path}\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_exploration_stability(metrics_list, save_path='2_exploration_stability_dqn.png'):\n",
    "    \"\"\"Exploration Stability\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('2. Exploration & Behavioral Stability', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    colors = ['#2E86AB', '#A23B72', '#F18F01']\n",
    "    \n",
    "    # Plot 1: Epsilon decay with reward overlay\n",
    "    ax = axes[0, 0]\n",
    "    ax2 = ax.twinx()\n",
    "    for i, m in enumerate(metrics_list):\n",
    "        episodes = range(1, len(m.epsilon_history) + 1)\n",
    "        ax.plot(episodes, m.epsilon_history, label=f'ε (Seed {m.seed})', alpha=0.5, linestyle='--', color=colors[i % len(colors)])\n",
    "        ax2.plot(episodes, m.moving_avg_reward, label=f'Reward (Seed {m.seed})', alpha=0.7, color=colors[i % len(colors)])\n",
    "    ax.set_xlabel('Episode')\n",
    "    ax.set_ylabel('Epsilon (Exploration Rate)')\n",
    "    ax2.set_ylabel('Moving Avg Reward')\n",
    "    ax.set_title('Exploration Decay vs Learning Performance')\n",
    "    ax.legend(loc='upper left')\n",
    "    ax2.legend(loc='upper right')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Reward variance per block\n",
    "    ax = axes[0, 1]\n",
    "    for i, m in enumerate(metrics_list):\n",
    "        blocks = range(1, len(m.reward_variance_per_block) + 1)\n",
    "        block_episodes = [b * 1000 for b in blocks]\n",
    "        ax.plot(block_episodes, m.reward_variance_per_block, label=f'Seed {m.seed}', marker='o', alpha=0.7, color=colors[i % len(colors)])\n",
    "    ax.set_xlabel('Episode (×1000)')\n",
    "    ax.set_ylabel('Reward Variance')\n",
    "    ax.set_title('Learning Stability: Reward Variance per 1000 Episodes')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Score variance per block\n",
    "    ax = axes[1, 0]\n",
    "    for i, m in enumerate(metrics_list):\n",
    "        blocks = range(1, len(m.score_variance_per_block) + 1)\n",
    "        block_episodes = [b * 1000 for b in blocks]\n",
    "        ax.plot(block_episodes, m.score_variance_per_block, label=f'Seed {m.seed}', marker='o', alpha=0.7, color=colors[i % len(colors)])\n",
    "    ax.set_xlabel('Episode (×1000)')\n",
    "    ax.set_ylabel('Score Variance')\n",
    "    ax.set_title('Performance Stability: Score Variance per 1000 Episodes')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 4: Action distribution (last seed only)\n",
    "    ax = axes[1, 1]\n",
    "    m = metrics_list[-1]\n",
    "    action_names = ['UP (0)', 'DOWN (1)', 'LEFT (2)', 'RIGHT (3)']\n",
    "    total_actions = sum(m.action_counts.values())\n",
    "    action_percentages = [m.action_counts[i] / total_actions * 100 for i in range(4)]\n",
    "    ax.bar(action_names, action_percentages, color='#2E86AB', alpha=0.7)\n",
    "    ax.set_ylabel('Percentage of Total Actions (%)')\n",
    "    ax.set_title(f'Action Distribution (Seed {m.seed})')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"✅ Saved: {save_path}\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_computational_efficiency(metrics_list, save_path='3_computational_efficiency_dqn.png'):\n",
    "    \"\"\"Runtime / Computational Efficiency\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('3. Computational Efficiency', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    colors = ['#2E86AB', '#A23B72', '#F18F01']\n",
    "    \n",
    "    # Plot 1: Training time\n",
    "    ax = axes[0, 0]\n",
    "    seeds = [m.seed for m in metrics_list]\n",
    "    training_times = [m.get_training_duration() / 60 for m in metrics_list]\n",
    "    ax.bar(range(len(seeds)), training_times, color='#2E86AB', alpha=0.7)\n",
    "    ax.set_xticks(range(len(seeds)))\n",
    "    ax.set_xticklabels([f'Seed {s}' for s in seeds])\n",
    "    ax.set_ylabel('Training Time (minutes)')\n",
    "    ax.set_title('Total Training Duration')\n",
    "    ax.axhline(np.mean(training_times), color='red', linestyle='--', label=f'Mean: {np.mean(training_times):.2f} min')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Plot 2: Average time per episode\n",
    "    ax = axes[0, 1]\n",
    "    avg_times = [m.get_avg_episode_time() * 1000 for m in metrics_list]\n",
    "    ax.bar(range(len(seeds)), avg_times, color='#A23B72', alpha=0.7)\n",
    "    ax.set_xticks(range(len(seeds)))\n",
    "    ax.set_xticklabels([f'Seed {s}' for s in seeds])\n",
    "    ax.set_ylabel('Time per Episode (ms)')\n",
    "    ax.set_title('Average Episode Duration')\n",
    "    ax.axhline(np.mean(avg_times), color='red', linestyle='--', label=f'Mean: {np.mean(avg_times):.2f} ms')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Plot 3: CPU usage\n",
    "    ax = axes[1, 0]\n",
    "    for i, m in enumerate(metrics_list):\n",
    "        sampled_episodes = range(0, len(m.cpu_usage), 100)\n",
    "        sampled_cpu = [m.cpu_usage[i] for i in sampled_episodes]\n",
    "        ax.plot(sampled_episodes, sampled_cpu, label=f'Seed {m.seed}', alpha=0.7, color=colors[i % len(colors)])\n",
    "    ax.set_xlabel('Episode')\n",
    "    ax.set_ylabel('CPU Usage (%)')\n",
    "    ax.set_title('CPU Utilization During Training')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 4: Memory usage\n",
    "    ax = axes[1, 1]\n",
    "    for i, m in enumerate(metrics_list):\n",
    "        sampled_episodes = range(0, len(m.memory_usage), 100)\n",
    "        sampled_mem = [m.memory_usage[i] for i in sampled_episodes]\n",
    "        ax.plot(sampled_episodes, sampled_mem, label=f'Seed {m.seed}', alpha=0.7, color=colors[i % len(colors)])\n",
    "    ax.set_xlabel('Episode')\n",
    "    ax.set_ylabel('Memory Usage (%)')\n",
    "    ax.set_title('Memory Utilization During Training')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"✅ Saved: {save_path}\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_convergence_stability(metrics_list, save_path='4_convergence_stability_dqn.png'):\n",
    "    \"\"\"Convergence and Stability Visualization\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('4. Convergence & Stability Analysis', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    colors = ['#2E86AB', '#A23B72', '#F18F01']\n",
    "    \n",
    "    # Plot 1: All reward curves together\n",
    "    ax = axes[0, 0]\n",
    "    for i, m in enumerate(metrics_list):\n",
    "        episodes = range(1, len(m.moving_avg_reward) + 1)\n",
    "        ax.plot(episodes, m.moving_avg_reward, label=f'Seed {m.seed}', alpha=0.7, linewidth=2, color=colors[i % len(colors)])\n",
    "    \n",
    "    max_len = max(len(m.moving_avg_reward) for m in metrics_list)\n",
    "    all_rewards = np.zeros((len(metrics_list), max_len))\n",
    "    for i, m in enumerate(metrics_list):\n",
    "        all_rewards[i, :len(m.moving_avg_reward)] = m.moving_avg_reward\n",
    "        if len(m.moving_avg_reward) < max_len:\n",
    "            all_rewards[i, len(m.moving_avg_reward):] = m.moving_avg_reward[-1]\n",
    "    \n",
    "    mean_reward = np.mean(all_rewards, axis=0)\n",
    "    std_reward = np.std(all_rewards, axis=0)\n",
    "    episodes = range(1, max_len + 1)\n",
    "    ax.fill_between(episodes, mean_reward - std_reward, mean_reward + std_reward, alpha=0.2, color='gray', label='±1 Std Dev')\n",
    "    ax.plot(episodes, mean_reward, 'k--', linewidth=2, label='Mean')\n",
    "    ax.set_xlabel('Episode')\n",
    "    ax.set_ylabel('Moving Avg Reward')\n",
    "    ax.set_title('Reward Convergence Across Multiple Seeds')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: All score curves together\n",
    "    ax = axes[0, 1]\n",
    "    for i, m in enumerate(metrics_list):\n",
    "        episodes = range(1, len(m.moving_avg_score) + 1)\n",
    "        ax.plot(episodes, m.moving_avg_score, label=f'Seed {m.seed}', alpha=0.7, linewidth=2, color=colors[i % len(colors)])\n",
    "    \n",
    "    all_scores = np.zeros((len(metrics_list), max_len))\n",
    "    for i, m in enumerate(metrics_list):\n",
    "        all_scores[i, :len(m.moving_avg_score)] = m.moving_avg_score\n",
    "        if len(m.moving_avg_score) < max_len:\n",
    "            all_scores[i, len(m.moving_avg_score):] = m.moving_avg_score[-1]\n",
    "    \n",
    "    mean_score = np.mean(all_scores, axis=0)\n",
    "    std_score = np.std(all_scores, axis=0)\n",
    "    ax.fill_between(episodes, mean_score - std_score, mean_score + std_score, alpha=0.2, color='gray', label='±1 Std Dev')\n",
    "    ax.plot(episodes, mean_score, 'k--', linewidth=2, label='Mean')\n",
    "    ax.set_xlabel('Episode')\n",
    "    ax.set_ylabel('Moving Avg Score')\n",
    "    ax.set_title('Score Convergence Across Multiple Seeds')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Variance over time\n",
    "    ax = axes[1, 0]\n",
    "    window = 100\n",
    "    variance_over_time = []\n",
    "    for i in range(0, max_len, window):\n",
    "        window_scores = all_scores[:, i:min(i+window, max_len)]\n",
    "        variance_over_time.append(np.mean(np.var(window_scores, axis=0)))\n",
    "    episodes_binned = range(window, max_len + 1, window)\n",
    "    ax.plot(episodes_binned, variance_over_time, marker='o', color='#2E86AB', linewidth=2)\n",
    "    ax.set_xlabel('Episode')\n",
    "    ax.set_ylabel(f'Mean Variance (per {window} episodes)')\n",
    "    ax.set_title('Stability: Inter-Seed Variance Over Training')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 4: Convergence speed histogram\n",
    "    ax = axes[1, 1]\n",
    "    conv_episodes = [m.episodes_to_threshold.get('80%_max', np.nan) for m in metrics_list]\n",
    "    ax.hist(conv_episodes, bins=len(metrics_list), color='#F18F01', alpha=0.7)\n",
    "    ax.set_xlabel('Episodes to Reach 80% of Max Performance')\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.set_title('Convergence Speed Across Seeds')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"✅ Saved: {save_path}\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_policy_behavior(metrics_list, save_path='5_policy_behavior_dqn.png'):\n",
    "    \"\"\"Visualizing agent's final policy behavior\"\"\"\n",
    "    m = metrics_list[-1]  # Use last seed\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    action_names = ['UP', 'DOWN', 'LEFT', 'RIGHT']\n",
    "    total_actions = sum(m.action_counts.values())\n",
    "    percentages = [m.action_counts[i] / total_actions * 100 for i in range(4)]\n",
    "    ax.bar(action_names, percentages, color='#2E86AB', alpha=0.7)\n",
    "    ax.set_ylabel('Percentage of Total Actions (%)')\n",
    "    ax.set_title(f'DQN Final Policy Action Distribution (Seed {m.seed})')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"✅ Saved: {save_path}\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def generate_summary_report(metrics_list, agent_name=\"DQN\", save_path='summary_report_dqn.txt'):\n",
    "    \"\"\"Generate textual summary report\"\"\"\n",
    "    with open(save_path, 'w') as f:\n",
    "        f.write(\"=\"*80 + \"\\n\")\n",
    "        f.write(f\"{agent_name} METRICS SUMMARY REPORT\\n\")\n",
    "        f.write(\"=\"*80 + \"\\n\\n\")\n",
    "        for m in metrics_list:\n",
    "            f.write(f\"Seed: {m.seed}\\n\")\n",
    "            f.write(f\"Final Moving Avg Reward: {m.moving_avg_reward[-1]:.2f}\\n\")\n",
    "            f.write(f\"Final Moving Avg Score: {m.moving_avg_score[-1]:.2f}\\n\")\n",
    "            f.write(f\"Episodes to 50% Max: {m.episodes_to_threshold.get('50%_max', 'N/A')}\\n\")\n",
    "            f.write(f\"Episodes to 80% Max: {m.episodes_to_threshold.get('80%_max', 'N/A')}\\n\")\n",
    "            f.write(f\"Episodes to 90% Max: {m.episodes_to_threshold.get('90%_max', 'N/A')}\\n\")\n",
    "            # f.write(f\"Training Duration (min): {m.get_training_duration() / 60:.2f}\\n\")\n",
    "            f.write(\"-\"*40 + \"\\n\")\n",
    "        f.write(\"\\nOverall Mean & Std Dev Across Seeds:\\n\")\n",
    "        final_rewards = [m.moving_avg_reward[-1] for m in metrics_list]\n",
    "        final_scores = [m.moving_avg_score[-1] for m in metrics_list]\n",
    "        f.write(f\"Mean Final Reward: {np.mean(final_rewards):.2f} ± {np.std(final_rewards):.2f}\\n\")\n",
    "        f.write(f\"Mean Final Score: {np.mean(final_scores):.2f} ± {np.std(final_scores):.2f}\\n\")\n",
    "    print(f\"✅ Saved summary report: {save_path}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"COMPREHENSIVE METRICS VISUALIZATION (DQN)\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    # Load metrics\n",
    "    print(\"Loading DQN metrics files...\")\n",
    "    metrics_list = load_metrics(\"metrics_dqn_seed*.pkl\")\n",
    "    \n",
    "    if not metrics_list:\n",
    "        print(\"❌ No DQN metrics files found! Run your DQN training first.\")\n",
    "        exit(1)\n",
    "    \n",
    "    print(f\"✅ Loaded {len(metrics_list)} DQN metric files\\n\")\n",
    "    \n",
    "    # Generate all visualizations\n",
    "    print(\"Generating visualizations...\\n\")\n",
    "    \n",
    "    plot_sample_efficiency(metrics_list, '1_sample_efficiency_dqn.png')\n",
    "    plot_exploration_stability(metrics_list, '2_exploration_stability_dqn.png')\n",
    "    # plot_computational_efficiency(metrics_list, '3_computational_efficiency_dqn.png')\n",
    "    plot_convergence_stability(metrics_list, '4_convergence_stability_dqn.png')\n",
    "    plot_policy_behavior(metrics_list, '5_policy_behavior_dqn.png')\n",
    "    \n",
    "    # Generate summary report\n",
    "    print(\"\\nGenerating summary report...\\n\")\n",
    "    generate_summary_report(metrics_list, agent_name=\"DQN\", save_path='summary_report_dqn.txt')\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"🎉 ALL DQN VISUALIZATIONS COMPLETE!\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\nGenerated files:\")\n",
    "    print(\"  1. 1_sample_efficiency_dqn.png\")\n",
    "    print(\"  2. 2_exploration_stability_dqn.png\")\n",
    "    # print(\"  3. 3_computational_efficiency_dqn.png\")\n",
    "    print(\"  4. 4_convergence_stability_dqn.png\")\n",
    "    print(\"  5. 5_policy_behavior_dqn.png\")\n",
    "    print(\"  6. summary_report_dqn.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4717a979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...existing code...\n",
    "def record_episode(env, state_func, policy_net, epsilon, episode_num, agent_name, seed,\n",
    "                   video_path, max_steps=1000):\n",
    "    \"\"\"\n",
    "    Evaluation-time recorder that tracks the last valid score even if the env resets on death.\n",
    "    \"\"\"\n",
    "    # Reset and size video from first frame\n",
    "    env.reset()\n",
    "    _ = state_func(env)\n",
    "\n",
    "    surf = getattr(env, \"screen\", None) or getattr(env, \"surface\", None)\n",
    "    assert surf is not None, \"SnakeGame has no screen/surface attribute\"\n",
    "\n",
    "    first_frame = pygame.surfarray.array3d(surf)\n",
    "    first_frame = np.transpose(first_frame, (1, 0, 2))\n",
    "    first_frame = cv2.cvtColor(first_frame, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    overlay_h = 50\n",
    "    h, w, _ = first_frame.shape\n",
    "    frame_h = h + overlay_h\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    fps = 15\n",
    "    vw = cv2.VideoWriter(video_path, fourcc, fps, (w, frame_h))\n",
    "\n",
    "    state = state_func(env)\n",
    "    done = False\n",
    "    steps = 0\n",
    "    total_reward = 0.0\n",
    "    actions_taken = []\n",
    "    last_score = max(0, getattr(env.snake, \"length\", 1) - 1)\n",
    "\n",
    "    while not done and steps < max_steps:\n",
    "        # Greedy (tiny exploration cap)\n",
    "        if random.random() < min(epsilon, 0.1):\n",
    "            action = random.randint(0, 3)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "                action = int(policy_net(state_tensor).argmax(dim=1).item())\n",
    "        actions_taken.append(action)\n",
    "\n",
    "        # Step and robust score tracking\n",
    "        prior_score = max(0, getattr(env.snake, \"length\", 1) - 1)\n",
    "        _, reward, done, info = env.step(action)\n",
    "        state = state_func(env)\n",
    "        total_reward += reward\n",
    "\n",
    "        info_score = info.get(\"score\") if isinstance(info, dict) else None\n",
    "        cur_score = max(0, getattr(env.snake, \"length\", 1) - 1)\n",
    "        last_score = max(last_score, prior_score, cur_score if info_score is None else int(info_score))\n",
    "\n",
    "        # Frame capture\n",
    "        surf = getattr(env, \"screen\", None) or getattr(env, \"surface\", None)\n",
    "        frame = pygame.surfarray.array3d(surf)\n",
    "        frame = np.transpose(frame, (1, 0, 2))\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        # Overlay\n",
    "        overlay = np.zeros((frame_h, w, 3), dtype=np.uint8)\n",
    "        overlay[0:h, :, :] = frame\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        fs, color, th = 0.35, (255, 255, 255), 1\n",
    "        y0 = h + 15\n",
    "        for i, text in enumerate([\n",
    "            f\"Agent: {agent_name}\",\n",
    "            f\"Seed: {seed}\",\n",
    "            f\"Episode: {episode_num}\",\n",
    "            f\"Step: {steps+1}\",\n",
    "            f\"Score: {last_score}\",\n",
    "        ]):\n",
    "            cv2.putText(overlay, text, (5, y0 + i*12), font, fs, color, th, cv2.LINE_AA)\n",
    "\n",
    "        vw.write(np.ascontiguousarray(overlay, dtype=np.uint8))\n",
    "        steps += 1\n",
    "\n",
    "    vw.release()\n",
    "    return last_score, total_reward, steps, actions_taken\n",
    "# ...existing code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9a9be1ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replay done. Score: 8, Steps: 52\n"
     ]
    }
   ],
   "source": [
    "from snake_gym.envs.snake import SnakeGame\n",
    "import torch\n",
    "\n",
    "seed = 42\n",
    "agent_name = \"DQN\"\n",
    "policy_net = DQN()\n",
    "policy_net.load_state_dict(torch.load(f\"dqn_policy_net_seed{seed}.pth\", map_location=\"cpu\"))\n",
    "policy_net.eval()\n",
    "\n",
    "env = SnakeGame()\n",
    "video_path = f\"replay_dqn_seed{seed}.mp4\"\n",
    "score, total_reward, steps, actions = record_episode(\n",
    "    env, get_state, policy_net, epsilon=0.0,\n",
    "    episode_num=1, agent_name=agent_name, seed=seed, video_path=video_path\n",
    ")\n",
    "print(f\"Replay done. Score: {score}, Steps: {steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60bdd86d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seaborn in c:\\users\\diana nicole\\snake-gym\\venv\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: pandas>=1.2 in c:\\users\\diana nicole\\snake-gym\\venv\\lib\\site-packages (from seaborn) (2.3.3)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in c:\\users\\diana nicole\\snake-gym\\venv\\lib\\site-packages (from seaborn) (2.2.6)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in c:\\users\\diana nicole\\snake-gym\\venv\\lib\\site-packages (from seaborn) (3.10.7)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\diana nicole\\snake-gym\\venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.9)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\diana nicole\\snake-gym\\venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\diana nicole\\snake-gym\\venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\diana nicole\\snake-gym\\venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (12.0.0)\n",
      "Requirement already satisfied: pyparsing>=3 in c:\\users\\diana nicole\\snake-gym\\venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.5)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\diana nicole\\snake-gym\\venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\diana nicole\\snake-gym\\venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.60.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\diana nicole\\snake-gym\\venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (25.0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\diana nicole\\snake-gym\\venv\\lib\\site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\diana nicole\\snake-gym\\venv\\lib\\site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\diana nicole\\snake-gym\\venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install seaborn\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
