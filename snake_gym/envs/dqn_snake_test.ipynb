{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "727aed34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install stable-baselines3 gym\n",
    "# !pip install \"shimmy>=2.0\"\n",
    "# !pip install opencv-python\n",
    "# !pip install seaborn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55fd2577",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
      "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
      "Users of this version of Gym should be able to simply replace 'import gym' with 'import gymnasium as gym' in the vast majority of cases.\n",
      "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n"
     ]
    }
   ],
   "source": [
    "# Add project root to import path (temporary)\n",
    "import sys\n",
    "sys.path.insert(0, r\"c:\\Users\\Acer\\Desktop\\snake-gym\")\n",
    "import snake_gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b69adc82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Acer\\Desktop\\snake-gym\\.venv\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:31: UserWarning: \u001b[33mWARN: A Box observation space has an unconventional shape (neither an image, nor a 1D vector). We recommend flattening the observation to have only a 1D vector or use a custom policy to properly process the data. Actual observation shape: (150, 150)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import snake_gym\n",
    "import random\n",
    "\n",
    "env = gym.make(\"snake-v0\")\n",
    "# obs = env.reset()\n",
    "# done = False\n",
    "\n",
    "# while not done:\n",
    "#     action = random.choice([0, 1, 2, 3])\n",
    "#     obs, reward, done, info = env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "517c0b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ðŸ† New best score: 1 at episode 21\n",
      "  ðŸ† New best score: 2 at episode 45\n",
      "  ðŸ† New best score: 4 at episode 75\n",
      "  âœ“ Reached 50%_max performance at episode 61\n",
      "  âœ“ Reached 80%_max performance at episode 75\n",
      "  âœ“ Reached 90%_max performance at episode 81\n",
      "  â†’ Logged block statistics for last 100 episodes.\n",
      "Episode   100 | Avg Score:  0.28 | Max Score:   4 | Avg Reward:   -1.85 | Avg Reward/Step: -0.067 | Îµ: 0.606\n",
      "  â†’ Logged block statistics for last 100 episodes.\n",
      "Episode   200 | Avg Score:  0.68 | Max Score:   4 | Avg Reward:    4.19 | Avg Reward/Step: 0.111 | Îµ: 0.367\n",
      "  ðŸ† New best score: 6 at episode 247\n",
      "  â†’ Logged block statistics for last 100 episodes.\n",
      "Episode   300 | Avg Score:  1.02 | Max Score:   6 | Avg Reward:    9.01 | Avg Reward/Step: 0.233 | Îµ: 0.222\n",
      "  ðŸ† New best score: 7 at episode 373\n",
      "  â†’ Logged block statistics for last 100 episodes.\n",
      "Episode   400 | Avg Score:  0.92 | Max Score:   7 | Avg Reward:   13.81 | Avg Reward/Step: 0.329 | Îµ: 0.135\n",
      "  ðŸ† New best score: 11 at episode 407\n",
      "  ðŸ† New best score: 12 at episode 421\n",
      "  â†’ Logged block statistics for last 100 episodes.\n",
      "Episode   500 | Avg Score:  1.35 | Max Score:  12 | Avg Reward:   25.63 | Avg Reward/Step: 0.555 | Îµ: 0.082\n",
      "  ðŸ† New best score: 13 at episode 516\n",
      "  â†’ Logged block statistics for last 100 episodes.\n",
      "Episode   600 | Avg Score:  0.72 | Max Score:  13 | Avg Reward:   27.45 | Avg Reward/Step: 0.553 | Îµ: 0.049\n",
      "  â†’ Logged block statistics for last 100 episodes.\n",
      "Episode   700 | Avg Score:  0.76 | Max Score:  12 | Avg Reward:   33.36 | Avg Reward/Step: 0.584 | Îµ: 0.030\n",
      "  â†’ Logged block statistics for last 100 episodes.\n",
      "Episode   800 | Avg Score:  0.81 | Max Score:  12 | Avg Reward:   43.69 | Avg Reward/Step: 0.641 | Îµ: 0.018\n",
      "  â†’ Logged block statistics for last 100 episodes.\n",
      "Episode   900 | Avg Score:  0.37 | Max Score:  10 | Avg Reward:   41.23 | Avg Reward/Step: 0.657 | Îµ: 0.011\n",
      "  ðŸ† New best score: 14 at episode 960\n",
      "  ðŸ† New best score: 16 at episode 970\n",
      "  â†’ Logged block statistics for last 100 episodes.\n",
      "Episode  1000 | Avg Score:  0.63 | Max Score:  16 | Avg Reward:   42.79 | Avg Reward/Step: 0.625 | Îµ: 0.010\n",
      "  â†’ Logged block statistics for last 100 episodes.\n",
      "Episode  1100 | Avg Score:  0.34 | Max Score:  15 | Avg Reward:   42.52 | Avg Reward/Step: 0.674 | Îµ: 0.010\n",
      "  ðŸ† New best score: 17 at episode 1116\n",
      "  â†’ Logged block statistics for last 100 episodes.\n",
      "Episode  1200 | Avg Score:  0.38 | Max Score:  17 | Avg Reward:   41.85 | Avg Reward/Step: 0.637 | Îµ: 0.010\n",
      "  â†’ Logged block statistics for last 100 episodes.\n",
      "Episode  1300 | Avg Score:  0.53 | Max Score:  11 | Avg Reward:   37.64 | Avg Reward/Step: 0.621 | Îµ: 0.010\n",
      "  â†’ Logged block statistics for last 100 episodes.\n",
      "Episode  1400 | Avg Score:  0.49 | Max Score:  15 | Avg Reward:   43.52 | Avg Reward/Step: 0.657 | Îµ: 0.010\n",
      "  â†’ Logged block statistics for last 100 episodes.\n",
      "Episode  1500 | Avg Score:  0.39 | Max Score:   8 | Avg Reward:   39.84 | Avg Reward/Step: 0.653 | Îµ: 0.010\n",
      "  â†’ Logged block statistics for last 100 episodes.\n",
      "Episode  1600 | Avg Score:  0.39 | Max Score:  12 | Avg Reward:   46.13 | Avg Reward/Step: 0.677 | Îµ: 0.010\n",
      "  â†’ Logged block statistics for last 100 episodes.\n",
      "Episode  1700 | Avg Score:  0.83 | Max Score:  13 | Avg Reward:   44.36 | Avg Reward/Step: 0.679 | Îµ: 0.010\n",
      "  â†’ Logged block statistics for last 100 episodes.\n",
      "Episode  1800 | Avg Score:  0.43 | Max Score:  14 | Avg Reward:   48.14 | Avg Reward/Step: 0.622 | Îµ: 0.010\n",
      "  â†’ Logged block statistics for last 100 episodes.\n",
      "Episode  1900 | Avg Score:  0.70 | Max Score:  14 | Avg Reward:   48.95 | Avg Reward/Step: 0.701 | Îµ: 0.010\n",
      "  â†’ Logged block statistics for last 100 episodes.\n",
      "Episode  2000 | Avg Score:  0.18 | Max Score:   7 | Avg Reward:   49.45 | Avg Reward/Step: 0.677 | Îµ: 0.010\n",
      "\n",
      "ðŸŽ¬ Recording final gameplay with trained DQN agent...\n",
      "âœ… Final gameplay recording complete! Saved to recordings_dqn_seed42/FINAL_gameplay_seed42.mp4\n",
      "âœ… Metrics saved to metrics_dqn_seed42.pkl\n",
      "  ðŸ† New best score: 1 at episode 1\n",
      "  ðŸ† New best score: 2 at episode 29\n",
      "  âœ“ Reached 50%_max performance at episode 1\n",
      "  âœ“ Reached 80%_max performance at episode 1\n",
      "  âœ“ Reached 90%_max performance at episode 1\n",
      "  â†’ Logged block statistics for last 100 episodes.\n",
      "Episode   100 | Avg Score:  0.34 | Max Score:   2 | Avg Reward:   -0.76 | Avg Reward/Step: -0.080 | Îµ: 0.606\n",
      "  ðŸ† New best score: 4 at episode 101\n",
      "  ðŸ† New best score: 5 at episode 155\n",
      "  â†’ Logged block statistics for last 100 episodes.\n",
      "Episode   200 | Avg Score:  0.70 | Max Score:   5 | Avg Reward:    4.97 | Avg Reward/Step: 0.070 | Îµ: 0.367\n",
      "  â†’ Logged block statistics for last 100 episodes.\n",
      "Episode   300 | Avg Score:  0.70 | Max Score:   5 | Avg Reward:   11.86 | Avg Reward/Step: 0.318 | Îµ: 0.222\n",
      "  ðŸ† New best score: 7 at episode 301\n",
      "  ðŸ† New best score: 8 at episode 329\n",
      "  â†’ Logged block statistics for last 100 episodes.\n",
      "Episode   400 | Avg Score:  1.01 | Max Score:   8 | Avg Reward:   14.89 | Avg Reward/Step: 0.363 | Îµ: 0.135\n",
      "  ðŸ† New best score: 10 at episode 477\n",
      "  â†’ Logged block statistics for last 100 episodes.\n",
      "Episode   500 | Avg Score:  0.91 | Max Score:  10 | Avg Reward:   22.00 | Avg Reward/Step: 0.510 | Îµ: 0.082\n",
      "  ðŸ† New best score: 17 at episode 556\n",
      "  â†’ Logged block statistics for last 100 episodes.\n",
      "Episode   600 | Avg Score:  1.03 | Max Score:  17 | Avg Reward:   32.53 | Avg Reward/Step: 0.567 | Îµ: 0.049\n",
      "  â†’ Logged block statistics for last 100 episodes.\n",
      "Episode   700 | Avg Score:  0.74 | Max Score:  12 | Avg Reward:   38.81 | Avg Reward/Step: 0.599 | Îµ: 0.030\n",
      "  â†’ Logged block statistics for last 100 episodes.\n",
      "Episode   800 | Avg Score:  0.33 | Max Score:   8 | Avg Reward:   41.74 | Avg Reward/Step: 0.622 | Îµ: 0.018\n",
      "  â†’ Logged block statistics for last 100 episodes.\n",
      "Episode   900 | Avg Score:  0.30 | Max Score:   8 | Avg Reward:   45.32 | Avg Reward/Step: 0.651 | Îµ: 0.011\n",
      "  ðŸ† New best score: 18 at episode 926\n",
      "  â†’ Logged block statistics for last 100 episodes.\n",
      "Episode  1000 | Avg Score:  0.71 | Max Score:  18 | Avg Reward:   39.63 | Avg Reward/Step: 0.681 | Îµ: 0.010\n",
      "  â†’ Logged block statistics for last 100 episodes.\n",
      "Episode  1100 | Avg Score:  0.50 | Max Score:  12 | Avg Reward:   45.95 | Avg Reward/Step: 0.654 | Îµ: 0.010\n",
      "  â†’ Logged block statistics for last 100 episodes.\n",
      "Episode  1200 | Avg Score:  0.48 | Max Score:  12 | Avg Reward:   45.21 | Avg Reward/Step: 0.652 | Îµ: 0.010\n",
      "  â†’ Logged block statistics for last 100 episodes.\n",
      "Episode  1300 | Avg Score:  0.96 | Max Score:  13 | Avg Reward:   44.95 | Avg Reward/Step: 0.639 | Îµ: 0.010\n",
      "  â†’ Logged block statistics for last 100 episodes.\n",
      "Episode  1400 | Avg Score:  0.26 | Max Score:   9 | Avg Reward:   40.81 | Avg Reward/Step: 0.673 | Îµ: 0.010\n",
      "  â†’ Logged block statistics for last 100 episodes.\n",
      "Episode  1500 | Avg Score:  0.44 | Max Score:  13 | Avg Reward:   42.48 | Avg Reward/Step: 0.640 | Îµ: 0.010\n",
      "  â†’ Logged block statistics for last 100 episodes.\n",
      "Episode  1600 | Avg Score:  0.39 | Max Score:  12 | Avg Reward:   41.36 | Avg Reward/Step: 0.611 | Îµ: 0.010\n",
      "  â†’ Logged block statistics for last 100 episodes.\n",
      "Episode  1700 | Avg Score:  0.16 | Max Score:   7 | Avg Reward:   45.89 | Avg Reward/Step: 0.607 | Îµ: 0.010\n",
      "  â†’ Logged block statistics for last 100 episodes.\n",
      "Episode  1800 | Avg Score:  0.38 | Max Score:  10 | Avg Reward:   46.40 | Avg Reward/Step: 0.640 | Îµ: 0.010\n",
      "  â†’ Logged block statistics for last 100 episodes.\n",
      "Episode  1900 | Avg Score:  0.67 | Max Score:  12 | Avg Reward:   47.82 | Avg Reward/Step: 0.631 | Îµ: 0.010\n",
      "  â†’ Logged block statistics for last 100 episodes.\n",
      "Episode  2000 | Avg Score:  0.25 | Max Score:  13 | Avg Reward:   46.64 | Avg Reward/Step: 0.665 | Îµ: 0.010\n",
      "\n",
      "ðŸŽ¬ Recording final gameplay with trained DQN agent...\n",
      "âœ… Final gameplay recording complete! Saved to recordings_dqn_seed123/FINAL_gameplay_seed123.mp4\n",
      "âœ… Metrics saved to metrics_dqn_seed123.pkl\n",
      "  ðŸ† New best score: 1 at episode 11\n",
      "  ðŸ† New best score: 2 at episode 47\n",
      "  ðŸ† New best score: 4 at episode 77\n",
      "  âœ“ Reached 50%_max performance at episode 35\n",
      "  âœ“ Reached 80%_max performance at episode 79\n",
      "  âœ“ Reached 90%_max performance at episode 85\n",
      "  â†’ Logged block statistics for last 100 episodes.\n",
      "Episode   100 | Avg Score:  0.32 | Max Score:   4 | Avg Reward:   -1.53 | Avg Reward/Step: -0.087 | Îµ: 0.606\n",
      "  ðŸ† New best score: 8 at episode 193\n",
      "  â†’ Logged block statistics for last 100 episodes.\n",
      "Episode   200 | Avg Score:  0.85 | Max Score:   8 | Avg Reward:    5.44 | Avg Reward/Step: 0.136 | Îµ: 0.367\n",
      "  â†’ Logged block statistics for last 100 episodes.\n",
      "Episode   300 | Avg Score:  0.86 | Max Score:   6 | Avg Reward:   12.37 | Avg Reward/Step: 0.348 | Îµ: 0.222\n",
      "  ðŸ† New best score: 9 at episode 365\n",
      "  â†’ Logged block statistics for last 100 episodes.\n",
      "Episode   400 | Avg Score:  1.07 | Max Score:   9 | Avg Reward:   17.06 | Avg Reward/Step: 0.348 | Îµ: 0.135\n",
      "  ðŸ† New best score: 10 at episode 429\n",
      "  ðŸ† New best score: 13 at episode 481\n",
      "  â†’ Logged block statistics for last 100 episodes.\n",
      "Episode   500 | Avg Score:  0.87 | Max Score:  13 | Avg Reward:   21.39 | Avg Reward/Step: 0.440 | Îµ: 0.082\n",
      "  â†’ Logged block statistics for last 100 episodes.\n",
      "Episode   600 | Avg Score:  0.55 | Max Score:   8 | Avg Reward:   32.81 | Avg Reward/Step: 0.608 | Îµ: 0.049\n",
      "  ðŸ† New best score: 15 at episode 624\n",
      "  ðŸ† New best score: 17 at episode 676\n",
      "  â†’ Logged block statistics for last 100 episodes.\n",
      "Episode   700 | Avg Score:  1.10 | Max Score:  17 | Avg Reward:   35.66 | Avg Reward/Step: 0.602 | Îµ: 0.030\n",
      "  â†’ Logged block statistics for last 100 episodes.\n",
      "Episode   800 | Avg Score:  1.05 | Max Score:  15 | Avg Reward:   46.18 | Avg Reward/Step: 0.636 | Îµ: 0.018\n",
      "  â†’ Logged block statistics for last 100 episodes.\n",
      "Episode   900 | Avg Score:  0.63 | Max Score:  16 | Avg Reward:   41.38 | Avg Reward/Step: 0.681 | Îµ: 0.011\n",
      "  â†’ Logged block statistics for last 100 episodes.\n",
      "Episode  1000 | Avg Score:  0.30 | Max Score:  12 | Avg Reward:   43.13 | Avg Reward/Step: 0.639 | Îµ: 0.010\n",
      "  â†’ Logged block statistics for last 100 episodes.\n",
      "Episode  1100 | Avg Score:  0.44 | Max Score:   8 | Avg Reward:   45.03 | Avg Reward/Step: 0.633 | Îµ: 0.010\n",
      "  â†’ Logged block statistics for last 100 episodes.\n",
      "Episode  1200 | Avg Score:  0.18 | Max Score:  11 | Avg Reward:   48.00 | Avg Reward/Step: 0.673 | Îµ: 0.010\n",
      "  â†’ Logged block statistics for last 100 episodes.\n",
      "Episode  1300 | Avg Score:  0.48 | Max Score:  13 | Avg Reward:   47.68 | Avg Reward/Step: 0.654 | Îµ: 0.010\n",
      "  â†’ Logged block statistics for last 100 episodes.\n",
      "Episode  1400 | Avg Score:  0.34 | Max Score:  12 | Avg Reward:   50.58 | Avg Reward/Step: 0.662 | Îµ: 0.010\n",
      "  â†’ Logged block statistics for last 100 episodes.\n",
      "Episode  1500 | Avg Score:  0.41 | Max Score:  14 | Avg Reward:   43.80 | Avg Reward/Step: 0.640 | Îµ: 0.010\n",
      "  â†’ Logged block statistics for last 100 episodes.\n",
      "Episode  1600 | Avg Score:  0.19 | Max Score:   9 | Avg Reward:   43.84 | Avg Reward/Step: 0.619 | Îµ: 0.010\n",
      "  ðŸ† New best score: 19 at episode 1624\n",
      "  â†’ Logged block statistics for last 100 episodes.\n",
      "Episode  1700 | Avg Score:  0.39 | Max Score:  19 | Avg Reward:   44.01 | Avg Reward/Step: 0.636 | Îµ: 0.010\n",
      "  â†’ Logged block statistics for last 100 episodes.\n",
      "Episode  1800 | Avg Score:  0.23 | Max Score:   7 | Avg Reward:   48.37 | Avg Reward/Step: 0.603 | Îµ: 0.010\n",
      "  â†’ Logged block statistics for last 100 episodes.\n",
      "Episode  1900 | Avg Score:  0.12 | Max Score:   6 | Avg Reward:   41.85 | Avg Reward/Step: 0.665 | Îµ: 0.010\n",
      "  â†’ Logged block statistics for last 100 episodes.\n",
      "Episode  2000 | Avg Score:  0.71 | Max Score:  14 | Avg Reward:   45.97 | Avg Reward/Step: 0.664 | Îµ: 0.010\n",
      "\n",
      "ðŸŽ¬ Recording final gameplay with trained DQN agent...\n",
      "âœ… Final gameplay recording complete! Saved to recordings_dqn_seed456/FINAL_gameplay_seed456.mp4\n",
      "âœ… Metrics saved to metrics_dqn_seed456.pkl\n",
      "\n",
      "ðŸŽ‰ All DQN training runs complete!\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "import psutil\n",
    "import pickle\n",
    "import cv2\n",
    "import pygame\n",
    "from collections import deque\n",
    "from snake_gym.envs.snake import SnakeGame\n",
    "from snake_gym.envs.modules import GRIDSIZE\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "\n",
    "# =============================================================================\n",
    "# METRICS LOGGER\n",
    "# =============================================================================\n",
    "class MetricsLogger:\n",
    "    \"\"\"Comprehensive metrics tracking for RL training\"\"\"\n",
    "    \n",
    "    def __init__(self, agent_name, seed, record_gameplay=True, record_frequency=500):\n",
    "        self.agent_name = agent_name\n",
    "        self.seed = seed\n",
    "        self.start_time = time.time()\n",
    "        self.record_gameplay = record_gameplay\n",
    "        self.record_frequency = record_frequency\n",
    "        self.recorded_episodes = []\n",
    "        self.best_episode_score = 0\n",
    "        self.best_episode_number = 0\n",
    "        self.episode_rewards = []\n",
    "        self.episode_scores = []\n",
    "        self.episode_lengths = []\n",
    "        self.episode_times = []\n",
    "        self.reward_variance_per_block = []\n",
    "        self.score_variance_per_block = []\n",
    "        self.epsilon_history = []\n",
    "        self.action_counts = {0:0,1:0,2:0,3:0}\n",
    "        self.action_history_per_block = []\n",
    "        self.cpu_usage = []\n",
    "        self.memory_usage = []\n",
    "        self.moving_avg_reward = []\n",
    "        self.moving_avg_score = []\n",
    "        self.episodes_to_threshold = {}\n",
    "    \n",
    "    def should_record_episode(self, episode_num):\n",
    "        if not self.record_gameplay:\n",
    "            return False\n",
    "        if episode_num % self.record_frequency == 0:\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def is_best_episode(self, score):\n",
    "        return score > self.best_episode_score\n",
    "    \n",
    "    def update_best_episode(self, episode_num, score):\n",
    "        if score > self.best_episode_score:\n",
    "            self.best_episode_score = score\n",
    "            self.best_episode_number = episode_num\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def log_episode(self, reward, score, length, epsilon, episode_time, actions_taken):\n",
    "        self.episode_rewards.append(reward)\n",
    "        self.episode_scores.append(score)\n",
    "        self.episode_lengths.append(length)\n",
    "        self.episode_times.append(episode_time)\n",
    "        self.epsilon_history.append(epsilon)\n",
    "        for action in actions_taken:\n",
    "            self.action_counts[action] += 1\n",
    "        window = 100\n",
    "        if len(self.episode_rewards) >= window:\n",
    "            self.moving_avg_reward.append(np.mean(self.episode_rewards[-window:]))\n",
    "            self.moving_avg_score.append(np.mean(self.episode_scores[-window:]))\n",
    "        else:\n",
    "            self.moving_avg_reward.append(np.mean(self.episode_rewards))\n",
    "            self.moving_avg_score.append(np.mean(self.episode_scores))\n",
    "        self.cpu_usage.append(psutil.cpu_percent(interval=0.1))\n",
    "        self.memory_usage.append(psutil.virtual_memory().percent)\n",
    "    \n",
    "    def log_block_statistics(self, block_size=1000):\n",
    "        if len(self.episode_rewards) >= block_size:\n",
    "            recent_rewards = self.episode_rewards[-block_size:]\n",
    "            recent_scores = self.episode_scores[-block_size:]\n",
    "            self.reward_variance_per_block.append(np.var(recent_rewards))\n",
    "            self.score_variance_per_block.append(np.var(recent_scores))\n",
    "            total_actions = sum(self.action_counts.values())\n",
    "            if total_actions > 0:\n",
    "                action_dist = {k: v/total_actions for k, v in self.action_counts.items()}\n",
    "                self.action_history_per_block.append(action_dist)\n",
    "    \n",
    "    def check_convergence_threshold(self, threshold_percent=0.8):\n",
    "        if len(self.moving_avg_score) < 100:\n",
    "            return\n",
    "        max_score = max(self.moving_avg_score)\n",
    "        threshold = threshold_percent * max_score\n",
    "        threshold_name = f\"{int(threshold_percent*100)}%_max\"\n",
    "        if threshold_name not in self.episodes_to_threshold:\n",
    "            for i, score in enumerate(self.moving_avg_score):\n",
    "                if score >= threshold:\n",
    "                    self.episodes_to_threshold[threshold_name] = i + 1\n",
    "                    print(f\"  âœ“ Reached {threshold_name} performance at episode {i+1}\")\n",
    "                    break\n",
    "    \n",
    "    def get_training_duration(self):\n",
    "        if self.start_time is not None and self.end_time is not None:\n",
    "            return self.end_time - self.start_time\n",
    "        elif self.start_time is not None:\n",
    "            # fallback if end_time not yet set\n",
    "            return time.time() - self.start_time\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def get_avg_episode_time(self):\n",
    "        return np.mean(self.episode_times) if self.episode_times else 0\n",
    "    \n",
    "    def save(self, filename):\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump(self.__dict__, f)\n",
    "        print(f\"âœ… Metrics saved to {filename}\")\n",
    "\n",
    "# =============================================================================\n",
    "# DQN NETWORK AND REPLAY BUFFER\n",
    "# =============================================================================\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim=12, output_dim=4):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, output_dim)\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=10000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    def sample(self, batch_size=64):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        return np.array(states), actions, rewards, np.array(next_states), dones\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# =============================================================================\n",
    "# STATE REPRESENTATION\n",
    "# =============================================================================\n",
    "def get_state(env):\n",
    "    head_x, head_y = env.snake.get_head_position()\n",
    "    apple_x, apple_y = env.apple.position\n",
    "    direction = env.snake.direction\n",
    "    def is_danger(dx, dy):\n",
    "        new_x = head_x + dx * GRIDSIZE\n",
    "        new_y = head_y + dy * GRIDSIZE\n",
    "        if new_x < 0 or new_x >= 150 or new_y < 0 or new_y >= 150:\n",
    "            return 1\n",
    "        if (new_x, new_y) in env.snake.positions[:-1]:\n",
    "            return 1\n",
    "        return 0\n",
    "    danger_up = is_danger(0, -GRIDSIZE)\n",
    "    danger_down = is_danger(0, GRIDSIZE)\n",
    "    danger_left = is_danger(-GRIDSIZE, 0)\n",
    "    danger_right = is_danger(GRIDSIZE, 0)\n",
    "    apple_up = int(apple_y < head_y)\n",
    "    apple_down = int(apple_y > head_y)\n",
    "    apple_left = int(apple_x < head_x)\n",
    "    apple_right = int(apple_x > head_x)\n",
    "    dir_up = int(direction == (0, -1))\n",
    "    dir_down = int(direction == (0, 1))\n",
    "    dir_left = int(direction == (-1, 0))\n",
    "    dir_right = int(direction == (1, 0))\n",
    "    return (danger_up, danger_down, danger_left, danger_right,\n",
    "            apple_up, apple_down, apple_left, apple_right,\n",
    "            dir_up, dir_down, dir_left, dir_right)\n",
    "\n",
    "# =============================================================================\n",
    "# VIDEO RECORDING\n",
    "# =============================================================================\n",
    "def add_info_overlay(frame, episode, steps, score, agent_name, seed):\n",
    "    overlay = np.zeros((200, 150, 3), dtype=np.uint8)\n",
    "    overlay[0:150, :] = frame\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    font_scale = 0.35\n",
    "    color = (255,255,255)\n",
    "    thickness = 1\n",
    "    text_lines = [\n",
    "        f\"Agent: {agent_name}\",\n",
    "        f\"Seed: {seed}\",\n",
    "        f\"Episode: {episode}\",\n",
    "        f\"Step: {steps}\",\n",
    "        f\"Score: {score}\"\n",
    "    ]\n",
    "    y_offset = 160\n",
    "    for i, text in enumerate(text_lines):\n",
    "        cv2.putText(overlay, text, (5, y_offset + i*10), font, font_scale, color, thickness, cv2.LINE_AA)\n",
    "    return overlay\n",
    "\n",
    "def record_episode(env, state_func, policy_net, epsilon, episode_num, agent_name, seed, video_path, max_steps=1000):\n",
    "    # Capture first frame to determine video size\n",
    "    state = env.reset()\n",
    "    state = state_func(env)\n",
    "    first_frame = pygame.surfarray.array3d(env.screen)\n",
    "    first_frame = np.transpose(first_frame, (1,0,2))\n",
    "    first_frame = cv2.cvtColor(first_frame, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    overlay_height = 50\n",
    "    height, width, _ = first_frame.shape\n",
    "    frame_height = height + overlay_height\n",
    "\n",
    "    # Initialize VideoWriter\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    fps = 15\n",
    "    video_writer = cv2.VideoWriter(video_path, fourcc, fps, (width, frame_height))\n",
    "\n",
    "    state = env.reset()\n",
    "    state = state_func(env)\n",
    "    done = False\n",
    "    steps = 0\n",
    "    total_reward = 0\n",
    "    actions_taken = []\n",
    "\n",
    "    while not done and steps < max_steps:\n",
    "        if random.random() < min(epsilon, 0.1):\n",
    "            action = random.randint(0,3)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                state_tensor = torch.tensor(state).float().unsqueeze(0)\n",
    "                action = policy_net(state_tensor).argmax().item()\n",
    "\n",
    "        actions_taken.append(action)\n",
    "        _, reward, done, _ = env.step(action)\n",
    "        state = state_func(env)\n",
    "        total_reward += reward\n",
    "\n",
    "        # Capture frame\n",
    "        frame = pygame.surfarray.array3d(env.screen)\n",
    "        frame = np.transpose(frame, (1,0,2))\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        # Add overlay\n",
    "        overlay = np.zeros((frame_height, width, 3), dtype=np.uint8)\n",
    "        overlay[0:height,:,:] = frame\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        font_scale = 0.35\n",
    "        color = (255,255,255)\n",
    "        thickness = 1\n",
    "        y_offset = height + 15\n",
    "        text_lines = [\n",
    "            f\"Agent: {agent_name}\",\n",
    "            f\"Seed: {seed}\",\n",
    "            f\"Episode: {episode_num}\",\n",
    "            f\"Step: {steps}\",\n",
    "            f\"Score: {env.snake.length-1}\"\n",
    "        ]\n",
    "        for i, text in enumerate(text_lines):\n",
    "            cv2.putText(overlay, text, (5, y_offset + i*12), font, font_scale, color, thickness, cv2.LINE_AA)\n",
    "\n",
    "        overlay = np.ascontiguousarray(overlay, dtype=np.uint8)\n",
    "        video_writer.write(overlay)\n",
    "        steps += 1\n",
    "\n",
    "    video_writer.release()\n",
    "    score = env.snake.length-1\n",
    "    return score, total_reward, steps, actions_taken\n",
    "\n",
    "# =============================================================================\n",
    "# DQN TRAINING\n",
    "# =============================================================================\n",
    "def train_dqn(seed=42, num_episodes=2000, agent_name=\"DQN\", record_gameplay=True, record_frequency=500):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    env = SnakeGame()\n",
    "    recordings_dir = f\"recordings_{agent_name.lower().replace(' ','_')}_seed{seed}\"\n",
    "    os.makedirs(recordings_dir, exist_ok=True)\n",
    "    \n",
    "    input_dim = 12\n",
    "    output_dim = 4\n",
    "    policy_net = DQN(input_dim, output_dim)\n",
    "    target_net = DQN(input_dim, output_dim)\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    target_net.eval()\n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr=0.001)\n",
    "    criterion = nn.MSELoss()\n",
    "    replay_buffer = ReplayBuffer(10000)\n",
    "    \n",
    "    batch_size = 64\n",
    "    gamma = 0.95\n",
    "    epsilon = 1.0\n",
    "    epsilon_min = 0.01\n",
    "    epsilon_decay = 0.995\n",
    "    target_update = 10\n",
    "    \n",
    "    metrics = MetricsLogger(agent_name, seed, record_gameplay, record_frequency)\n",
    "    metrics.start_time = time.time()  # âœ… Start timing training\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        episode_start = time.time()\n",
    "        should_record = metrics.should_record_episode(episode+1)\n",
    "        \n",
    "        if should_record:\n",
    "            video_path = f\"{recordings_dir}/episode_{episode+1:05d}.mp4\"\n",
    "            score, total_reward, steps, actions_taken = record_episode(env, get_state, policy_net, epsilon, episode+1, agent_name, seed, video_path)\n",
    "            metrics.recorded_episodes.append(episode+1)\n",
    "        else:\n",
    "            state = get_state(env)\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "            steps = 0\n",
    "            actions_taken = []\n",
    "            while not done and steps < 1000:\n",
    "                \n",
    "                if random.random() < epsilon:\n",
    "                    action = random.randint(0,3)\n",
    "                else:\n",
    "                    with torch.no_grad():\n",
    "                        state_tensor = torch.tensor(state).float().unsqueeze(0)\n",
    "                        action = policy_net(state_tensor).argmax().item()\n",
    "                        \n",
    "                actions_taken.append(action)\n",
    "                _, reward, done, _ = env.step(action)\n",
    "                next_state = get_state(env)\n",
    "                \n",
    "                replay_buffer.push(state, action, reward, next_state, done)\n",
    "                if len(replay_buffer) >= batch_size:\n",
    "                    s, a, r, s_next, d = replay_buffer.sample(batch_size)\n",
    "                    \n",
    "                    s_tensor = torch.tensor(s).float()\n",
    "                    a_tensor = torch.tensor(a).long().unsqueeze(1)\n",
    "                    r_tensor = torch.tensor(r).float().unsqueeze(1)\n",
    "                    s_next_tensor = torch.tensor(s_next).float()\n",
    "                    d_tensor = torch.tensor(d).float().unsqueeze(1)\n",
    "                    \n",
    "                    q_values = policy_net(s_tensor).gather(1, a_tensor)\n",
    "                    next_q_values = target_net(s_next_tensor).max(1)[0].unsqueeze(1)\n",
    "                    target = r_tensor + gamma * next_q_values * (1 - d_tensor)\n",
    "                    loss = criterion(q_values, target)\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                state = next_state\n",
    "                total_reward += reward\n",
    "                steps +=1\n",
    "            score = env.snake.length-1\n",
    "        \n",
    "        if metrics.update_best_episode(episode+1, score):\n",
    "            print(f\"  ðŸ† New best score: {score} at episode {episode+1}\")\n",
    "        \n",
    "        epsilon = max(epsilon_min, epsilon*epsilon_decay)\n",
    "        if episode % target_update == 0:\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "        \n",
    "        metrics.log_episode(total_reward, score, steps, epsilon, time.time()-episode_start, actions_taken)\n",
    "        \n",
    "        if (episode+1) % 100 ==0:\n",
    "            metrics.check_convergence_threshold(0.5)\n",
    "            metrics.check_convergence_threshold(0.8)\n",
    "            metrics.check_convergence_threshold(0.9)\n",
    "            \n",
    "            metrics.log_block_statistics(block_size=100)\n",
    "            print(f\"  â†’ Logged block statistics for last 100 episodes.\")\n",
    "            \n",
    "            # Compute additional metrics for printing\n",
    "            avg_score = np.mean(metrics.episode_scores[-100:])\n",
    "            max_score = max(metrics.episode_scores[-100:])\n",
    "            avg_reward = np.mean(metrics.episode_rewards[-100:])\n",
    "            avg_reward_per_step = np.mean([r/l if l>0 else 0 for r,l in zip(metrics.episode_rewards[-100:], metrics.episode_lengths[-100:])])\n",
    "            current_epsilon = metrics.epsilon_history[-1] if metrics.epsilon_history else 0\n",
    "            \n",
    "            \n",
    "            \n",
    "            print(f\"Episode {episode+1:5d} | \"\n",
    "                f\"Avg Score: {avg_score:5.2f} | \"\n",
    "                f\"Max Score: {max_score:3d} | \"\n",
    "                f\"Avg Reward: {avg_reward:7.2f} | \"\n",
    "                f\"Avg Reward/Step: {avg_reward_per_step:.3f} | \"\n",
    "                f\"Îµ: {current_epsilon:.3f}\")\n",
    "\n",
    "    metrics.end_time = time.time()  # âœ… End timing training\n",
    "    \n",
    "    if record_gameplay and metrics.best_episode_number>0:\n",
    "        best_video_path = f\"{recordings_dir}/BEST_episode_{metrics.best_episode_number:05d}_score_{metrics.best_episode_score}.mp4\"\n",
    "        record_episode(env, get_state, policy_net, 0.0, metrics.best_episode_number, agent_name, seed, best_video_path)\n",
    "    \n",
    "    # ðŸŽ¬ Record final gameplay after full training (fully greedy policy)\n",
    "    print(\"\\nðŸŽ¬ Recording final gameplay with trained DQN agent...\")\n",
    "    final_video_path = f\"{recordings_dir}/FINAL_gameplay_seed{seed}.mp4\"\n",
    "    env = SnakeGame()\n",
    "    record_episode(\n",
    "        env, get_state, policy_net,\n",
    "        epsilon=0.0,                      # no exploration\n",
    "        episode_num=num_episodes,         # tag as final episode\n",
    "        agent_name=\"DQN-Final\",\n",
    "        seed=seed,\n",
    "        video_path=final_video_path\n",
    "    )\n",
    "    print(f\"âœ… Final gameplay recording complete! Saved to {final_video_path}\")\n",
    "    \n",
    "    torch.save(policy_net.state_dict(), f\"dqn_policy_net_seed{seed}.pth\")\n",
    "    metrics.save(f\"metrics_{agent_name.lower().replace(' ','_')}_seed{seed}.pkl\")\n",
    "    \n",
    "    return policy_net, metrics\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN EXECUTION\n",
    "# =============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    seeds = [42, 123, 456]\n",
    "    all_metrics = []\n",
    "    for seed in seeds:\n",
    "        policy_net, metrics = train_dqn(seed=seed, num_episodes=2000, agent_name=\"DQN\", record_gameplay=True, record_frequency=500)\n",
    "        all_metrics.append(metrics)\n",
    "        time.sleep(2)\n",
    "    \n",
    "    print(\"\\nðŸŽ‰ All DQN training runs complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48865e02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "DQN VIDEO COMPILATION CREATOR\n",
      "================================================================================\n",
      "\n",
      "Found 8 recorded episodes\n",
      "Creating progression video: progression_dqn_seed42.mp4\n",
      "  Processing episode_00200.mp4...\n",
      "  Processing episode_00400.mp4...\n",
      "  Processing episode_00500.mp4...\n",
      "  Processing episode_00600.mp4...\n",
      "  Processing episode_00800.mp4...\n",
      "  Processing episode_01000.mp4...\n",
      "  Processing episode_01500.mp4...\n",
      "  Processing episode_02000.mp4...\n",
      "âœ… Compilation video saved: progression_dqn_seed42.mp4\n",
      "Found 8 recorded episodes\n",
      "Creating progression video: progression_dqn_seed123.mp4\n",
      "  Processing episode_00200.mp4...\n",
      "  Processing episode_00400.mp4...\n",
      "  Processing episode_00500.mp4...\n",
      "  Processing episode_00600.mp4...\n",
      "  Processing episode_00800.mp4...\n",
      "  Processing episode_01000.mp4...\n",
      "  Processing episode_01500.mp4...\n",
      "  Processing episode_02000.mp4...\n",
      "âœ… Compilation video saved: progression_dqn_seed123.mp4\n",
      "Found 8 recorded episodes\n",
      "Creating progression video: progression_dqn_seed456.mp4\n",
      "  Processing episode_00200.mp4...\n",
      "  Processing episode_00400.mp4...\n",
      "  Processing episode_00500.mp4...\n",
      "  Processing episode_00600.mp4...\n",
      "  Processing episode_00800.mp4...\n",
      "  Processing episode_01000.mp4...\n",
      "  Processing episode_01500.mp4...\n",
      "  Processing episode_02000.mp4...\n",
      "âœ… Compilation video saved: progression_dqn_seed456.mp4\n",
      "Creating best episodes compilation for DQN...\n",
      "  Adding BEST_episode_00612_score_19.mp4...\n",
      "  Adding BEST_episode_01116_score_17.mp4...\n",
      "  Adding BEST_episode_00692_score_19.mp4...\n",
      "  Adding BEST_episode_00926_score_18.mp4...\n",
      "  Adding BEST_episode_00954_score_18.mp4...\n",
      "  Adding BEST_episode_01624_score_19.mp4...\n",
      "âœ… Best episodes compilation saved: best_episodes_dqn.mp4\n",
      "\n",
      "ðŸŽ‰ All DQN compilation videos created!\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# =========================\n",
    "# Safe text overlay helper\n",
    "# =========================\n",
    "def safe_put_text(img, text, pos, font_scale=0.4, color=(255,255,255), thickness=1):\n",
    "    \"\"\"\n",
    "    Safely put text on a frame (ensures contiguous array and correct type)\n",
    "    \"\"\"\n",
    "    img = np.ascontiguousarray(img, dtype=np.uint8)\n",
    "    cv2.putText(img, text, pos, cv2.FONT_HERSHEY_SIMPLEX, font_scale, color, thickness, cv2.LINE_AA)\n",
    "    return img\n",
    "\n",
    "# =========================\n",
    "# Training progression video\n",
    "# =========================\n",
    "def create_training_progression_video(recordings_dir, output_path='training_progression.mp4'):\n",
    "    \"\"\"\n",
    "    Create a compilation showing DQN training progression\n",
    "    \"\"\"\n",
    "    video_files = sorted(Path(recordings_dir).glob(\"episode_*.mp4\"))\n",
    "    \n",
    "    if not video_files:\n",
    "        print(f\"âŒ No videos found in {recordings_dir}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(video_files)} recorded episodes\")\n",
    "    print(f\"Creating progression video: {output_path}\")\n",
    "    \n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    fps = 15\n",
    "    \n",
    "    first_video = cv2.VideoCapture(str(video_files[0]))\n",
    "    width = int(first_video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(first_video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    first_video.release()\n",
    "    \n",
    "    output_video = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "    \n",
    "    for video_file in video_files:\n",
    "        print(f\"  Processing {video_file.name}...\")\n",
    "        cap = cv2.VideoCapture(str(video_file))\n",
    "        \n",
    "        # Title frame\n",
    "        title_frame = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "        title_frame = np.ascontiguousarray(title_frame, dtype=np.uint8)\n",
    "        text = f\"Episode {video_file.stem.split('_')[1]}\"\n",
    "        text_size = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.8, 2)[0]\n",
    "        text_x = (width - text_size[0]) // 2\n",
    "        text_y = (height + text_size[1]) // 2\n",
    "        title_frame = safe_put_text(title_frame, text, (text_x, text_y), font_scale=0.8, thickness=2)\n",
    "        \n",
    "        for _ in range(fps):\n",
    "            output_video.write(title_frame)\n",
    "        \n",
    "        # Write video frames\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame = np.ascontiguousarray(frame, dtype=np.uint8)\n",
    "            output_video.write(frame)\n",
    "        cap.release()\n",
    "        \n",
    "        # Black separator\n",
    "        black_frame = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "        black_frame = np.ascontiguousarray(black_frame, dtype=np.uint8)\n",
    "        for _ in range(fps // 2):\n",
    "            output_video.write(black_frame)\n",
    "    \n",
    "    output_video.release()\n",
    "    print(f\"âœ… Compilation video saved: {output_path}\")\n",
    "\n",
    "# =========================\n",
    "# Best episodes compilation\n",
    "# =========================\n",
    "def create_best_episodes_compilation(seed_list=[42, 123, 456], agent_name=\"DQN\"):\n",
    "    \"\"\"\n",
    "    Create compilation of best episodes from all DQN seeds\n",
    "    \"\"\"\n",
    "    output_path = f'best_episodes_{agent_name.lower()}.mp4'\n",
    "    print(f\"Creating best episodes compilation for {agent_name}...\")\n",
    "    \n",
    "    best_videos = []\n",
    "    for seed in seed_list:\n",
    "        recordings_dir = f\"recordings_{agent_name.lower()}_seed{seed}\"\n",
    "        best_video = list(Path(recordings_dir).glob(\"BEST_*.mp4\"))\n",
    "        if best_video:\n",
    "            best_videos.extend(best_video)\n",
    "    \n",
    "    if not best_videos:\n",
    "        print(\"âŒ No best episode videos found\")\n",
    "        return\n",
    "    \n",
    "    cap = cv2.VideoCapture(str(best_videos[0]))\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    cap.release()\n",
    "    \n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    output_video = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "    \n",
    "    for video_file in best_videos:\n",
    "        print(f\"  Adding {video_file.name}...\")\n",
    "        \n",
    "        # Title frame\n",
    "        title_frame = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "        title_frame = np.ascontiguousarray(title_frame, dtype=np.uint8)\n",
    "        title_text = video_file.stem.replace('_', ' ').title()\n",
    "        title_frame = safe_put_text(title_frame, title_text, (10, height//2), font_scale=0.4, thickness=1)\n",
    "        \n",
    "        for _ in range(fps):\n",
    "            output_video.write(title_frame)\n",
    "        \n",
    "        # Video frames\n",
    "        cap = cv2.VideoCapture(str(video_file))\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame = np.ascontiguousarray(frame, dtype=np.uint8)\n",
    "            output_video.write(frame)\n",
    "        cap.release()\n",
    "        \n",
    "        # Black separator\n",
    "        black_frame = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "        black_frame = np.ascontiguousarray(black_frame, dtype=np.uint8)\n",
    "        for _ in range(fps):\n",
    "            output_video.write(black_frame)\n",
    "    \n",
    "    output_video.release()\n",
    "    print(f\"âœ… Best episodes compilation saved: {output_path}\")\n",
    "\n",
    "# =========================\n",
    "# MAIN EXECUTION\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"DQN VIDEO COMPILATION CREATOR\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    seeds = [42, 123, 456]\n",
    "    \n",
    "    # Progression videos\n",
    "    for seed in seeds:\n",
    "        recordings_dir = f\"recordings_dqn_seed{seed}\"\n",
    "        if Path(recordings_dir).exists():\n",
    "            create_training_progression_video(\n",
    "                recordings_dir,\n",
    "                f'progression_dqn_seed{seed}.mp4'\n",
    "            )\n",
    "    \n",
    "    # Best episodes compilation\n",
    "    create_best_episodes_compilation(seeds, \"DQN\")\n",
    "    \n",
    "    print(\"\\nðŸŽ‰ All DQN compilation videos created!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a977696",
   "metadata": {},
   "source": [
    "METRICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6fe34037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "COMPREHENSIVE METRICS VISUALIZATION (DQN)\n",
      "================================================================================\n",
      "\n",
      "Loading DQN metrics files...\n",
      "âœ… Loaded 3 DQN metric files\n",
      "\n",
      "Generating visualizations...\n",
      "\n",
      "âœ… Saved: 1_sample_efficiency_dqn.png\n",
      "âœ… Saved: 2_exploration_stability_dqn.png\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MetricsLogger' object has no attribute 'get_training_duration'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 522\u001b[0m\n\u001b[0;32m    520\u001b[0m plot_sample_efficiency(metrics_list, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1_sample_efficiency_dqn.png\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    521\u001b[0m plot_exploration_stability(metrics_list, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2_exploration_stability_dqn.png\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 522\u001b[0m \u001b[43mplot_computational_efficiency\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetrics_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m3_computational_efficiency_dqn.png\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    523\u001b[0m plot_convergence_stability(metrics_list, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m4_convergence_stability_dqn.png\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    524\u001b[0m plot_policy_behavior(metrics_list, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m5_policy_behavior_dqn.png\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[12], line 172\u001b[0m, in \u001b[0;36mplot_computational_efficiency\u001b[1;34m(metrics_list, save_path)\u001b[0m\n\u001b[0;32m    170\u001b[0m ax \u001b[38;5;241m=\u001b[39m axes[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    171\u001b[0m seeds \u001b[38;5;241m=\u001b[39m [m\u001b[38;5;241m.\u001b[39mseed \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m metrics_list]\n\u001b[1;32m--> 172\u001b[0m training_times \u001b[38;5;241m=\u001b[39m [m\u001b[38;5;241m.\u001b[39mget_training_duration() \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m60\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m metrics_list]\n\u001b[0;32m    173\u001b[0m ax\u001b[38;5;241m.\u001b[39mbar(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(seeds)), training_times, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m#2E86AB\u001b[39m\u001b[38;5;124m'\u001b[39m, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m)\n\u001b[0;32m    174\u001b[0m ax\u001b[38;5;241m.\u001b[39mset_xticks(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(seeds)))\n",
      "Cell \u001b[1;32mIn[12], line 172\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    170\u001b[0m ax \u001b[38;5;241m=\u001b[39m axes[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    171\u001b[0m seeds \u001b[38;5;241m=\u001b[39m [m\u001b[38;5;241m.\u001b[39mseed \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m metrics_list]\n\u001b[1;32m--> 172\u001b[0m training_times \u001b[38;5;241m=\u001b[39m [\u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_training_duration\u001b[49m() \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m60\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m metrics_list]\n\u001b[0;32m    173\u001b[0m ax\u001b[38;5;241m.\u001b[39mbar(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(seeds)), training_times, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m#2E86AB\u001b[39m\u001b[38;5;124m'\u001b[39m, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m)\n\u001b[0;32m    174\u001b[0m ax\u001b[38;5;241m.\u001b[39mset_xticks(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(seeds)))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'MetricsLogger' object has no attribute 'get_training_duration'"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x1000 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "def load_metrics(pattern=\"metrics_dqn_seed*.pkl\"):\n",
    "    \"\"\"Load all metrics files matching pattern\"\"\"\n",
    "    from pathlib import Path\n",
    "\n",
    "    class MetricsLogger:\n",
    "        \"\"\"Dummy reconstruction class for DQN metrics\"\"\"\n",
    "        def __init__(self, agent_name, seed):\n",
    "            self.agent_name = agent_name\n",
    "            self.seed = seed\n",
    "\n",
    "    metrics_list = []\n",
    "    for file in Path('.').glob(pattern):\n",
    "        with open(file, 'rb') as f:\n",
    "            metrics = pickle.load(f)\n",
    "            # Reconstruct MetricsLogger object\n",
    "            m = MetricsLogger(metrics.get('agent_name', 'DQN'), metrics.get('seed', 0))\n",
    "            m.__dict__.update(metrics)\n",
    "            metrics_list.append(m)\n",
    "    return metrics_list\n",
    "\n",
    "\n",
    "def plot_sample_efficiency(metrics_list, save_path='1_sample_efficiency_dqn.png'):\n",
    "    \"\"\"Sample Efficiency Analysis\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('1. Sample Efficiency Analysis', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    colors = ['#2E86AB', '#A23B72', '#F18F01']\n",
    "    \n",
    "    # Plot 1: Reward vs Episodes\n",
    "    ax = axes[0, 0]\n",
    "    for i, m in enumerate(metrics_list):\n",
    "        episodes = range(1, len(m.moving_avg_reward) + 1)\n",
    "        ax.plot(episodes, m.moving_avg_reward, label=f'Seed {m.seed}', alpha=0.7, color=colors[i % len(colors)])\n",
    "    ax.set_xlabel('Episode')\n",
    "    ax.set_ylabel('Moving Avg Reward (window=100)')\n",
    "    ax.set_title('Learning Curves Across Seeds')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Score vs Episodes\n",
    "    ax = axes[0, 1]\n",
    "    for i, m in enumerate(metrics_list):\n",
    "        episodes = range(1, len(m.moving_avg_score) + 1)\n",
    "        ax.plot(episodes, m.moving_avg_score, label=f'Seed {m.seed}', alpha=0.7, color=colors[i % len(colors)])\n",
    "    ax.set_xlabel('Episode')\n",
    "    ax.set_ylabel('Moving Avg Score (window=100)')\n",
    "    ax.set_title('Score Progression Across Seeds')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Episodes to Convergence\n",
    "    ax = axes[1, 0]\n",
    "    thresholds = ['50%_max', '80%_max', '90%_max']\n",
    "    threshold_data = {t: [] for t in thresholds}\n",
    "    for m in metrics_list:\n",
    "        for t in thresholds:\n",
    "            value = m.episodes_to_threshold.get(t, np.nan)\n",
    "            threshold_data[t].append(value)\n",
    "    x_pos = np.arange(len(thresholds))\n",
    "    means = [np.nanmean(threshold_data[t]) for t in thresholds]\n",
    "    stds = [np.nanstd(threshold_data[t]) for t in thresholds]\n",
    "    ax.bar(x_pos, means, yerr=stds, capsize=5, color='#2E86AB', alpha=0.7)\n",
    "    ax.set_xticks(x_pos)\n",
    "    ax.set_xticklabels(thresholds)\n",
    "    ax.set_ylabel('Episodes Required')\n",
    "    ax.set_title('Sample Efficiency: Episodes to Reach Performance Thresholds')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Plot 4: Cumulative max score\n",
    "    ax = axes[1, 1]\n",
    "    for i, m in enumerate(metrics_list):\n",
    "        cumulative_max = np.maximum.accumulate(m.episode_scores)\n",
    "        episodes = range(1, len(cumulative_max) + 1)\n",
    "        ax.plot(episodes, cumulative_max, label=f'Seed {m.seed}', alpha=0.7, color=colors[i % len(colors)])\n",
    "    ax.set_xlabel('Episode')\n",
    "    ax.set_ylabel('Cumulative Max Score')\n",
    "    ax.set_title('Best Performance Over Time')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"âœ… Saved: {save_path}\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_exploration_stability(metrics_list, save_path='2_exploration_stability_dqn.png'):\n",
    "    \"\"\"Exploration Stability\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('2. Exploration & Behavioral Stability', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    colors = ['#2E86AB', '#A23B72', '#F18F01']\n",
    "    \n",
    "    # Plot 1: Epsilon decay with reward overlay\n",
    "    ax = axes[0, 0]\n",
    "    ax2 = ax.twinx()\n",
    "    for i, m in enumerate(metrics_list):\n",
    "        episodes = range(1, len(m.epsilon_history) + 1)\n",
    "        ax.plot(episodes, m.epsilon_history, label=f'Îµ (Seed {m.seed})', alpha=0.5, linestyle='--', color=colors[i % len(colors)])\n",
    "        ax2.plot(episodes, m.moving_avg_reward, label=f'Reward (Seed {m.seed})', alpha=0.7, color=colors[i % len(colors)])\n",
    "    ax.set_xlabel('Episode')\n",
    "    ax.set_ylabel('Epsilon (Exploration Rate)')\n",
    "    ax2.set_ylabel('Moving Avg Reward')\n",
    "    ax.set_title('Exploration Decay vs Learning Performance')\n",
    "    ax.legend(loc='upper left')\n",
    "    ax2.legend(loc='upper right')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Reward variance per block\n",
    "    ax = axes[0, 1]\n",
    "    for i, m in enumerate(metrics_list):\n",
    "        blocks = range(1, len(m.reward_variance_per_block) + 1)\n",
    "        block_episodes = [b * 1000 for b in blocks]\n",
    "        ax.plot(block_episodes, m.reward_variance_per_block, label=f'Seed {m.seed}', marker='o', alpha=0.7, color=colors[i % len(colors)])\n",
    "    ax.set_xlabel('Episode (Ã—1000)')\n",
    "    ax.set_ylabel('Reward Variance')\n",
    "    ax.set_title('Learning Stability: Reward Variance per 1000 Episodes')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Score variance per block\n",
    "    ax = axes[1, 0]\n",
    "    for i, m in enumerate(metrics_list):\n",
    "        blocks = range(1, len(m.score_variance_per_block) + 1)\n",
    "        block_episodes = [b * 1000 for b in blocks]\n",
    "        ax.plot(block_episodes, m.score_variance_per_block, label=f'Seed {m.seed}', marker='o', alpha=0.7, color=colors[i % len(colors)])\n",
    "    ax.set_xlabel('Episode (Ã—1000)')\n",
    "    ax.set_ylabel('Score Variance')\n",
    "    ax.set_title('Performance Stability: Score Variance per 1000 Episodes')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 4: Action distribution (last seed only)\n",
    "    ax = axes[1, 1]\n",
    "    m = metrics_list[-1]\n",
    "    action_names = ['UP (0)', 'DOWN (1)', 'LEFT (2)', 'RIGHT (3)']\n",
    "    total_actions = sum(m.action_counts.values())\n",
    "    action_percentages = [m.action_counts[i] / total_actions * 100 for i in range(4)]\n",
    "    ax.bar(action_names, action_percentages, color='#2E86AB', alpha=0.7)\n",
    "    ax.set_ylabel('Percentage of Total Actions (%)')\n",
    "    ax.set_title(f'Action Distribution (Seed {m.seed})')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"âœ… Saved: {save_path}\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_computational_efficiency(metrics_list, save_path='3_computational_efficiency_dqn.png'):\n",
    "    \"\"\"Runtime / Computational Efficiency\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('3. Computational Efficiency', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    colors = ['#2E86AB', '#A23B72', '#F18F01']\n",
    "    \n",
    "    # Plot 1: Training time\n",
    "    ax = axes[0, 0]\n",
    "    seeds = [m.seed for m in metrics_list]\n",
    "    training_times = [m.get_training_duration() / 60 for m in metrics_list]\n",
    "    ax.bar(range(len(seeds)), training_times, color='#2E86AB', alpha=0.7)\n",
    "    ax.set_xticks(range(len(seeds)))\n",
    "    ax.set_xticklabels([f'Seed {s}' for s in seeds])\n",
    "    ax.set_ylabel('Training Time (minutes)')\n",
    "    ax.set_title('Total Training Duration')\n",
    "    ax.axhline(np.mean(training_times), color='red', linestyle='--', label=f'Mean: {np.mean(training_times):.2f} min')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Plot 2: Average time per episode\n",
    "    ax = axes[0, 1]\n",
    "    avg_times = [m.get_avg_episode_time() * 1000 for m in metrics_list]\n",
    "    ax.bar(range(len(seeds)), avg_times, color='#A23B72', alpha=0.7)\n",
    "    ax.set_xticks(range(len(seeds)))\n",
    "    ax.set_xticklabels([f'Seed {s}' for s in seeds])\n",
    "    ax.set_ylabel('Time per Episode (ms)')\n",
    "    ax.set_title('Average Episode Duration')\n",
    "    ax.axhline(np.mean(avg_times), color='red', linestyle='--', label=f'Mean: {np.mean(avg_times):.2f} ms')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Plot 3: CPU usage\n",
    "    ax = axes[1, 0]\n",
    "    for i, m in enumerate(metrics_list):\n",
    "        sampled_episodes = range(0, len(m.cpu_usage), 100)\n",
    "        sampled_cpu = [m.cpu_usage[i] for i in sampled_episodes]\n",
    "        ax.plot(sampled_episodes, sampled_cpu, label=f'Seed {m.seed}', alpha=0.7, color=colors[i % len(colors)])\n",
    "    ax.set_xlabel('Episode')\n",
    "    ax.set_ylabel('CPU Usage (%)')\n",
    "    ax.set_title('CPU Utilization During Training')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 4: Memory usage\n",
    "    ax = axes[1, 1]\n",
    "    for i, m in enumerate(metrics_list):\n",
    "        sampled_episodes = range(0, len(m.memory_usage), 100)\n",
    "        sampled_mem = [m.memory_usage[i] for i in sampled_episodes]\n",
    "        ax.plot(sampled_episodes, sampled_mem, label=f'Seed {m.seed}', alpha=0.7, color=colors[i % len(colors)])\n",
    "    ax.set_xlabel('Episode')\n",
    "    ax.set_ylabel('Memory Usage (%)')\n",
    "    ax.set_title('Memory Utilization During Training')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"âœ… Saved: {save_path}\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_convergence_stability(metrics_list, save_path='4_convergence_stability_dqn.png'):\n",
    "    \"\"\"Convergence and Stability Visualization\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('4. Convergence & Stability Analysis', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    colors = ['#2E86AB', '#A23B72', '#F18F01']\n",
    "    \n",
    "    # Plot 1: All reward curves together\n",
    "    ax = axes[0, 0]\n",
    "    for i, m in enumerate(metrics_list):\n",
    "        episodes = range(1, len(m.moving_avg_reward) + 1)\n",
    "        ax.plot(episodes, m.moving_avg_reward, label=f'Seed {m.seed}', alpha=0.7, linewidth=2, color=colors[i % len(colors)])\n",
    "    \n",
    "    max_len = max(len(m.moving_avg_reward) for m in metrics_list)\n",
    "    all_rewards = np.zeros((len(metrics_list), max_len))\n",
    "    for i, m in enumerate(metrics_list):\n",
    "        all_rewards[i, :len(m.moving_avg_reward)] = m.moving_avg_reward\n",
    "        if len(m.moving_avg_reward) < max_len:\n",
    "            all_rewards[i, len(m.moving_avg_reward):] = m.moving_avg_reward[-1]\n",
    "    \n",
    "    mean_reward = np.mean(all_rewards, axis=0)\n",
    "    std_reward = np.std(all_rewards, axis=0)\n",
    "    episodes = range(1, max_len + 1)\n",
    "    ax.fill_between(episodes, mean_reward - std_reward, mean_reward + std_reward, alpha=0.2, color='gray', label='Â±1 Std Dev')\n",
    "    ax.plot(episodes, mean_reward, 'k--', linewidth=2, label='Mean')\n",
    "    ax.set_xlabel('Episode')\n",
    "    ax.set_ylabel('Moving Avg Reward')\n",
    "    ax.set_title('Reward Convergence Across Multiple Seeds')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: All score curves together\n",
    "    ax = axes[0, 1]\n",
    "    for i, m in enumerate(metrics_list):\n",
    "        episodes = range(1, len(m.moving_avg_score) + 1)\n",
    "        ax.plot(episodes, m.moving_avg_score, label=f'Seed {m.seed}', alpha=0.7, linewidth=2, color=colors[i % len(colors)])\n",
    "    \n",
    "    all_scores = np.zeros((len(metrics_list), max_len))\n",
    "    for i, m in enumerate(metrics_list):\n",
    "        all_scores[i, :len(m.moving_avg_score)] = m.moving_avg_score\n",
    "        if len(m.moving_avg_score) < max_len:\n",
    "            all_scores[i, len(m.moving_avg_score):] = m.moving_avg_score[-1]\n",
    "    \n",
    "    mean_score = np.mean(all_scores, axis=0)\n",
    "    std_score = np.std(all_scores, axis=0)\n",
    "    ax.fill_between(episodes, mean_score - std_score, mean_score + std_score, alpha=0.2, color='gray', label='Â±1 Std Dev')\n",
    "    ax.plot(episodes, mean_score, 'k--', linewidth=2, label='Mean')\n",
    "    ax.set_xlabel('Episode')\n",
    "    ax.set_ylabel('Moving Avg Score')\n",
    "    ax.set_title('Score Convergence Across Multiple Seeds')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Variance over time\n",
    "    ax = axes[1, 0]\n",
    "    window = 100\n",
    "    variance_over_time = []\n",
    "    for i in range(0, max_len, window):\n",
    "        window_scores = all_scores[:, i:min(i+window, max_len)]\n",
    "        variance_over_time.append(np.mean(np.var(window_scores, axis=0)))\n",
    "    episodes_binned = range(window, max_len + 1, window)\n",
    "    ax.plot(episodes_binned, variance_over_time, marker='o', color='#2E86AB', linewidth=2)\n",
    "    ax.set_xlabel('Episode')\n",
    "    ax.set_ylabel(f'Mean Variance (per {window} episodes)')\n",
    "    ax.set_title('Stability: Inter-Seed Variance Over Training')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 4: Convergence speed histogram\n",
    "    # ax = axes[1, 1]\n",
    "    # conv_episodes = [m.episodes_to_threshold.get('80%_max', np.nan) for m in metrics_list]\n",
    "    # ax.hist(conv_episodes, bins=len(metrics_list), color='#F18F01', alpha=0.7)\n",
    "    # ax.set_xlabel('Episodes to Reach 80% of Max Performance')\n",
    "    # ax.set_ylabel('Count')\n",
    "    # ax.set_title('Convergence Speed Across Seeds')\n",
    "    # ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # plt.tight_layout()\n",
    "    # plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    # print(f\"âœ… Saved: {save_path}\")\n",
    "    # plt.close()\n",
    "    \n",
    "    # Plot 4: Final performance comparison\n",
    "    ax = axes[1, 1]\n",
    "\n",
    "    final_scores = [np.mean(m.episode_scores[-100:]) for m in metrics_list]\n",
    "    final_rewards = [np.mean(m.episode_rewards[-100:]) for m in metrics_list]\n",
    "    seeds = [m.seed for m in metrics_list]\n",
    "\n",
    "    x_pos = np.arange(len(seeds))\n",
    "    width = 0.35\n",
    "\n",
    "    ax.bar(x_pos - width/2, final_scores, width, label='Avg Score (last 100)', color='#2E86AB', alpha=0.7)\n",
    "    ax.bar(x_pos + width/2, final_rewards, width, label='Avg Reward (last 100)', color='#F18F01', alpha=0.7)\n",
    "\n",
    "    ax.set_xticks(x_pos)\n",
    "    ax.set_xticklabels([f'Seed {s}' for s in seeds])\n",
    "    ax.set_ylabel('Performance')\n",
    "    ax.set_title('Final Performance Comparison')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"âœ… Saved: {save_path}\")\n",
    "    plt.close()\n",
    "    \n",
    "\n",
    "\n",
    "# def plot_policy_behavior(metrics_list, save_path='5_policy_behavior_dqn.png'):\n",
    "#     \"\"\"Visualizing agent's final policy behavior\"\"\"\n",
    "#     m = metrics_list[-1]  # Use last seed\n",
    "#     fig, ax = plt.subplots(figsize=(10, 8))\n",
    "#     action_names = ['UP', 'DOWN', 'LEFT', 'RIGHT']\n",
    "#     total_actions = sum(m.action_counts.values())\n",
    "#     percentages = [m.action_counts[i] / total_actions * 100 for i in range(4)]\n",
    "#     ax.bar(action_names, percentages, color='#2E86AB', alpha=0.7)\n",
    "#     ax.set_ylabel('Percentage of Total Actions (%)')\n",
    "#     ax.set_title(f'DQN Final Policy Action Distribution (Seed {m.seed})')\n",
    "#     ax.grid(True, alpha=0.3, axis='y')\n",
    "#     plt.tight_layout()\n",
    "#     plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "#     print(f\"âœ… Saved: {save_path}\")\n",
    "#     plt.close()\n",
    "\n",
    "def plot_policy_behavior(metrics_list, save_path='5_policy_behavior.png'):\n",
    "    \"\"\"\n",
    "    METRIC 5: Policy Behavior Check\n",
    "    Shows how the policy evolves over time\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle\n",
    "    fig.suptitle('5. Policy Behavior & Evolution', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    colors = ['#2E86AB', '#A23B72', '#F18F01']\n",
    "    \n",
    "    # Plot 1: Average survival time over training\n",
    "    ax = axes[0, 0]\n",
    "    for i, m in enumerate(metrics_list):\n",
    "        # Calculate moving average of episode lengths (survival time)\n",
    "        window = 100\n",
    "        if len(m.episode_lengths) >= window:\n",
    "            moving_avg_length = [np.mean(m.episode_lengths[max(0, j-window):j+1]) \n",
    "                                for j in range(len(m.episode_lengths))]\n",
    "        else:\n",
    "            moving_avg_length = m.episode_lengths\n",
    "        \n",
    "        episodes = range(1, len(moving_avg_length) + 1)\n",
    "        ax.plot(episodes, moving_avg_length,\n",
    "               label=f'Seed {m.seed}', alpha=0.7, color=colors[i % len(colors)])\n",
    "    \n",
    "    ax.set_xlabel('Episode')\n",
    "    ax.set_ylabel('Average Steps Survived (window=100)')\n",
    "    ax.set_title('Policy Evolution: Survival Time Over Training')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Early vs Late behavior comparison\n",
    "    ax = axes[0, 1]\n",
    "    \n",
    "    # Compare first 500 episodes vs last 500 episodes\n",
    "    early_window = slice(0, 500)\n",
    "    late_window = slice(-500, None)\n",
    "    \n",
    "    comparison_data = {\n",
    "        'Early Training\\n(Episodes 1-500)': [],\n",
    "        'Late Training\\n(Last 500 episodes)': []\n",
    "    }\n",
    "    \n",
    "    for m in metrics_list:\n",
    "        comparison_data['Early Training\\n(Episodes 1-500)'].append(\n",
    "            np.mean(m.episode_scores[early_window])\n",
    "        )\n",
    "        comparison_data['Late Training\\n(Last 500 episodes)'].append(\n",
    "            np.mean(m.episode_scores[late_window])\n",
    "        )\n",
    "    \n",
    "    x_pos = np.arange(len(comparison_data))\n",
    "    means = [np.mean(comparison_data[k]) for k in comparison_data.keys()]\n",
    "    stds = [np.std(comparison_data[k]) for k in comparison_data.keys()]\n",
    "    \n",
    "    ax.bar(x_pos, means, yerr=stds, capsize=5, color=['#E63946', '#06FFA5'], alpha=0.7)\n",
    "    ax.set_xticks(x_pos)\n",
    "    ax.set_xticklabels(comparison_data.keys())\n",
    "    ax.set_ylabel('Average Score')\n",
    "    ax.set_title('Behavioral Shift: Early vs Late Training')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add significance annotation\n",
    "    improvement = ((means[1] - means[0]) / means[0] * 100) if means[0] > 0 else 0\n",
    "    ax.text(0.5, max(means) * 0.9, f'+{improvement:.1f}% improvement', \n",
    "           ha='center', fontsize=12, fontweight='bold',\n",
    "           bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "    \n",
    "    # Plot 3: Action distribution evolution\n",
    "    ax = axes[1, 0]\n",
    "    \n",
    "    # Use first seed for detailed analysis\n",
    "    m = metrics_list[0]\n",
    "    \n",
    "    # Divide training into 4 phases\n",
    "    num_episodes = len(m.episode_scores)\n",
    "    phase_size = num_episodes // 4\n",
    "    phases = ['Early\\n(0-25%)', 'Mid-Early\\n(25-50%)', 'Mid-Late\\n(50-75%)', 'Late\\n(75-100%)']\n",
    "    \n",
    "    # We need to reconstruct action distribution per phase\n",
    "    # Since we only have total counts, we'll show the final distribution\n",
    "    # and note that this is a limitation\n",
    "    \n",
    "    action_names = ['UP', 'DOWN', 'LEFT', 'RIGHT']\n",
    "    total_actions = sum(m.action_counts.values())\n",
    "    action_percentages = [m.action_counts[i] / total_actions * 100 for i in range(4)]\n",
    "    \n",
    "    colors_actions = ['#E63946', '#F1FAEE', '#A8DADC', '#457B9D']\n",
    "    ax.bar(action_names, action_percentages, color=colors_actions, alpha=0.7)\n",
    "    ax.set_ylabel('Percentage of Total Actions (%)')\n",
    "    ax.set_title(f'Action Distribution (Seed {m.seed})')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add balanced line\n",
    "    ax.axhline(25, color='red', linestyle='--', linewidth=1, label='Balanced (25%)')\n",
    "    ax.legend()\n",
    "    \n",
    "    # Plot 4: Performance consistency (success rate per block)\n",
    "    ax = axes[1, 1]\n",
    "    \n",
    "    # Calculate \"success rate\" as percentage of episodes with score > 0\n",
    "    block_size = 500\n",
    "    \n",
    "    for i, m in enumerate(metrics_list):\n",
    "        success_rates = []\n",
    "        block_episodes = []\n",
    "        \n",
    "        for start in range(0, len(m.episode_scores), block_size):\n",
    "            end = min(start + block_size, len(m.episode_scores))\n",
    "            block_scores = m.episode_scores[start:end]\n",
    "            success_rate = sum(1 for s in block_scores if s > 0) / len(block_scores) * 100\n",
    "            success_rates.append(success_rate)\n",
    "            block_episodes.append(start + block_size // 2)\n",
    "        \n",
    "        ax.plot(block_episodes, success_rates,\n",
    "               label=f'Seed {m.seed}', marker='o', alpha=0.7, color=colors[i % len(colors)])\n",
    "    \n",
    "    ax.set_xlabel('Episode')\n",
    "    ax.set_ylabel('Success Rate (% with score > 0)')\n",
    "    ax.set_title(f'Learning Progress: Success Rate per {block_size} Episodes')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_ylim([0, 105])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"âœ… Saved: {save_path}\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def generate_summary_report(metrics_list, agent_name=\"DQN\", save_path='summary_report_dqn.txt'):\n",
    "    \"\"\"Generate textual summary report\"\"\"\n",
    "    with open(save_path, 'w') as f:\n",
    "        f.write(\"=\"*80 + \"\\n\")\n",
    "        f.write(f\"{agent_name} METRICS SUMMARY REPORT\\n\")\n",
    "        f.write(\"=\"*80 + \"\\n\\n\")\n",
    "        for m in metrics_list:\n",
    "            f.write(f\"Seed: {m.seed}\\n\")\n",
    "            f.write(f\"Final Moving Avg Reward: {m.moving_avg_reward[-1]:.2f}\\n\")\n",
    "            f.write(f\"Final Moving Avg Score: {m.moving_avg_score[-1]:.2f}\\n\")\n",
    "            f.write(f\"Episodes to 50% Max: {m.episodes_to_threshold.get('50%_max', 'N/A')}\\n\")\n",
    "            f.write(f\"Episodes to 80% Max: {m.episodes_to_threshold.get('80%_max', 'N/A')}\\n\")\n",
    "            f.write(f\"Episodes to 90% Max: {m.episodes_to_threshold.get('90%_max', 'N/A')}\\n\")\n",
    "            f.write(f\"Training Duration (min): {m.get_training_duration() / 60:.2f}\\n\")\n",
    "            f.write(\"-\"*40 + \"\\n\")\n",
    "        f.write(\"\\nOverall Mean & Std Dev Across Seeds:\\n\")\n",
    "        final_rewards = [m.moving_avg_reward[-1] for m in metrics_list]\n",
    "        final_scores = [m.moving_avg_score[-1] for m in metrics_list]\n",
    "        f.write(f\"Mean Final Reward: {np.mean(final_rewards):.2f} Â± {np.std(final_rewards):.2f}\\n\")\n",
    "        f.write(f\"Mean Final Score: {np.mean(final_scores):.2f} Â± {np.std(final_scores):.2f}\\n\")\n",
    "    print(f\"âœ… Saved summary report: {save_path}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"COMPREHENSIVE METRICS VISUALIZATION (DQN)\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    # Load metrics\n",
    "    print(\"Loading DQN metrics files...\")\n",
    "    metrics_list = load_metrics(\"metrics_dqn_seed*.pkl\")\n",
    "    \n",
    "    if not metrics_list:\n",
    "        print(\"âŒ No DQN metrics files found! Run your DQN training first.\")\n",
    "        exit(1)\n",
    "    \n",
    "    print(f\"âœ… Loaded {len(metrics_list)} DQN metric files\\n\")\n",
    "    \n",
    "    # Generate all visualizations\n",
    "    print(\"Generating visualizations...\\n\")\n",
    "    \n",
    "    plot_sample_efficiency(metrics_list, '1_sample_efficiency_dqn.png')\n",
    "    plot_exploration_stability(metrics_list, '2_exploration_stability_dqn.png')\n",
    "    plot_computational_efficiency(metrics_list, '3_computational_efficiency_dqn.png')\n",
    "    plot_convergence_stability(metrics_list, '4_convergence_stability_dqn.png')\n",
    "    plot_policy_behavior(metrics_list, '5_policy_behavior_dqn.png')\n",
    "    \n",
    "    # Generate summary report\n",
    "    print(\"\\nGenerating summary report...\\n\")\n",
    "    generate_summary_report(metrics_list, agent_name=\"DQN\", save_path='summary_report_dqn.txt')\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ðŸŽ‰ ALL DQN VISUALIZATIONS COMPLETE!\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\nGenerated files:\")\n",
    "    print(\"  1. 1_sample_efficiency_dqn.png\")\n",
    "    print(\"  2. 2_exploration_stability_dqn.png\")\n",
    "    print(\"  3. 3_computational_efficiency_dqn.png\")\n",
    "    print(\"  4. 4_convergence_stability_dqn.png\")\n",
    "    print(\"  5. 5_policy_behavior_dqn.png\")\n",
    "    print(\"  6. summary_report_dqn.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4717a979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ...existing code...\n",
    "# def record_episode(env, state_func, policy_net, epsilon, episode_num, agent_name, seed,\n",
    "#                    video_path, max_steps=1000):\n",
    "#     \"\"\"\n",
    "#     Evaluation-time recorder that tracks the last valid score even if the env resets on death.\n",
    "#     \"\"\"\n",
    "#     # Reset and size video from first frame\n",
    "#     env.reset()\n",
    "#     _ = state_func(env)\n",
    "\n",
    "#     surf = getattr(env, \"screen\", None) or getattr(env, \"surface\", None)\n",
    "#     assert surf is not None, \"SnakeGame has no screen/surface attribute\"\n",
    "\n",
    "#     first_frame = pygame.surfarray.array3d(surf)\n",
    "#     first_frame = np.transpose(first_frame, (1, 0, 2))\n",
    "#     first_frame = cv2.cvtColor(first_frame, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "#     overlay_h = 50\n",
    "#     h, w, _ = first_frame.shape\n",
    "#     frame_h = h + overlay_h\n",
    "\n",
    "#     fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "#     fps = 15\n",
    "#     vw = cv2.VideoWriter(video_path, fourcc, fps, (w, frame_h))\n",
    "\n",
    "#     state = state_func(env)\n",
    "#     done = False\n",
    "#     steps = 0\n",
    "#     total_reward = 0.0\n",
    "#     actions_taken = []\n",
    "#     last_score = max(0, getattr(env.snake, \"length\", 1) - 1)\n",
    "\n",
    "#     while not done and steps < max_steps:\n",
    "#         # Greedy (tiny exploration cap)\n",
    "#         if random.random() < min(epsilon, 0.1):\n",
    "#             action = random.randint(0, 3)\n",
    "#         else:\n",
    "#             with torch.no_grad():\n",
    "#                 state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "#                 action = int(policy_net(state_tensor).argmax(dim=1).item())\n",
    "#         actions_taken.append(action)\n",
    "\n",
    "#         # Step and robust score tracking\n",
    "#         prior_score = max(0, getattr(env.snake, \"length\", 1) - 1)\n",
    "#         _, reward, done, info = env.step(action)\n",
    "#         state = state_func(env)\n",
    "#         total_reward += reward\n",
    "\n",
    "#         info_score = info.get(\"score\") if isinstance(info, dict) else None\n",
    "#         cur_score = max(0, getattr(env.snake, \"length\", 1) - 1)\n",
    "#         last_score = max(last_score, prior_score, cur_score if info_score is None else int(info_score))\n",
    "\n",
    "#         # Frame capture\n",
    "#         surf = getattr(env, \"screen\", None) or getattr(env, \"surface\", None)\n",
    "#         frame = pygame.surfarray.array3d(surf)\n",
    "#         frame = np.transpose(frame, (1, 0, 2))\n",
    "#         frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "#         # Overlay\n",
    "#         overlay = np.zeros((frame_h, w, 3), dtype=np.uint8)\n",
    "#         overlay[0:h, :, :] = frame\n",
    "#         font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "#         fs, color, th = 0.35, (255, 255, 255), 1\n",
    "#         y0 = h + 15\n",
    "#         for i, text in enumerate([\n",
    "#             f\"Agent: {agent_name}\",\n",
    "#             f\"Seed: {seed}\",\n",
    "#             f\"Episode: {episode_num}\",\n",
    "#             f\"Step: {steps+1}\",\n",
    "#             f\"Score: {last_score}\",\n",
    "#         ]):\n",
    "#             cv2.putText(overlay, text, (5, y0 + i*12), font, fs, color, th, cv2.LINE_AA)\n",
    "\n",
    "#         vw.write(np.ascontiguousarray(overlay, dtype=np.uint8))\n",
    "#         steps += 1\n",
    "\n",
    "#     vw.release()\n",
    "#     return last_score, total_reward, steps, actions_taken\n",
    "# # ...existing code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a9be1ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replay done. Score: 0, Steps: 72\n"
     ]
    }
   ],
   "source": [
    "from snake_gym.envs.snake import SnakeGame\n",
    "import torch\n",
    "\n",
    "seed = 42\n",
    "agent_name = \"DQN\"\n",
    "policy_net = DQN()\n",
    "policy_net.load_state_dict(torch.load(f\"dqn_policy_net_seed{seed}.pth\", map_location=\"cpu\"))\n",
    "policy_net.eval()\n",
    "\n",
    "env = SnakeGame()\n",
    "video_path = f\"replay_dqn_seed{seed}.mp4\"\n",
    "score, total_reward, steps, actions = record_episode(\n",
    "    env, get_state, policy_net, epsilon=0.0,\n",
    "    episode_num=1, agent_name=agent_name, seed=seed, video_path=video_path\n",
    ")\n",
    "print(f\"Replay done. Score: {score}, Steps: {steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82a0bea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_policy_behavior(metrics_list, save_path='5_policy_behavior.png'):\n",
    "#     \"\"\"\n",
    "#     METRIC 5: Policy Behavior Check\n",
    "#     Shows how the policy evolves over time\n",
    "#     \"\"\"\n",
    "#     fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "#     fig.suptitle\n",
    "#     fig.suptitle('5. Policy Behavior & Evolution', fontsize=16, fontweight='bold')\n",
    "    \n",
    "#     colors = ['#2E86AB', '#A23B72', '#F18F01']\n",
    "    \n",
    "#     # Plot 1: Average survival time over training\n",
    "#     ax = axes[0, 0]\n",
    "#     for i, m in enumerate(metrics_list):\n",
    "#         # Calculate moving average of episode lengths (survival time)\n",
    "#         window = 100\n",
    "#         if len(m.episode_lengths) >= window:\n",
    "#             moving_avg_length = [np.mean(m.episode_lengths[max(0, j-window):j+1]) \n",
    "#                                 for j in range(len(m.episode_lengths))]\n",
    "#         else:\n",
    "#             moving_avg_length = m.episode_lengths\n",
    "        \n",
    "#         episodes = range(1, len(moving_avg_length) + 1)\n",
    "#         ax.plot(episodes, moving_avg_length,\n",
    "#                label=f'Seed {m.seed}', alpha=0.7, color=colors[i % len(colors)])\n",
    "    \n",
    "#     ax.set_xlabel('Episode')\n",
    "#     ax.set_ylabel('Average Steps Survived (window=100)')\n",
    "#     ax.set_title('Policy Evolution: Survival Time Over Training')\n",
    "#     ax.legend()\n",
    "#     ax.grid(True, alpha=0.3)\n",
    "    \n",
    "#     # Plot 2: Early vs Late behavior comparison\n",
    "#     ax = axes[0, 1]\n",
    "    \n",
    "#     # Compare first 500 episodes vs last 500 episodes\n",
    "#     early_window = slice(0, 500)\n",
    "#     late_window = slice(-500, None)\n",
    "    \n",
    "#     comparison_data = {\n",
    "#         'Early Training\\n(Episodes 1-500)': [],\n",
    "#         'Late Training\\n(Last 500 episodes)': []\n",
    "#     }\n",
    "    \n",
    "#     for m in metrics_list:\n",
    "#         comparison_data['Early Training\\n(Episodes 1-500)'].append(\n",
    "#             np.mean(m.episode_scores[early_window])\n",
    "#         )\n",
    "#         comparison_data['Late Training\\n(Last 500 episodes)'].append(\n",
    "#             np.mean(m.episode_scores[late_window])\n",
    "#         )\n",
    "    \n",
    "#     x_pos = np.arange(len(comparison_data))\n",
    "#     means = [np.mean(comparison_data[k]) for k in comparison_data.keys()]\n",
    "#     stds = [np.std(comparison_data[k]) for k in comparison_data.keys()]\n",
    "    \n",
    "#     ax.bar(x_pos, means, yerr=stds, capsize=5, color=['#E63946', '#06FFA5'], alpha=0.7)\n",
    "#     ax.set_xticks(x_pos)\n",
    "#     ax.set_xticklabels(comparison_data.keys())\n",
    "#     ax.set_ylabel('Average Score')\n",
    "#     ax.set_title('Behavioral Shift: Early vs Late Training')\n",
    "#     ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "#     # Add significance annotation\n",
    "#     improvement = ((means[1] - means[0]) / means[0] * 100) if means[0] > 0 else 0\n",
    "#     ax.text(0.5, max(means) * 0.9, f'+{improvement:.1f}% improvement', \n",
    "#            ha='center', fontsize=12, fontweight='bold',\n",
    "#            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "    \n",
    "#     # Plot 3: Action distribution evolution\n",
    "#     ax = axes[1, 0]\n",
    "    \n",
    "#     # Use first seed for detailed analysis\n",
    "#     m = metrics_list[0]\n",
    "    \n",
    "#     # Divide training into 4 phases\n",
    "#     num_episodes = len(m.episode_scores)\n",
    "#     phase_size = num_episodes // 4\n",
    "#     phases = ['Early\\n(0-25%)', 'Mid-Early\\n(25-50%)', 'Mid-Late\\n(50-75%)', 'Late\\n(75-100%)']\n",
    "    \n",
    "#     # We need to reconstruct action distribution per phase\n",
    "#     # Since we only have total counts, we'll show the final distribution\n",
    "#     # and note that this is a limitation\n",
    "    \n",
    "#     action_names = ['UP', 'DOWN', 'LEFT', 'RIGHT']\n",
    "#     total_actions = sum(m.action_counts.values())\n",
    "#     action_percentages = [m.action_counts[i] / total_actions * 100 for i in range(4)]\n",
    "    \n",
    "#     colors_actions = ['#E63946', '#F1FAEE', '#A8DADC', '#457B9D']\n",
    "#     ax.bar(action_names, action_percentages, color=colors_actions, alpha=0.7)\n",
    "#     ax.set_ylabel('Percentage of Total Actions (%)')\n",
    "#     ax.set_title(f'Action Distribution (Seed {m.seed})')\n",
    "#     ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "#     # Add balanced line\n",
    "#     ax.axhline(25, color='red', linestyle='--', linewidth=1, label='Balanced (25%)')\n",
    "#     ax.legend()\n",
    "    \n",
    "#     # Plot 4: Performance consistency (success rate per block)\n",
    "#     ax = axes[1, 1]\n",
    "    \n",
    "#     # Calculate \"success rate\" as percentage of episodes with score > 0\n",
    "#     block_size = 500\n",
    "    \n",
    "#     for i, m in enumerate(metrics_list):\n",
    "#         success_rates = []\n",
    "#         block_episodes = []\n",
    "        \n",
    "#         for start in range(0, len(m.episode_scores), block_size):\n",
    "#             end = min(start + block_size, len(m.episode_scores))\n",
    "#             block_scores = m.episode_scores[start:end]\n",
    "#             success_rate = sum(1 for s in block_scores if s > 0) / len(block_scores) * 100\n",
    "#             success_rates.append(success_rate)\n",
    "#             block_episodes.append(start + block_size // 2)\n",
    "        \n",
    "#         ax.plot(block_episodes, success_rates,\n",
    "#                label=f'Seed {m.seed}', marker='o', alpha=0.7, color=colors[i % len(colors)])\n",
    "    \n",
    "#     ax.set_xlabel('Episode')\n",
    "#     ax.set_ylabel('Success Rate (% with score > 0)')\n",
    "#     ax.set_title(f'Learning Progress: Success Rate per {block_size} Episodes')\n",
    "#     ax.legend()\n",
    "#     ax.grid(True, alpha=0.3)\n",
    "#     ax.set_ylim([0, 105])\n",
    "    \n",
    "#     plt.tight_layout()\n",
    "#     plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "#     print(f\"âœ… Saved: {save_path}\")\n",
    "#     plt.close()\n",
    "    \n",
    "# if __name__ == \"__main__\":\n",
    "#     print(\"\\n\" + \"=\"*80)\n",
    "#     print(\"COMPREHENSIVE METRICS VISUALIZATION (DQN)\")\n",
    "#     print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "#     # Load metrics\n",
    "#     print(\"Loading DQN metrics files...\")\n",
    "#     metrics_list = load_metrics(\"metrics_dqn_seed*.pkl\")\n",
    "    \n",
    "#     if not metrics_list:\n",
    "#         print(\"âŒ No DQN metrics files found! Run your DQN training first.\")\n",
    "#         exit(1)\n",
    "    \n",
    "#     print(f\"âœ… Loaded {len(metrics_list)} DQN metric files\\n\")\n",
    "    \n",
    "#     # Generate all visualizations\n",
    "#     print(\"Generating visualizations...\\n\")\n",
    "    \n",
    "#     plot_policy_behavior(metrics_list, '5_policy_behavior_dqn.png')\n",
    "    \n",
    "\n",
    "#     print(\"\\n\" + \"=\"*80)\n",
    "#     print(\"ðŸŽ‰ ALL DQN VISUALIZATIONS COMPLETE!\")\n",
    "#     print(\"=\"*80)\n",
    "#     print(\"\\nGenerated files:\")\n",
    "  \n",
    "#     print(\"  5. 5_policy_behavior_dqn.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c15266b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_convergence_stability(metrics_list, save_path='4_convergence_stability_dqn.png'):\n",
    "#     \"\"\"Convergence and Stability Visualization\"\"\"\n",
    "#     fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "#     fig.suptitle('4. Convergence & Stability Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "#     colors = ['#2E86AB', '#A23B72', '#F18F01']\n",
    "\n",
    "#     # Plot 1: All reward curves together\n",
    "#     ax = axes[0, 0]\n",
    "#     for i, m in enumerate(metrics_list):\n",
    "#         episodes = range(1, len(m.moving_avg_reward) + 1)\n",
    "#         ax.plot(episodes, m.moving_avg_reward, label=f'Seed {m.seed}', alpha=0.7, linewidth=2, color=colors[i % len(colors)])\n",
    "\n",
    "#     max_len = max(len(m.moving_avg_reward) for m in metrics_list)\n",
    "#     all_rewards = np.zeros((len(metrics_list), max_len))\n",
    "#     for i, m in enumerate(metrics_list):\n",
    "#         all_rewards[i, :len(m.moving_avg_reward)] = m.moving_avg_reward\n",
    "#         if len(m.moving_avg_reward) < max_len:\n",
    "#             all_rewards[i, len(m.moving_avg_reward):] = m.moving_avg_reward[-1]\n",
    "\n",
    "#     mean_reward = np.mean(all_rewards, axis=0)\n",
    "#     std_reward = np.std(all_rewards, axis=0)\n",
    "#     episodes = range(1, max_len + 1)\n",
    "#     ax.fill_between(episodes, mean_reward - std_reward, mean_reward + std_reward, alpha=0.2, color='gray', label='Â±1 Std Dev')\n",
    "#     ax.plot(episodes, mean_reward, 'k--', linewidth=2, label='Mean')\n",
    "#     ax.set_xlabel('Episode')\n",
    "#     ax.set_ylabel('Moving Avg Reward')\n",
    "#     ax.set_title('Reward Convergence Across Multiple Seeds')\n",
    "#     ax.legend()\n",
    "#     ax.grid(True, alpha=0.3)\n",
    "\n",
    "#     # Plot 2: All score curves together\n",
    "#     ax = axes[0, 1]\n",
    "#     for i, m in enumerate(metrics_list):\n",
    "#         episodes = range(1, len(m.moving_avg_score) + 1)\n",
    "#         ax.plot(episodes, m.moving_avg_score, label=f'Seed {m.seed}', alpha=0.7, linewidth=2, color=colors[i % len(colors)])\n",
    "\n",
    "#     all_scores = np.zeros((len(metrics_list), max_len))\n",
    "#     for i, m in enumerate(metrics_list):\n",
    "#         all_scores[i, :len(m.moving_avg_score)] = m.moving_avg_score\n",
    "#         if len(m.moving_avg_score) < max_len:\n",
    "#             all_scores[i, len(m.moving_avg_score):] = m.moving_avg_score[-1]\n",
    "\n",
    "#     mean_score = np.mean(all_scores, axis=0)\n",
    "#     std_score = np.std(all_scores, axis=0)\n",
    "#     ax.fill_between(episodes, mean_score - std_score, mean_score + std_score, alpha=0.2, color='gray', label='Â±1 Std Dev')\n",
    "#     ax.plot(episodes, mean_score, 'k--', linewidth=2, label='Mean')\n",
    "#     ax.set_xlabel('Episode')\n",
    "#     ax.set_ylabel('Moving Avg Score')\n",
    "#     ax.set_title('Score Convergence Across Multiple Seeds')\n",
    "#     ax.legend()\n",
    "#     ax.grid(True, alpha=0.3)\n",
    "\n",
    "#     # Plot 3: Variance over time\n",
    "#     ax = axes[1, 0]\n",
    "#     window = 100\n",
    "#     variance_over_time = []\n",
    "#     for i in range(0, max_len, window):\n",
    "#         window_scores = all_scores[:, i:min(i+window, max_len)]\n",
    "#         variance_over_time.append(np.mean(np.var(window_scores, axis=0)))\n",
    "#     episodes_binned = range(window, max_len + 1, window)\n",
    "#     ax.plot(episodes_binned, variance_over_time, marker='o', color='#2E86AB', linewidth=2)\n",
    "#     ax.set_xlabel('Episode')\n",
    "#     ax.set_ylabel(f'Mean Variance (per {window} episodes)')\n",
    "#     ax.set_title('Stability: Inter-Seed Variance Over Training')\n",
    "#     ax.grid(True, alpha=0.3)\n",
    "\n",
    "\n",
    "#     # Plot 4: Final performance comparison\n",
    "#     ax = axes[1, 1]\n",
    "\n",
    "#     final_scores = [np.mean(m.episode_scores[-100:]) for m in metrics_list]\n",
    "#     final_rewards = [np.mean(m.episode_rewards[-100:]) for m in metrics_list]\n",
    "#     seeds = [m.seed for m in metrics_list]\n",
    "\n",
    "#     x_pos = np.arange(len(seeds))\n",
    "#     width = 0.35\n",
    "\n",
    "#     ax.bar(x_pos - width/2, final_scores, width, label='Avg Score (last 100)', color='#2E86AB', alpha=0.7)\n",
    "#     ax.bar(x_pos + width/2, final_rewards, width, label='Avg Reward (last 100)', color='#F18F01', alpha=0.7)\n",
    "\n",
    "#     ax.set_xticks(x_pos)\n",
    "#     ax.set_xticklabels([f'Seed {s}' for s in seeds])\n",
    "#     ax.set_ylabel('Performance')\n",
    "#     ax.set_title('Final Performance Comparison')\n",
    "#     ax.legend()\n",
    "#     ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "#     print(f\"âœ… Saved: {save_path}\")\n",
    "#     plt.close()\n",
    "    \n",
    "# if __name__ == \"__main__\":\n",
    "#     print(\"\\n\" + \"=\"*80)\n",
    "#     print(\"COMPREHENSIVE METRICS VISUALIZATION (DQN)\")\n",
    "#     print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "#     # Load metrics\n",
    "#     print(\"Loading DQN metrics files...\")\n",
    "#     metrics_list = load_metrics(\"metrics_dqn_seed*.pkl\")\n",
    "    \n",
    "#     if not metrics_list:\n",
    "#         print(\"âŒ No DQN metrics files found! Run your DQN training first.\")\n",
    "#         exit(1)\n",
    "    \n",
    "#     print(f\"âœ… Loaded {len(metrics_list)} DQN metric files\\n\")\n",
    "    \n",
    "#     # Generate all visualizations\n",
    "#     print(\"Generating visualizations...\\n\")\n",
    "    \n",
    "\n",
    "#     plot_convergence_stability(metrics_list, '4_convergence_stability_dqn.png')\n",
    "\n",
    "    \n",
    "#     # # Generate summary report\n",
    "#     # print(\"\\nGenerating summary report...\\n\")\n",
    "#     # generate_summary_report(metrics_list, agent_name=\"DQN\", save_path='summary_report_dqn.txt')\n",
    "    \n",
    "#     print(\"\\n\" + \"=\"*80)\n",
    "#     print(\"  4. 4_convergence_stability_dqn.png\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (snake-gym)",
   "language": "python",
   "name": "snake-gym"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
